# Spatial Statistics: Gaussian Process and Kriging

## Gaussian Process
### Definition and properties
### Hierarchical model for geostatistical process

## Kriging
Kriging is a method of interpolation based on Gaussian process, and is also known as Gaussian process regression (GPR). Georges Matheron established the theoretic basis of this spatial interpolation technique in 1960, and named it kriging (*French: krigeage*) to honor the pioneering work of Danie G. Krige in geostatistics.

Given a random field $\lbrace Z(s): s\in\mathcal{D}\rbrace$ with its observation at location $s_1,\cdots,s_n$, the goal of kriging is to predict $Z(\cdot)$ at an arbitrary location $s\in\mathcal{D}.$

### Simple Kriging
In simple kriging, the mean of spatial field is known. Without loss of generality, set it to be zero.

Let $f:\mathrm{X}\to{\mathbb{R}}$ be the field of interest and $f\sim\mathcal{GP}\left(0,k(\cdot,\cdot)\right)$, we will interpolate the value of $f$ at an arbitrary location using its finite observations. Please note that $f$ is not a random field or a Gaussian process itself since it is deterministic. Instead, $f$ is a realization of the aforementioned Gaussian process. In the context of Bayesian regression, $\mathcal{GP}(0,k(\cdot,\cdot)$ is the prior distribution of $f$.

We first introduce some notations. Given locations $x_1,\cdots,x_N\in\mathcal{X}$, we use $\mathbf{K}$ to denote the covariance matrix of $\left(f(x_1),\cdots,f(x_N)\right)^\top,$ i.e., $\mathbf{K} = \lbrace k(x_i,x_j)\rbrace_{i,j=1}^N\in\mathbb{R}^{n\times n}$. We define $\mathbf{k}(\cdot):\mathcal{X}\to\mathbf{R}^N$ to be such that
<p>
  $$\mathbf{k}(x) = \left(k(x,x_1),\cdots,k(x,x_N)\right)^\top.$$
</p>

#### Case 1. Noise-free observations
We suppose that our observations are faithful, that is, the observation at location $x_i$ is the true value of $f(x_i)$. If we have observations at $N$ distinct locations $x_1,\cdots,x_N\in\mathcal{X}$, then our dataset is $\left\lbrace\left(x_i,f(x_i)\right)\right\rbrace_{i=1}^N.$ Denote $\mathbf{f} = \left(f(x_1),\cdots,f(x_N)\right)^\top,$ the posterior of $f$ at any location is given by the following theorem.

**Theorem 1.** (Posterior of $f$ conditioning on noise-free observations). Under the notations above plus $\mathbf{K}$ is nonsingular, the posterior of $f$ at an arbitrary location $x\in\mathcal{X}$ is Gaussian. The mean and variance is given by
<p>
  $$\begin{aligned}\mathbb{E}\left[f(x)\left|\mathbf{f}\right.\right] &= \mathbf{k}(x)^\top\mathbf{K}^{-1} \mathbf{f},\\
  \mathrm{Var}\left\lbrace f(x)\left|\mathbf{f}\right.\right\rbrace &= k(x,x) - \mathbf{k}(x)^\top\mathbf{K}^{-1}\mathbf{k}(x).
  \end{aligned}$$
</p>

Moreover, $\forall x,x'\in\mathcal{X}$,
<p>
  $$\mathrm{Cov}\left\lbrace f(x), f(x')\left|\mathbf{f}\right.\right\rbrace = k(x,x') - \mathbf{k}(x)^\top\mathbf{K}^{-1} \mathbf{k}(x').$$
</p>

**Proof.** $\forall x,x'\in\mathcal{X}$, the joint distribution of $\left(f(x),f(x'),f(x_1),\cdots,f(x_n)\right)^\top$ can be derived:
<p>
  $$\begin{pmatrix} f(x) \\ f(x') \\ \mathbf{f}\end{pmatrix} \sim N\left\lbrace \mathbf{0},\begin{pmatrix}
  k(x,x) & k(x,x') & \mathbf{k}(x)^\top \\
  k(x', x) & k(x',x') & \mathbf{k}(x')^\top \\
  \mathbf{k}(x) & \mathbf{k}(x') & \mathbf{K}
  \end{pmatrix}\right\rbrace.$$
</p>

Then conditional on $\mathbf{f},$  the distribution of $(f(x),f(x'))^\top$ is still Gaussian:
<p>
  $$\left.\begin{pmatrix} f(x) \\ f(x')\end{pmatrix}\right|\,\mathbf{f} \sim N\left\lbrace
  \begin{pmatrix}
  \mathbf{k}(x)^\top \\
  \mathbf{k}(x')^\top
  \end{pmatrix}\mathbf{K}^{-1}\mathbf{f},\ 
  \begin{pmatrix}
  k(x,x) & k(x,x') \\
  k(x', x) & k(x',x')
  \end{pmatrix} - \begin{pmatrix}
  \mathbf{k}(x)^\top \\
  \mathbf{k}(x')^\top
  \end{pmatrix}\mathbf{K}^{-1}\left(\mathbf{k}(x),\mathbf{k}(x')\right)\right\rbrace.$$
</p>


**Remark.**
+ When $x$ is some $x_i\in\lbrace x_1,\cdots, x_N\rbrace$, the posterior of $f(x_i)$ reduces to the Dirac measure centered on $\mathbf{f}_i.$ To see this, note that
  <p>
    $$\mathbf{e}_i = \mathbf{K}^{-1}\mathbf{Ke}_i = \mathbf{K}^{-1}\mathbf{k}(x_i),$$
  </p>
  then <p>
    $$\mathbb{E}\left[f(x_i)\left|\mathbf{f}\right.\right] = \mathbf{e}_i\mathbf{f} = f(x_i),\ \mathrm{Var}\left\lbrace f(x)\left|\mathbf{f}\right.\right\rbrace = k(x_i,x_i) - \mathbf{k}(x_i)^\top\mathbf{e}_i = 0.$$
  </p>

#### Case 2. Observations with additive noise

**Theorem 2.** (Posterior of $f$ conditioning on observations with additive noise). Under the notations above, the posterior of $f$ at an arbitrary location $x\in\mathcal{X}$ is Gaussian. The mean and variance is given by
<p>
  $$\begin{aligned}\mathbb{E}\left[f(x)\left|Y\right.\right] &= \mathbf{k}(x)^\top\left(\mathbf{K} + \sigma^2\mathbf{I}_{N}\right)^{-1} Y,\\
  \mathrm{Var}\left\lbrace f(x)\left|Y\right.\right\rbrace &= k(x,x) - \mathbf{k}(x)^\top\left(\mathbf{K} + \sigma^2\mathbf{I}_{N}\right)^{-1}{\mathbf{k}}(x).
  \end{aligned}$$
</p>

Moreover, $\forall x,x'\in\mathcal{X}$,
<p>
  $$\mathrm{Cov}\left\lbrace f(x), f(x')\left|Y\right.\right\rbrace = k(x,x') - \mathbf{k}(x)^\top\left(\mathbf{K} + \sigma^2\mathbf{I}_{N}\right)^{-1}{\mathbf{k}}(x').$$
</p>

**Proof.** Analogous to Theorem 1, use the joint distribution of $\left(f(x),f(x'),Y\right)$ to derive the conditional (posterior) distribution. 

#### Case 3. Overlapping observations
In previous discussions, locations $x_1,\cdots,x_N$ are supposed to be distinct. In other words, we have only one observation for a unique location.

Now we suppose that we have $N$ observations at $n$ unique locations $\bar{x}_ 1,\cdots,\bar{x}_ n$ with $n < N.$ At location $\bar{x}_ j$, we have $a_ j$ observations $y_ {j,1},\cdots,y_ {j,a_ j},$ with $j=1,\cdots,n,\ a_1+\cdots+a_n = N$ and $\max_j a_j > 1$.

We denote
<p>
  $$(x_1,\cdots, x_N) = (\underbrace{\bar{x}_1,\cdots,\bar{x}_1}_{a_1},\ \underbrace{\bar{x}_2,\cdots,\bar{x}_2}_{a_2},\cdots,\underbrace{\bar{x}_n,\cdots,\bar{x}_n}_{a_n}) \in\mathcal{X}^N,$$
</p>
<p>
  $$Y = (y_{1,1},\cdots,y_{1,a_1},y_{2,1},\cdots,y_{2,a_2},\cdots,y_{n,1},\cdots,y_{n,a_n})\in\mathbb{R}^N,$$
</p>
<p>
  $$\bar{Y} = (\bar{y}_1,\cdots,\bar{y}_n)\in\mathbb{R}^n,\ \ \bar{y}_j = \frac{1}{a_j}\sum_{l=1}^{a_j}y_{j,a_j}.$$
</p>

Moreover, denote
<p>
  $$\bar{\mathbf{k}}(x) = \left(k(x,\bar{x}_1),\cdots,k(x,\bar{x}_n)\right)^\top\in\mathbb{R}^n,\ \ \bar{\mathbf{K}} = \lbrace k(\bar{x}_i,\bar{x}_j)\rbrace_{i,j=1}^n\in\mathbb{R}^{n\times n}.$$
</p>

Let $\mathbf{U}\in\mathbb{R}^{N\times n}$ be a block diagonal matrix such that $\mathbf{U}=\mathrm{diag}\lbrace \mathbf{1}_ {a_ 1},\cdots,\mathbf{1}_ {a_ n}\rbrace$, where $\mathbf{1}_ {a_j}$ is an $a_j$-dimensional all-one vector, and let $\mathbf{A} = \mathrm{diag}\lbrace a_1,\cdots,a_n\rbrace.$ 

**Proposition 1.** With the notations above, we have $\bar{Y} = \mathbf{A}^{-1}\mathbf{U}^\top Y,\ \bar{\mathbf{k}}(x) = \mathbf{A}^{-1}\mathbf{U}^\top\mathbf{k}(x),\ \mathbf{k}(x) = \mathbf{U}\bar{\mathbf{k}}(x),\ \mathbf{K} = \mathbf{U}\bar{\mathbf{K}}\mathbf{U}^\top.$

These equalities can be verified with simple calculation.

**Proposition 2.** Suppose $\sigma^2 > 0,$ then $\mathbf{U}^\top\left(\sigma^2\mathbf{I}_N + \mathbf{K}\right)^{-1}\mathbf{U} = \left(\sigma^2\mathbf{A}^{-1} + \bar{\mathrm{K}}\right)^{-1}.$

**Proof.** Recall the Sherman-Morrison-Woodbury formula:

$$(\mathbf{A} + \mathbf{UCV})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{U}\left(\mathbf{C}^{-1} + \mathrm{V}\mathbf{A}^{-1}\mathrm{U}\right)^{-1}\mathbf{V}\mathbf{A}^{-1}.$$

We have
<p>
  $$\left(\sigma^2\mathbf{I}_N + \mathbf{K}\right)^{-1} = \left(\sigma^2\mathbf{I}_N + \mathbf{U}\bar{\mathbf{K}}\mathbf{U}^\top\right)^{-1} = \sigma^{-2}\mathbf{I}_N - \sigma^{-4}\mathrm{U}\left(\bar{\mathbf{K}}^{-1} + \sigma^{-2}\mathbf{U}^\top\mathbf{U}\right)^{-1}\mathbf{U}^\top;$$
</p>

Note that $\mathbf{U}^\top\mathbf{U} = \mathbf{A}$, then
<p>
  $$\mathbf{U}^\top\left(\sigma^2\mathbf{I}_N + \mathbf{K}\right)^{-1}\mathbf{U} = \sigma^{-2}\mathbf{A} - \sigma^{-4}\mathbf{A}\left(\bar{\mathbf{K}}^{-1} + \sigma^{-2}\mathbf{A}\right)^{-1}\mathbf{A},$$
</p>

Using the Sherman-Morrison-Woodbury formula, we have
<p>
  $$\sigma^{-2}\mathbf{A} + \sigma^{-4}\mathbf{A}\left(\bar{\mathbf{K}}^{-1} + \sigma^{-2}\mathbf{A}\right)^{-1}\mathbf{A} = \left(\sigma^2\mathbf{A}^{-1} + \bar{\mathbf{K}}\right)^{-1},$$
</p>

which concludes the proof.

**Theorem 3.** (Posterior of $f$ conditioning on overlapping observations). Under the notations above, the posterior of $f$ at an arbitrary location $x\in\mathcal{X}$ is Gaussian. The mean and variance is given by
<p>
  $$\begin{aligned}\mathbb{E}\left[f(x)\left|\bar{Y}\right.\right] &= \bar{\mathbf{k}}(x)^\top\left(\bar{\mathbf{K}} + \sigma^2\mathbf{A}^{-1}\right)^{-1}\bar{Y},\\
  \mathrm{Var}\left\lbrace f(x)\left|\bar{Y}\right.\right\rbrace &= k(x,x) - \bar{\mathbf{k}}(x)^\top\left(\bar{\mathbf{K}} + \sigma^2\mathbf{A}^{-1}\right)^{-1}\bar{\mathbf{k}}(x).
  \end{aligned}$$
</p>

Moreover, $\forall x,x'\in\mathcal{X}$,
<p>
  $$\mathrm{Cov}\left\lbrace f(x), f(x')\left|Y\right.\right\rbrace = k(x,x') - \bar{\mathbf{k}}(x)^\top\left(\bar{\mathbf{K}} + \sigma^2\mathbf{A}^{-1}\right)^{-1}\bar{\mathbf{k}}(x').$$
</p>

**Proof.** Plug in Proposition 1 and 2 to the posterior of $f$ given in Theorem 2.

### Universal Kriging
In universal kriging, the mean of spatial field is unknown.
