# Structural Causal Models (II): Selection Bias and Transportability
## Selection Bias
Nonrandom sampling compromises the representativeness of the collected data for the underlying population, which can explain away the conclusions of both statistical and causal inference.
### Augmented Causal DAG
To capture the sample selection mechanism, a dichotomous variable $S$ is introduced to indicate whether a unit is part of the sample. Endogenous variables that affect the sampling probability will be connected to $S$ with directed edges. (Here we do not consider the cases where $S$ emits arrows.) The augmented DAG $\mathcal{G}$ is denoted as $\mathcal{G}_S.$ 

An oversimplified example is given below.

<div align=center>
   <img src='https://github.com/JurrivhLeon/JurrivhLeon.github.io/raw/main/figs/causalDAG4.png' width='320'/> 
</div>

After augmentation, we need to simultaneously control for confounding and selection bias in do-calculus. Not only is it necessary to transform interventional distributions
into do-free expressions, but the probabilities that make up these expressions now also need to be conditional on $S = 1$, because that is all the analyst is able to observe.

### Recoverability

We first give the necessary and sufficient condition of recoverability in do-free expressions.

**Theorem 1** (Bareinboim et al., 2014). The conditional distribution $p(y\vert t)$ is recoverable from $\mathcal{G}_ S,$ as $p(y\vert t, S = 1),$ if and only if $(Y\perp S \vert T).$

Combining Theorem 1 with do-calculus suggests a straightforward strategy for recovering do-expressions from selection bias:

**Corollary 1** (Bareinboim and Tian, 2015). The causal effect $Q = p(y\vert\mathrm{do}(x))$ is recoverable from selection-biased data, i.e. $p(v\vert S = 1),$ if using the rules of the do-calculus,
$Q$ is reducible to a do-free expression, and recoverability is determined by Theorem 1.

**Remark.** Corollary 1 is not a necessary condition for recovering conditional probabilities with respect to do-expressions.

**Analytical Example.** Consider the following two causal DAGs below.

<div align=center>
   <img src='https://github.com/JurrivhLeon/JurrivhLeon.github.io/raw/main/figs/causalDAG5.png' width='320'/> 
</div>

The following lemma is a straightforward conclusion of Theorem 1.

**Lemma 1.** The distribution $p(v_i\vert pa_i)$ is recoverable if and only if $V_i$ is not an ancestor of $S.$ When recoverable, $p(v_i\vert pa_i) = p(v_i\vert pa_i, S=1).$

Then, the necessary and sufficient condition of recovering a causal effect from selection bias is given below.

**Theorem 2** (Bareinboim and Tian, 2015). The causal effect $Q = p(y\vert\mathrm{do}(x))$ is recoverable from selection-biased data, if and only if for every $V_i\in D,$ where $D=An(Y)_ {\mathcal{G}_ {V\backslash X}},$ $V_i$ is not an ancestor of $S$. When recoverable, $p(y\vert\mathrm{do}(x))$ is given by
<p>
   $$p(y\vert\mathrm{do}(x)) = \sum_{\sigma(D\backslash Y)}\prod_{i:V_i\in D}p(v_i\vert pa_i, S=1), \tag{1}$$
</p>

$D=An(Y)_ {\mathcal{G}_ {V\backslash X}}$ denotes the ancestors of $Y$ in graph $\mathcal{G}_ {V\backslash X}$ obtained by removing all nodes in $X$ and their incoming and outgoing edges, and $\sigma(D\backslash Y)$ denotes the correpsonding (joint) domain of variables in $D\backslash Y.$

We only concentrate on the if statement. Let $C_X = V\backslash X,$ we have
<p>
   $$p(y\vert\mathrm{do}(x)) = \sum_{\sigma(C_X\backslash Y)}p(c'\vert\mathrm{do}(x)) = \sum_{\sigma(C_X\backslash Y)}\prod_{i:V_i\in C_X} p(v_i\vert pa_i), \tag{2}$$
</p>

where the first equality holds by the law of total probability, and the second is from Markov factorization.

For all $V_i\in C_X\backslash D,$ i.e. $V_i$ is not an ancestor of $Y,$ it can be summed out:
<p>
   $$\begin{align}
   \sum_{\sigma(C_X\backslash Y)}\prod_{i:V_i\in C_X} p(v_i\vert pa_i) &= \sum_{\sigma(D\backslash Y)}\sum_{\sigma(C_X\backslash D)}\prod_{i:V_i\in D} p(v_i\vert pa_i)\prod_{j:V_j\in C_X\backslash D} p(v_j\vert pa_j)\\
   &= \left[\sum_{\sigma(D\backslash Y)}\prod_{i:V_i\in D}p(v_i\vert pa_i)\right] \left[\sum_{\sigma(C_X\backslash D)}\prod_{j:V_j\in C_X\backslash D} p(v_j\vert pa_j)\right]\\
   &= \sum_{\sigma(D\backslash Y)}\prod_{i:V_i\in D}p(v_i\vert pa_i)\\
   &= \sum_{\sigma(D\backslash Y)}\prod_{i:V_i\in D}p(v_i\vert pa_i,S=1),
   \end{align} 
   \tag{3}$$
</p>

where the last equality holds by Lemma 1.

### Combination of biased and unbiased data

## Transportability
