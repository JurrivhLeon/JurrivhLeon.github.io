# Reinforcement Learning: Pessimism in Offline RL
**Setting.** Consider an episodic MDP $\mathcal{M}=(\mathcal{S},\mathcal{A},H,\mathcal{P},r)$ with the state space $\mathcal{S},$ action space $\mathcal{A},$ horizon $H,$ transition kernel $\mathcal{P}=\lbrace\mathcal{P}_ h\rbrace_ {r=1}^H,$ and reward function $r=\lbrace r_ h:\mathcal{S}\times\mathcal{A}\to [0,1]\rbrace_ {h=1}^H.$ For any policy $\pi=\lbrace \pi_h\rbrace_ {h=1}^H,$ define the value function $V_ h^\pi:\mathcal{S}\to\mathbb{R}$ and the Q-value function $Q_ h^\pi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ at each step $h\in[H]$ as
<p>
  $$\begin{align*}
  V_h^\pi(x) &= \mathbb{E}_\pi\left[\left.\sum_{t=h}^H r_t(s_t,a_t)\right|s_h=x\right],\\
  Q_h^\pi(x,a) &= \mathbb{E}_\pi\left[\left.\sum_{t=h}^H r_t(s_t,a_t)\right|s_h=x,a_h=a\right],
  \end{align*}$$
</p>

where $\mathbb{E}_ \pi$ is taken with respect to the probability measure of the trajectory $\tau=\lbrace(s_ h,a_ h)\rbrace_ {h=1}^H$ induced by $\pi,$ i.e. at each step $h\in[H],$
<p>
  $$a_h\sim\pi_h(\cdot\vert s_h),\ s_{h+1}\sim\mathcal{P}_h(\cdot|s_h,a_h).$$
</p>

For any function $V:\mathcal{S}\to\mathcal{R},$ Define the transition operator $\mathbb{P}_ h$ and the Bellman operator $\mathbb{B}_ h$ at each step $h\in[H]$ as follows:
<p>
  $$\begin{align*}
  (\mathbb{P}_h V)(x,a) &:= \mathbb{E}_{s'\sim\mathcal{P}_h(\cdot|x,a)} V(s'),\\
  (\mathbb{B}_h V)(x,a) &:= r_h(x,a) + (\mathbb{P}_h V)(x,a).
  \end{align*}$$
</p>

Let $\pi^{* }$ be the optimal policy in this MDP and $\lbrace Q^{* }_ h\rbrace_ {h=1}^H, \lbrace V^{* }_ h\rbrace_ {h=1}^H$ the corresponding value and Q-value functions. Then we have the Bellman optimality equation
<p>
  $$V_h^*(x) = \max_{a\in\mathcal{A}}Q_h^*(x,a),\ Q_h^*(x,a) = (\mathbb{B}_hV_{h+1}^*)(x,a).$$
</p>
