# Pessimism in Offline RL
**Setting.** Consider an episodic MDP $\mathcal{M}=(\mathcal{S},\mathcal{A},H,\mathcal{P},r)$ with the state space $\mathcal{S},$ action space $\mathcal{A},$ horizon $H,$ transition kernel $\mathcal{P}=\lbrace\mathcal{P}_ h\rbrace_ {r=1}^H,$ and reward function $r=\lbrace r_ h:\mathcal{S}\times\mathcal{A}\to [0,1]\rbrace_ {h=1}^H.$ For any policy $\pi=\lbrace \pi_h\rbrace_ {h=1}^H,$ define the value function $V_ h^\pi:\mathcal{S}\to\mathbb{R}$ and the Q-value function $Q_ h^\pi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ at each step $h\in[H]$ as
<p>
  $$\begin{align*}
  V_h^\pi(x) &= \mathbb{E}_\pi\left[\left.\sum_{t=h}^H r_t(s_t,a_t)\right|s_h=x\right],\\
  Q_h^\pi(x,a) &= \mathbb{E}_\pi\left[\left.\sum_{t=h}^H r_t(s_t,a_t)\right|s_h=x,a_h=a\right],
  \end{align*}$$
</p>

where $\mathbb{E}_ \pi$ is taken with respect to the probability measure of the trajectory $\tau=\lbrace(s_ h,a_ h)\rbrace_ {h=1}^H$ induced by $\pi,$ i.e. at each step $h\in[H],$
<p>
  $$a_h\sim\pi_h(\cdot\vert s_h),\ s_{h+1}\sim\mathcal{P}_h(\cdot|s_h,a_h).$$
</p>

For any function $V:\mathcal{S}\to\mathcal{R},$ Define the transition operator $\mathbb{P}_ h$ and the Bellman operator $\mathbb{B}_ h$ at each step $h\in[H]$ as follows:
<p>
  $$\begin{align*}
  (\mathbb{P}_h V)(x,a) &:= \mathbb{E}_{s'\sim\mathcal{P}_h(\cdot|x,a)} V(s'),\\
  (\mathbb{B}_h V)(x,a) &:= r_h(x,a) + (\mathbb{P}_h V)(x,a).
  \end{align*}$$
</p>

Let $\pi^{* }$ be the optimal policy in this MDP and $\lbrace Q^{* }_ h\rbrace_ {h=1}^H, \lbrace V^{* }_ h\rbrace_ {h=1}^H$ the corresponding value and Q-value functions. Then we have the Bellman optimality equation
<p>
  $$V_h^*(x) = \max_{a\in\mathcal{A}}Q_h^*(x,a),\ Q_h^*(x,a) = (\mathbb{B}_hV_{h+1}^*)(x,a).$$
</p>

**Goal.** We aim to learn a policy that maximizes the expected cumulative reward. Given a policy $\pi,$ define its suboptimality as
<p>
  $$\mathrm{SubOpt}(\pi;x) = V_1^*(x) - V_1^\pi(x),$$
</p>

which is the performance metric in the following discussion.

## Offline Data Collection
In the offline setting, an agent has only access to a dataset $\mathcal{D}$ consisting of $K$ trajectories $\left\lbrace(x_ h^\tau, a_ h^\tau, r_ h^\tau)\right\rbrace_ {\tau=1,h=1}^{K,H}$ collected a priori by an experimenter. For the data collecting process, we assume the following condition.

**Assumption 1.** (Compliance). The dataset $\mathcal{D}$ that the learner has access to is compliant with the underlying MDP $\mathcal{M}=(\mathcal{S},\mathcal{A},H,\mathcal{P},r).$ That is,
<p>
  $$\begin{align*}
  \mathbb{P}_\mathcal{D}\left(r_h^\tau = r',x_{h+1}^\tau=x'|\lbrace(x_h^j,a_h^j)\rbrace_{j=1}^\tau, \lbrace(r_h^j,x_{h+1}^j)\rbrace_{j=1}^{\tau-1}\right)\\
  = \mathbb{P}_\mathcal{M}\left(r_h(s_h,a_h)=r',s_{h+1}=x'|s_h=x_h^\tau,a_h=a_h^\tau\right),
  \end{align*}$$
</p>

for all $r'\in[0,1],x'\in\mathcal{S},h\in[H],\tau\in[K],$ where $\mathbb{P}_ \mathcal{D}$ is the joint distribution of the data collecting process, and $\mathbb{P}_ \mathcal{M}$ the underlying MDP.

**Remark.** The compliance assumption ensures the Markov perperty of the data collecting process, that $(r_ h^\tau, s_ {h+1}^\tau)$ is dependent on $\lbrace(x_ h^j,a_ h^j)\rbrace_{j=1}^\tau, \lbrace(r_ h^j,x_ {h+1}^j)\rbrace_{j=1}^{\tau-1}$ only through $(x_ h^\tau, a_ h^\tau).$ Moreover, they are generated by the reward function and transition kernel of the underlying MDP.

## Decomposition of Suboptimality
Consider a meta algorithm which yields estimated value and Q-value functions $\widehat{V}_ h:\mathcal{S}\to\mathbb{R}$ and $\widehat{Q}_ h:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ based ont the dataset $\mathcal{D}$. We denote $\widehat{\pi}$ by the policy correponding to $\widehat{V}_ h$ and $\widehat{Q}_ h$ in sense that $\widehat{V}_ h(x) = \langle\widehat{Q}_ h(x,\cdot),\widehat{\pi}(\cdot\vert x)\rangle_ \mathcal{A}.$ Define the model evaluation error at each step $h\in[H]$ as
<p>
  $$\iota_h(x,a)=(\mathbb{B}_h\widehat{V}_{h+1})(x,a)-\widehat{Q}_h(x,a).$$
</p>

For any policy $\pi=\lbrace\pi_ h\rbrace_ {h=1}^H,$ we have
<p>
  $$\begin{align*}
  \mathbb{E}_\pi[\iota_h(s_h,a_h)|s_1=x] = 
  \end{align*}$$
</p>
