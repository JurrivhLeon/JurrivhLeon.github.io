# Contextual Bandits with Confounding Bias and Missing Observations
## Offline Policy Learning in Confounded Contextual Bandit (CCB)
In a confounded contextual bandit, each trial can be represented by a 5-tuple $(U,X,A,Y,O),$ where
+ $U\in\mathcal{U}$ is the set of unobserved confounders,
+ $X\in\mathcal{X}$ is the context,
+ $A\in\mathcal{A}$ is the treatment or action,
+ $Y\in\mathbb{R}$ is the reward, and
+ $O\in\mathcal{O}$ are the side observations.

### Offline data collection
The collection of offline dataset $\mathcal{D}$ is described by the observational process. 

1. In the $t^\text{th}$ trial, the environment samples $(u_ t,x_ t,o_ t)$ as a realization of $(U,X,O)$ according to the prior $p(u,x,o).$
2. The agent then conducts a treatment according to the observational policy $\pi_ \mathrm{obs}:\mathcal{X}\times\mathcal{O}\times\mathcal{U}\to\Delta(\mathcal{A}).$ This policy is confounded by $U,$ which can be viewed as a predilection for certain treatment due to some hidden causes encoded by $U.$
3. The agent receives a reward $Y$ conditioning on $(U,X,O,A).$ The joint distribution in the observational process is given by
   <p>$$p_\mathrm{obs}(u,x,o,a,y)=p(u,x,o)\pi_\mathrm{obs}(a|u,x,o)p(y|u,x,o,a).$$</p>

With $T$ trials in the observational process, the full dataset $\tilde{\mathcal{D}} = \lbrace(u_ t,x_ t,o_ t,a_ t, y_ t)\rbrace_ {t=1}^T$ is not available due to the unmeasurement of confounders $u_ t$ and the missingness in both contexts $x_ t$ and side observations $o_ t.$

**Missing Mechanism.** Denote the collected dataset as
$\mathcal{D}=\lbrace(\check{x}_ t, a_ t, y_ t, \check{o}_ t)\rbrace_ {t=1}^T,$ where $\check{x}_ t$ and $\check{o}_ t$ encode the missingness. Let $R_ X$ and $R_ O$ denote the missing indicators for $X$ and $O,$ respectively, and $r_ {X,t},r_ {O,t}$ their realizations in the $t^\text{th}$ trial. With a dummy value $\mathrm{None}$ introduced to represent a missing record, we have
<p>$$\check{x_t}=\begin{cases}
  x_t,\ \text{if}\ r_{X,t}=1\\
  \mathrm{None},\ \text{if}\ r_{X,t}=0
  \end{cases}\qquad
  \check{o_t}=\begin{cases}
  o_t,\ \text{if}\ r_{O,t}=1\\
  \mathrm{None},\ \text{if}\ r_{O,t}=0.
  \end{cases}$$</p>

The missingness of context $X$ and side observation $O$ are supposed to be not at random but independent of outcome $Y.$  That is, $R_ X$ and $R_ O$ are not independent of $(U,X,O,A,Y),$ but are independent of $Y.$

### Interventional Process 
In the interventional process, an interventional policy is carried out after the model is learned from the offline dataset. The interventional process is different from the observational process in the following three aspects:
+ side observations $O$ are unmeasurable while context $X$ is fully measurable;
+ the agent adopts an interventional policy $\pi:\mathcal{X}\to\Delta(\mathcal{A})$ independent of unmeasurable $U$ and $O;$
+ context $X$ follows a new marginal distribution $\tilde{p}(x).$

Then the joint distribution $p^\pi_\mathrm{in}$ of random variables in the interventional process is given by
<p>$$p^\pi_\mathrm{in}(u,x,o,a,y) = \tilde{p}(x)p(u,o|x)\pi(a|x)p(y|u,x,o,a).$$</p>

**Goal.** In the interventional process, the average reward $v^\pi$ is defined as
<p>$$v^\pi = \mathbb{E}_{p^\pi_\mathrm{in}}[Y],$$</p>

where $\mathbb{E}_ {p^\pi_ \mathrm{in}}$ corresponds to the expectation taken with respect to $p^\pi_ \mathrm{in}.$ The goal of the agent is to find an optimal policy $\pi\in\Pi:\mathcal{X}\to\Delta(\mathcal{A})$ which maximizes the average reward:
<p>$$\pi^* = \underset{\pi\in\Pi}{\mathrm{argmax}}\ v^\pi.$$</p>

The performance metric of a policy $\pi$ is defined as its suboptimality:
<p>$$\mathrm{SubOpt}(\pi) = v^{\pi^*} - v^\pi.$$</p>
