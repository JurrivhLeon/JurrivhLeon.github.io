# Contextual Bandits with Confounding Bias and Missing Observations
## Offline Policy Learning in Confounded Contextual Bandit (CCB)
In a confounded contextual bandit, each trial can be represented by a 5-tuple $(U,X,A,Y,O),$ where
+ $U\in\mathcal{U}$ is the set of unobserved confounders,
+ $X\in\mathcal{X}$ is the context,
+ $A\in\mathcal{A}$ is the treatment or action,
+ $Y\in\mathbb{R}$ is the reward, and
+ $O\in\mathcal{O}$ are the side observations.

### Offline data collection
The collection of offline dataset $\mathcal{D}$ is described by the observational process. 

1. In the $t^\text{th}$ trial, the environment samples $(u_ t,x_ t,o_ t)$ as a realization of $(U,X,O)$ according to the prior $p(u,x,o).$
2. The agent then conducts a treatment according to the observational policy $\pi_ \mathrm{obs}:\mathcal{X}\times\mathcal{O}\times\mathcal{U}\to\Delta(\mathcal{A}).$ This policy is confounded by $U,$ which can be viewed as a predilection for certain treatment due to some hidden causes encoded by $U.$
3. The agent receives a reward $Y$ conditioning on $(U,X,O,A).$ The joint distribution in the observational process is given by
   <p>$$p_\mathrm{obs}(u,x,o,a,y)=p(u,x,o)\pi_\mathrm{obs}(a|u,x,o)p(y|u,x,o,a).$$</p>

With $T$ trials in the observational process, the full dataset $\tilde{\mathcal{D}} = \lbrace(u_ t,x_ t,o_ t,a_ t, y_ t)\rbrace_ {t=1}^T$ is not available due to the unmeasurement of confounders $u_ t$ and the missingness in both contexts $x_ t$ and side observations $o_ t.$

**Missing Mechanism.** Denote the collected dataset as
$\mathcal{D}=\lbrace(\check{x}_ t, a_ t, y_ t, \check{o}_ t)\rbrace_ {t=1}^T,$ where $\check{x}_ t$ and $\check{o}_ t$ encode the missingness. Let $R_ X$ and $R_ O$ denote the missing indicators for $X$ and $O,$ respectively, and $r_ {X,t},r_ {O,t}$ their realizations in the $t^\text{th}$ trial. With a dummy value $\mathrm{None}$ introduced to represent a missing record, we have
<p>$$\check{x_t}=\begin{cases}
  x_t,\ \text{if}\ r_{X,t}=1\\
  \mathrm{None},\ \text{if}\ r_{X,t}=0
  \end{cases}\qquad
  \check{o_t}=\begin{cases}
  o_t,\ \text{if}\ r_{O,t}=1\\
  \mathrm{None},\ \text{if}\ r_{O,t}=0.
  \end{cases}$$</p>

The missingness of context $X$ and side observation $O$ are supposed to be not at random but independent of outcome $Y.$  That is, $R_ X$ and $R_ O$ are not independent of $(U,X,O,A,Y),$ but are independent of $Y.$

### Interventional Process 
In the interventional process, an interventional policy is carried out after the model is learned from the offline dataset. The interventional process is different from the observational process in the following three aspects:
+ side observations $O$ are unmeasurable while context $X$ is fully measurable;
+ the agent adopts an interventional policy $\pi:\mathcal{X}\to\Delta(\mathcal{A})$ independent of unmeasurable $U$ and $O;$
+ context $X$ follows a new marginal distribution $\tilde{p}(x).$

Then the joint distribution $p^\pi_\mathrm{in}$ of random variables in the interventional process is given by
<p>$$p^\pi_\mathrm{in}(u,x,o,a,y) = \tilde{p}(x)p(u,o|x)\pi(a|x)p(y|u,x,o,a).$$</p>

The underlying DAGs of observational and interventional processes are presented below.

<div align='center'>
   <img src='https://github.com/JurrivhLeon/JurrivhLeon.github.io/raw/main/figs/CCB1.png' width='640'>
</div>

**Goal.** In the interventional process, the average reward $v^\pi$ is defined as
<p>$$v^\pi = \mathbb{E}_{p^\pi_\mathrm{in}}[Y],$$</p>

where $\mathbb{E}_ {p^\pi_ \mathrm{in}}$ corresponds to the expectation taken with respect to $p^\pi_ \mathrm{in}.$ The goal of the agent is to find an optimal policy $\pi\in\Pi:\mathcal{X}\to\Delta(\mathcal{A})$ which maximizes the average reward:
<p>$$\pi^* = \underset{\pi\in\Pi}{\mathrm{argmax}}\ v^\pi.$$</p>

The performance metric of a policy $\pi$ is defined as its suboptimality:
<p>$$\mathrm{SubOpt}(\pi) = v^{\pi^*} - v^\pi.$$</p>

## Causal-Adjusted Pessimistic (CAP) Policy Learning
### Average reward evaluation
Define the conditional average treatment effect (CATE) as
<p>$$g^*(x,a)=\mathbb{E}_\mathrm{obs}[Y|X=x,\mathrm{do}(A=a)],$$</p>

then the average reward can be learned via
<p>$$v^\pi = \mathbb{E}_{p_\mathrm{in}^\pi}[g^*(X,A)].$$</p>

### Policy Optimization
Let $g$ denote an estimation of the CATE and $g^*$ denote the exact CATE thereafter. The average reward function corresponding to $g$ and $\pi$ is defined as
<p>$$v(g,\pi) = \mathbb{E}_{p_\mathrm{in}^\pi}[g(X,A)].$$</p>

Then a greedy policy $\widehat{\pi}$ can be obtained. However, this policy often suffers from the spurious correlation between $g$ and $\widehat{\pi}.$ The technique of uncertainty quantification and pessimism can be adopted to address with this spurious correlation.

We first construct a confidence set $\mathrm{CI}_ \mathcal{D}$ to which $g^{* }$ belongs with high probability. By optimizing the policy with respect to $g\in\mathrm{CI}_ \mathcal{D}$ that minimizes $v(g,\cdot),$ we have $v(g,\widehat{\pi})\leq v(g^{* }, \widehat{\pi})$ and thus eliminate the spurious correlation. Hence, the estimated policy is given by
<p>$$\widehat{\pi} = \underset{\pi\in\Pi}{\mathrm{argsup}}\inf_{g\in\mathrm{CI}_\mathcal{D}} v(g,\pi).$$</p>

### Causal-adjusted pessimistic (CAP) algorithm
Having identified CATE from an integral equation system (IES), we can construct the confidence set $\mathrm{CI}_ \mathcal{D}$ from a hypothesis class $\mathcal{H}$ by minimizing some empirical loss function $\mathcal{L}_ \mathcal{D}:\mathcal{H}\to\mathbb{R}.$ An overview of the causal-adjusted pessimistic algorithm is presented below.
+ Input: dataset $\mathcal{D}=\lbrace\check{x}_ t,a_ t,y_ t,\check{o}_ t)_ {t=1}^T,$ hypothesis class $\mathcal{H},$ policy class $\Pi,$ threshold $e_ \mathcal{D}.$
+ Construct the confidence set $\mathrm{CI}_ \mathcal{D}(e_ \mathcal{D})$ as the level set of $\mathcal{H}$ with respect to metric $\mathcal{L}_ \mathcal{D}(\cdot)$ and threshold $e_ \mathcal{D}.$
+ $\widehat{\pi} = \mathrm{argsup}_ {\pi\in\Pi}\inf_ {g\in\mathrm{CI}_ \mathcal{D}(e_ \mathcal{D})} v(g,\pi).$
+ Output: $\widehat{\pi}$.

## Causal Identification of CATE
### CCB with Instrumental Variable (CCB-IV)

<div align='center'>
   <img src='https://github.com/JurrivhLeon/JurrivhLeon.github.io/raw/main/figs/CCB-IV.png' width='640'>
</div>

### CCB with Proximal Variable (CCB-PV)

<div align='center'>
   <img src='https://github.com/JurrivhLeon/JurrivhLeon.github.io/raw/main/figs/CCB-PV.png' width='640'>
</div>
