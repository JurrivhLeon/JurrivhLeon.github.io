# Bayesian Optimization in Inverse Problems
## Inverse Problems
Let $y^{* }\in\mathcal{Y}$ denote some noisy structured observational data that could be a scalar, a function or a tensor, and $H:\Theta\to\mathcal{Y}$ denotes a correctly specified model from a parameter space $\Theta$ to the output space $\mathcal{Y}.$ Then the objective of the inverse problem is to identify $\theta^{* }\in\Theta$ such that
<p>$$y^* = H(\theta^*) + \epsilon,$$</p>

where $\epsilon$ represents the unknown parameters $\theta\in\Theta.$ Practically, the inverse of $H$ is often intractable, hence we formulate the problem as a least squares problem:
<p>$$\theta^* = \underset{\theta\in\Theta}{\mathrm{argmin}}\Vert H(\theta) - y^*\Vert^2_{\mathcal{Y}},$$</p>

where $\Vert\cdot\Vert_\mathcal{Y}:\mathcal{Y}\to[0,\infty]$ is some norm that serves as a distance measure in space $\mathcal{Y}.$

The least squares problem can be very knotty, because the model $H$ is usually complicated or black-box, without a computable gradient or Hessian. The high computational cost of $H$ also rules out the possibility of precise numerical approximation. Moreover, multiple local minima could exist due to the noise-contaminated observations and possible information loss in the forward process of evaluating $H.$

## Bayesian Optimization
We consider the task of minimizing some black-box function $f:\mathcal{X}\to\mathbb{R}$ which could be expensive to evaluate. 

Suppose we have a noise-free dataset $\mathcal{D}_ n = \lbrace (x_i,f(x_i))\rbrace_ {i=1}^n.$ We place a Gaussian process prior on the target $f:$
<p>$$f\sim\mathcal{GP}(\mu,k(\cdot,\cdot)),$$</p>

where $\mu$ is the mean function and $k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ is a preselected kernel function. For an arbitrary location $x\in\mathcal{X},$ define $\mathbf{k}(x) = \left(k(x,x_1),\cdots,k(x,x_n)\right)^\top,$ and denote the covariance matrix on dataset $\mathcal{D}_ n$ by $\mathbf{K}=\lbrace k(x_ i,x_ j)\rbrace_ {i,j=1}^n.$ Then we can derive the conditional distribution of $f(x)$ on $\mathcal{D}_ n:$
<p>$$\begin{align*}
  f(x)&|\mathcal{D}_n \sim N\left(\tilde{\mu}(x),\tilde{\sigma}^2(x)\right),\\
  \tilde{\mu}(x) &= \mu(x) + \mathbf{k}(x)^\top\mathbf{K}^{-1}(f(x_{1:n})-\mu(x_{1:n})),\\
  \tilde{\sigma}^2(x) &= k(x,x) - \mathbf{k}(x)^\top\mathbf{K}^{-1}\mathbf{k}(x).
\end{align*}$$</p>

Bayesian optimization  selects the next sample via an acquisition function $\alphaâˆ¶ \mathcal{X}\to\mathbb{R}$ that assigns utility to any unseen input $x$ based on the inferred predictive distribution from the GP model. 

Define the Expected Improvement (EI) acquisition function as follows:
<p>$$\alpha_\mathrm{EI}(x;\mathcal{D}_n) := \mathbb{E}\left[\lbrace f_\min - f(x) \rbrace^+ | \mathcal{D}_n\right].$$</p>

This aquisition computes how much we can expect to improve over the best value $f_\min = \min_i f(x_i)$ we have obtained so far. Given the posterior of $f(x),$ a closed form of $\alpha_ \mathrm{EI}$ can be derived:
<p>$$\alpha_\mathrm{EI}(x;\mathcal{D}_n) = \tilde{\sigma}(x)\left\lbrace\left(\frac{f_\min - \tilde{\mu}(x)}{\tilde{\sigma}(x)}\right)\Phi\left(\frac{f_\min - \tilde{\mu}(x)}{\tilde{\sigma}(x)}\right) + \phi\left(\frac{f_\min - \tilde{\mu}(x)}{\tilde{\sigma}(x)}\right)\right\rbrace,$$</p>

where $\Phi(\cdot)$ and $\phi(\cdot)$ are the c.d.f. and p.d.f. of the standard Gaussian distribution $N(0,1),$ respectively.
