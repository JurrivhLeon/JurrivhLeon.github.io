# Bayesian Optimization in Inverse Problems
## Inverse Problems
Let $y^{* }\in\mathcal{Y}$ denote some noisy structured observational data that could be a scalar, a function or a tensor, and $H:\Theta\to\mathcal{Y}$ denotes a correctly specified model from a parameter space $\Theta$ to the output space $\mathcal{Y}.$ Then the objective of the inverse problem is to identify $\theta^{* }\in\Theta$ such that
<p>$$y^* = H(\theta^*) + \epsilon,$$</p>

where $\epsilon$ represents the unknown parameters $\theta\in\Theta.$ Practically, the inverse of $H$ is often intractable, hence we formulate the problem as a least squares problem:
<p>$$\theta^* = \underset{\theta\in\Theta}{\mathrm{argmin}}\Vert H(\theta) - y^*\Vert^2_{\mathcal{Y}},$$</p>

where $\Vert\cdot\Vert_\mathcal{Y}:\mathcal{Y}\to[0,\infty]$ is some norm that serves as a distance measure in space $\mathcal{Y}.$

The least squares problem can be very knotty, because the model $H$ is usually complicated or black-box, without a computable gradient or Hessian. The high computational cost of $H$ also rules out the possibility of precise numerical approximation. Moreover, multiple local minima could exist due to the noise-contaminated observations and possible information loss in the forward process of evaluating $H.$

## Bayesian Optimization
