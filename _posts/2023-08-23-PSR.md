# Proper Scoring Rules

We concentrate on the problem of probabilistic forecasts on a non-empty set $\Omega.$ Let $\mathcal{A}$ be a $\sigma$-algebra pf subsets of $\Omega,$ and let $\mathcal{P}$ be the convex class of probability measures on $(\Omega,\mathcal{A}).$ A function $f:\Omega\to\overline{\mathbb{R}}$ is said to be $\mathcal{P}$-quasi-integrable if it is $\mathcal{A}$-measurable and is quasi-integrable with respect to all $P\in\mathcal{P}.$ Any $P\in\mathcal{P}$ is called a probabilistic forecast.

A scoring rule is any extended real-valued function $S:\mathcal{P}\times\Omega\to\overline{\mathbb{R}}$ such that $S(P,\cdot)$ is $\mathcal{P}$-quasi-integrable for all $P\in\mathcal{P}.$ That is, the forecaster's reward is $S(P,\omega)$ with a realization of probabilistic forecast $P$ and event $\omega.$

## Convexity and Propriety
**Propriety.** Under a probability measure $Q,$ the expected score of forecast $P$ can be written as
<p>$$S(P,Q)=\int S(P,\omega)\mathrm{d}Q(\omega).$$</p>

Intuitively, we prefer a scoring rule that gains the highest score when the forecast $P$ is consistent with the real measure $Q.$ Therefore, the scoring rule is said to be proper relative to $\mathcal{P}$ if
<p>$$S(Q,Q)\leq S(P,Q)\ \ \forall P,Q\in\mathcal{P}.$$</p>

Moreover, $S$ is strictly proper if the equality holds if and only if $P=Q.$

**Convexity.** A function $G:\mathcal{P}\to\mathbb{R}$ is convex if
<p>$$G((1-\lambda)P_0 + \lambda P_1)\leq (1-\lambda)G(P_0) + \lambda G(P_1)\ \ \forall \lambda\in(0,1),\ P_0,P_1\in\mathcal{P},$$</p>

and $G$ is strictly convex if the equality holds if and only if $P_ 0=P_ 1.$

Analogous to the concept of subgradient, a function $G^*(P,\cdot):\Omega\to\mathbb{R}$ is a subtangent of $G$ at point $P\in\mathcal{P}$ if it is integrable with respect to $P$ and quasi-integrable with respect to all $Q\in\mathcal{P},$ and satisfies
<p>$$G(Q)\geq G(P) + \int G^*(P,\omega)\mathrm{d}[Q-P](\omega)$$</p>

for all $Q\in\mathcal{P}.$

**Characterization of proper scoring rules.** The connection between convexity and propriety is established as follows.

*Definition 1* (Regularity). A scoring rule $S:\mathcal{P}\times\Omega\to\overline{\mathbb{R}}$ is regular relative to $\mathcal{P}$ if $S(P,Q)$ is real-valued for all $P,Q\in\mathcal{P}$ except possibly $S(P,Q)=-\infty$ for some $P\neq Q.$

*Theorem 1.* A regular scoring rule $S:\mathcal{P}\times\Omega\to\overline{\mathbb{R}}$ is proper relative to $\mathcal{P}$ if and only if there exists a convex, real-valued function $G:\mathcal{P}\to\mathbb{R}$ such that
<p>$$S(P,\omega) = G(P) - \int G^*(P,\omega)\mathrm{d}P(\omega) + G^*(P,\omega)$$</p>

for $P\in\mathcal{P}$ and $\omega\in\Omega,$ where $G(P,\cdot):\Omega\to\overline{\mathbb{R}}$ is a subtangent of $G$ at point $P\in\mathcal{P}.$

*Proof.* Sufficiency: With the stated form of $S,$ the propriety $S(P,Q)\leq S(Q,Q)$ holds by the definition of subtangent.

Necessity: Suppose that $S$ is a regular proper scoring rule. Define $G:\mathcal{P}\to\mathbb{R}$ by $G(P) = S(P,P) = \sup_ {Q\in\mathcal{P}} S(Q,P).$ Since $S(Q,\cdot)$ is convex (and concave), $G$ as a pointwise supremum over a class of convex functions, is thus convex. Then, it can be verified that $S(P,\cdot):\Omega\to\overline{\mathbb{R}}$ is a subtangent of $G$ at the point $P.$ Furthermore, $S$ has the stated representation with $G^*(P,\omega) = S(P,\omega)$ plugged in.

*Remark.* An alternative statement of Theorem 1 is that a regular scoring rule $S$ is proper relative to the class $\mathcal{P}$ if and only if the expected score function $G(P) = S(P,P)$ is convex and $S(P,\omega)$ is a subtangent of $G$ at point $P$ for all $P\in\mathcal{P}.$ Moreover, strict propriety and strict convexity imply each other.

## Information Measures, Bregman Divergences and Decision Theory
Suppose that the scoring rule $S$ is proper relative to class $\mathcal{P},$ the expected score function
<p>$$G(P) = \sup_{Q\in\mathcal{P}} S(Q,P),\ \ P\in\mathcal{P},$$</p>

is referred to as the *information measure* or *generalized entropy function* associated with $S.$ 

Suppose that the scoring rule $S$ is regular and proper, then the divergence function associated with $S$ is defined as
<p>$$d(P,Q) = S(Q,Q) - S(P,Q),\ \ P,Q\in\mathcal{P}.$$</p>

The divergence is nonnegative, and if $S$ is strictly proper, then $d(P,Q)$ is strictly positive for $P\neq Q.$ If the sample space is finite and the entropy function is sufficiently smooth, then the divergence function becomes the *Bregman divergence* (Bregman 1967) associated with the convex function $G:$
<p>$$d(P,Q) = G(Q) - G(P) - \int G^*(P,\omega)\mathrm{d}[Q-P](\omega).$$</p>

Proper scores occur naturally in statistical decision problems. Given an outcome space and an action space, let $U(\omega,a)$ be the utility of outcome $\omega$ and action $a,$ and let $\mathcal{P}$ be a convex class of probability measures on the outcome space. Let $a_P$ denote the Bayes act for $P\in\mathcal{P}.$ Then the scoring rule
<p>$$S(P,\omega)=U(\omega,a_P)$$</p>

is proper relative to class $\mathcal{P}.$ Indeed, 
<p>$$S(Q,Q) = \int U(\omega,a_Q)\mathrm{d}Q(\omega) \geq \int U(\omega,a_P)\mathrm{d}Q(\omega) = S(P,Q)$$</p>

by the fact that Bayesian decision maximizes expected utility.

## Savage Representation
In this section we investigate the probabilistic forecasts of categorical variables, where the sample space $\Omega=\{1,\cdots,m\}$ consists of a finite number $m$ of mutually exclusive events, and a probabilistic forecast is a probability simplex $\mathbf{p}=(p_1,\cdots,p_m).$ Then the convex class $\mathcal{P}$ becomes
<p>$$\mathcal{P}_m = \{\mathbf{p}=(p_1,\cdots,p_m):p_1,\cdots,p_m\geq 0, p_1+\cdots+p_m=1\}.$$</p>

A scoring rule $S$ can be identified with a collection of $m$ functions:
<p>$$S(\cdot,i):\mathcal{P}_m\to\overline{\mathbb{R}},\ \ i=1,\cdots,m.$$</p>

Let $G:\mathcal{P}_m\to\mathbb{R}$ be a convex function, then a unique subgradient of $G$ at point $\mathbf{p}$ is
<p>$$\nabla G(\mathbf{p}) = \left(\frac{\partial}{\partial p_1}G(\mathbf{p}),\cdots,\frac{\partial}{\partial p_m}G(\mathbf{p})\right),$$</p>

if $G$ is differentiable and $\mathbf{p}$ is an interior point of $\mathcal{P}_ m.$ Generally, denote a subgradient of $G$ at $\mathbf{p}\in\mathcal{P}_ m$ by $\mathbf{G}^\prime(\mathbf{p}) = (G_ 1^\prime(\mathbf{p}),\cdots,G_ m^\prime(\mathbf{p}))^\top:$
<p>$$G(\mathbf{q})\geq G(\mathbf{p}) + \langle \mathbf{G}^\prime(\mathbf{p}),\mathbf{q}-\mathbf{p}\rangle\ \ \forall\mathbf{q}\in\mathcal{P}_m.$$</p>

The following statement is a special case of Theorem 1.

*Definition 2* (Regularity). A scoring rule $S$ for categorical forecasts is regular if $S(\cdots,i)$ is real-valued for $i=1,\cdots,m,$ except possibly that $S(\mathbf{p},i)=-\infty$ if $p_i=0.$

*Theorem 2* (McCarthy, Savage). A regular scoring rule $S$ for categorical forecasts is proper if and only if
<p>$$S(\mathbf{p},i) = G(\mathbf{p}) - \langle\mathbf{G}^\prime(\mathbf{p}),\mathbf{p}\rangle + G_i^\prime(\mathbf{p})\ \ \text{for}\ i=1,\cdots,m.$$</p>
