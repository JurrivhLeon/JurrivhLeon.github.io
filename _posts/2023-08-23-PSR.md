# Proper Scoring Rules

We concentrate on the problem of probabilistic forecasts on a non-empty set $\Omega.$ Let $\mathcal{A}$ be a $\sigma$-algebra pf subsets of $\Omega,$ and let $\mathcal{P}$ be the convex class of probability measures on $(\Omega,\mathcal{A}).$ A function $f:\Omega\to\overline{\mathbb{R}}$ is said to be $\mathcal{P}$-quasi-integrable if it is $\mathcal{A}$-measurable and is quasi-integrable with respect to all $P\in\mathcal{P}.$ Any $P\in\mathcal{P}$ is called a probabilistic forecast.

A scoring rule is any extended real-valued function $S:\mathcal{P}\times\Omega\to\overline{\mathbb{R}}$ such that $S(P,\cdot)$ is $\mathcal{P}$-quasi-integrable for all $P\in\mathcal{P}.$ That is, the forecaster's reward is $S(P,\omega)$ with a realization of probabilistic forecast $P$ and event $\omega.$

## Convexity and Propriety
**Propriety.** Under a probability measure $Q,$ the expected score of forecast $P$ can be written as
<p>$$S(P,Q)=\int S(P,\omega)\mathrm{d}Q(\omega).$$</p>

Intuitively, we prefer a scoring rule that gains the highest score when the forecast $P$ is consistent with the real measure $Q.$ Therefore, the scoring rule is said to be proper relative to $\mathcal{P}$ if
<p>$$S(Q,Q)\leq S(P,Q)\ \ \forall P,Q\in\mathcal{P}.$$</p>

Moreover, $S$ is strictly proper if the equality holds if and only if $P=Q.$

**Convexity.** A function $G:\mathcal{P}\to\mathbb{R}$ is convex if
<p>$$G((1-\lambda)P_0 + \lambda P_1)\leq (1-\lambda)G(P_0) + \lambda G(P_1)\ \ \forall \lambda\in(0,1),\ P_0,P_1\in\mathcal{P},$$</p>

and $G$ is strictly convex if the equality holds if and only if $P_ 0=P_ 1.$

Analogous to the concept of subgradient, a function $G^*(P,\cdot):\Omega\to\mathbb{R}$ is a subtangent of $G$ at point $P\in\mathcal{P}$ if it is integrable with respect to $P$ and quasi-integrable with respect to all $Q\in\mathcal{P},$ and satisfies
<p>$$G(Q)\geq G(P) + \int G^*(P,\omega)\mathrm{d}[Q-P](\omega)$$</p>

for all $Q\in\mathcal{P}.$

**Characterization of proper scoring rules.** The connection between convexity and propriety is established as follows.

*Definition 1* (Regularity). A scoring rule $S:\mathcal{P}\times\Omega\to\overline{\mathbb{R}}$ is regular relative to $\mathcal{P}$ if $S(P,Q)$ is real-valued for all $P,Q\in\mathcal{P}$ except possibly $S(P,Q)=-\infty$ for some $P\neq Q.$

*Theorem 1.* A regular scoring rule $S:\mathcal{P}\times\Omega\to\overline{\mathbb{R}}$ is proper relative to $\mathcal{P}$ if and only if there exists a convex, real-valued function $G:\mathcal{P}\to\mathbb{R}$ such that
<p>$$S(P,\omega) = G(P) - \int G^*(P,\omega)\mathrm{d}P(\omega) + G^*(P,\omega)$$</p>

for $P\in\mathcal{P}$ and $\omega\in\Omega,$ where $G(P,\cdot):\Omega\to\overline{\mathbb{R}}$ is a subtangent of $G$ at point $P\in\mathcal{P}.$

*Proof.* Sufficiency: With the stated form of $S,$ the propriety $S(P,Q)\leq S(Q,Q)$ holds by the definition of subtangent.

Necessity: Suppose that $S$ is a regular proper scoring rule. Define $G:\mathcal{P}\to\mathbb{R}$ by $G(P) = S(P,P) = \sup_ {Q\in\mathcal{P}} S(Q,P).$ Since $S(Q,\cdot)$ is convex (and concave), $G$ as a pointwise supremum over a class of convex functions, is thus convex. Then, it can be verified that $S(P,\cdot):\Omega\to\overline{\mathbb{R}}$ is a subtangent of $G$ at the point $P.$ Furthermore, $S$ has the stated representation with $G^*(P,\omega) = S(P,\omega)$ plugged in.

*Remark.* An alternative statement of Theorem 1 is that a regular scoring rule $S$ is proper relative to the class $\mathcal{P}$ if and only if the expected score function $G(P) = S(P,P)$ is convex and $S(P,\omega)$ is a subtangent of $G$ at point $P$ for all $P\in\mathcal{P}.$ Moreover, strict propriety and strict convexity imply each other.

## Information Measures, Bregman Divergences and Decision Theory
Suppose that the scoring rule $S$ is proper relative to class $\mathcal{P},$ the expected score function
<p>$$G(P) = \sup_{Q\in\mathcal{P}} S(Q,P),\ \ P\in\mathcal{P},$$</p>

is referred to as the *information measure* or *generalized entropy function* associated with $S.$ 

Suppose that the scoring rule $S$ is regular and proper, then the divergence function associated with $S$ is defined as
<p>$$d(P,Q) = S(Q,Q) - S(P,Q),\ \ P,Q\in\mathcal{P}.$$</p>

The divergence is nonnegative, and if $S$ is strictly proper, then $d(P,Q)$ is strictly positive for $P\neq Q.$ If the sample space is finite and the entropy function is sufficiently smooth, then the divergence function becomes the *Bregman divergence* (Bregman 1967) associated with the convex function $G:$
<p>$$d(P,Q) = G(Q) - G(P) - \int G^*(P,\omega)\mathrm{d}[Q-P](\omega).$$</p>

Proper scores occur naturally in statistical decision problems. Given an outcome space and an action space, let $U(\omega,a)$ be the utility of outcome $\omega$ and action $a,$ and let $\mathcal{P}$ be a convex class of probability measures on the outcome space. Let $a_P$ denote the Bayes act for $P\in\mathcal{P}.$ Then the scoring rule
<p>$$S(P,\omega)=U(\omega,a_P)$$</p>

is proper relative to class $\mathcal{P}.$ Indeed, 
<p>$$S(Q,Q) = \int U(\omega,a_Q)\mathrm{d}Q(\omega) \geq \int U(\omega,a_P)\mathrm{d}Q(\omega) = S(P,Q)$$</p>

by the fact that Bayesian decision maximizes expected utility.
