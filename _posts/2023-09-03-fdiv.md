# f-Divergences
## Formulation
**Definition.** Let $P$ and $Q$ be two probability measures on a non-empty set $\mathcal{X}.$ Then for any convex function $f:\mathbb{R}^+\to\mathbb{R}$ such that $f(1)=0$ and $f$ is strictly convex at $1.$ Suppose that $Q$ is absolutely continuous with respect to $P.$ Then the $f$-divergence of $Q$ with respect to $P$ is defined as
<p>$$D_f(Q\Vert P) := \int f\left(\frac{\mathrm{d}Q}{\mathrm{d}P}\right)\mathrm{d}P(x),$$</p>

where the notation $\frac{\mathrm{d}Q}{\mathrm{d}P}$ stands for the Radon-Nikodym derivative of $Q$ with respect to $P.$

**Remark.** In practice, there are two more convenient forms of $f$-divergence.
+ When $\mathcal{X}$ is discrete,
  <p>$$D_f(Q\Vert P) = \sum_{x\in\mathcal{X}}f\left(\frac{Q(x)}{P(x)}\right)P(x).$$</p>
  
+ When $P$ and $Q$ are specified by density functions $p$ and $q,$ respectively, then
  <p>$$D_f(q\Vert p) = \int f\left(\frac{q(x)}{p(x)}\right)p(x)\mathrm{d}x.$$</p>

### Properties of $f$-divergences
+ **(Non-negativity).** $D_f(Q\Vert P)\geq 0$ with equality holds if and only if $P=Q.$
  
  *Proof.* From Jensen's inequality, the convexity of $f$ implies
  <p>$$D_f(Q\Vert P) = \mathbb{E}_P\left[f\left(\frac{\mathrm{d}Q}{\mathrm{d}P}\right)\right] \geq f\left(\mathbb{E}_P\left[\frac{\mathrm{d}Q}{\mathrm{d}P}\right]\right) = f(1) = 0.$$</p>
  Plus the strict convexity of $f$ at $1,$ the equality holds if and only if $P=Q.$

+ **(Joint convexity).** $(Q,P)\mapsto D_f(Q\Vert P)$ is a jointly convex function.

  *Proof.* Let $f:\mathbb{R}^+\to\mathbb{R}$ be convex in the open set $(0,+\infty).$ Then the perspective $(p,q)\mapsto pf(q/p)$ of $f,$ defined on $\mathbb{R}_ {+}^2,$ is also convex. Furthermore, $D_ f(Q\Vert P)$ as the integral taken on a collection of perspectives, is jointly convex.
  
+ **(Conditional increment).** Define the conditional $f$-divergence:
  <p>$$D_f(Q_{Y|X}\Vert P_{Y|X}|P_X):= \mathbb{E}_{X\sim P_X}\left[D_f(Q_{Y|X}\Vert P_{Y|X})\right].$$ </p>

  Let $P_Y = P_{Y\vert X}P_X,$ $Q_Y = Q_{Y\vert X}P_X,$  then
  <p>$$D_f(Q_Y\Vert P_Y)\leq D_f(Q_{Y|X}\Vert P_{Y|X}|P_X).$$</p>

  This is a specific form of Jensen's inequality.

## Variational Representation
The variational form of $f$-divergence is based on the Fenchel conjugate of $f,$ which is defined as $f^*(t) = \sup_{x\in\mathbb{R}^+}\lbrace tx - f(x)\rbrace,\ t\in\mathbb{R}.$ The $f$-divergence admits the following variational formulation.

**Lemma 1** (Variational representation of $f$-divergence). Let $\Phi$ be a convex class of measurable functions $\phi:\mathcal{X}\to\mathbb{R}.$ Suppose $f$ is differentiable. Then
<p>$$D_f(Q\Vert P) \geq \sup_{\phi\in\Phi} \left\lbrace\mathbb{E}_{Z\sim Q}[\phi(Z)] - \mathbb{E}_{X\sim P}[f^*(\phi(X))]\right\rbrace,$$</p>

where the equality holds if and only if $f'(\mathrm{d}Q/\mathrm{d}P)\in\Phi$ and the supremum is reached at $\phi^* = f'(\mathrm{d}Q/\mathrm{d}P).$

*Proof.* We fix the measurable function $\phi\in\Phi.$ By Fenchel's duality, we have
<p>$$\phi(x)\frac{\mathrm{d}Q(x)}{\mathrm{d}P(x)} - f\left(\frac{\mathrm{d}Q(x)}{\mathrm{d}P(x)}\right)\leq f^*(\phi(x)).$$</p>

Take integration with respect to $P$ on both sides of the equation above, we have
<p>$$\mathbb{E}_{Z\sim Q}[\phi(Z)] - D_f(Q\Vert P) \leq \mathbb{E}_{X\sim P}[f^*(\phi(X))].$$</p>

Since $\phi$ is arbitrarily chosen, we immediately conclude the inequality in Lemma 1. 

## Data Processing Inequality
We consider a channel that generates a random variable $Y$ given $X$ based on a conditional distribution $P_{Y\vert X}.$ 

**Theorem 1** (Data processing inequality). Fix the conditional distribution $P_ {Y\vert X}$ of $Y$ given $X.$ Denote by $P_Y$ the marginal distribution of $Y$ when $X$ is generated from distribution $P_X,$ and $Q_Y$ when $X$ is from $Q_X.$ Then for any $f$-divergence $D_f(\cdot\Vert\cdot),$
<p>$$\begin{align*}
  D_f(Q_Y\Vert P_Y) \leq D_f(Q_X\Vert P_X).
  \end{align*}$$</p>

*Proof.* Denote the joint distribution of $X$ and $Y$ by $P_ {XY}$ when $X$ is generated from $P_X,$ and $Q_ {XY}$ from $Q_ X.$ Since the conditional distribution $P_ {Y\vert X}$ is fixed, we have
<p>$$\begin{align*}
  D_f(Q_X\Vert P_X) = D_f(Q_{XY}\Vert P_{XY}).
\end{align*}$$</p>

Since $f$ is convex, we have from Jensen's inequality that
<p>$$\begin{align*}
  D_f(Q_{XY}\Vert P_{XY}) &= \mathbb{E}_{X,Y\sim P_{XY}}\left[f\left(\frac{\mathrm{d}Q_{XY}}{\mathrm{d}P_{XY}}\right)\right]\\
  &= \mathbb{E}_{Y\sim P_Y}\left[\mathbb{E}_{X\sim P_{X\vert Y}}\left[f\left(\frac{\mathrm{d}Q_{XY}}{\mathrm{d}P_{XY}}\right)\right]\right]\\
  &\geq \mathbb{E}_{Y\sim P_Y}\left[f\left(\mathbb{E}_{X\sim P_{X|Y}}\left[\frac{\mathrm{d}Q_{XY}}{\mathrm{d}P_{XY}}\right]\right)\right]\\
  &= \mathbb{E}_{Y\sim P_Y}\left[f\left(\frac{\mathrm{d}Q_{Y}}{\mathrm{d}P_{Y}}\right)\right] = D_f(Q_Y\Vert P_Y).
\end{align*}$$</p>

**Remark.** An intuitive interpretation of this inequality is that processing $X$ makes it more difficult to distinguish between $P_X$ and $Q_X.$ If we take $P_{Y\vert X}$ to be a Dirac measure, then there is a deterministic map $f$ such that $Y=f(X)$ with probability 1. 

## Total Variation
Let $f(x)=\frac{1}{2}\vert x-1\vert,$ we obtain the total variation:
<p>$$d_{\mathrm{TV}}(P,Q) = D_f(Q\Vert P) = \frac{1}{2}\int\left\vert\frac{\mathrm{d}Q}{\mathrm{d}P}-1\right\vert\mathrm{d}P = \frac{1}{2}\int\vert\mathrm{d}Q-\mathrm{d}P\vert.$$</p>

Note that $d_{\mathrm{TV}}(\cdot,\cdot)$ is symmetric and bounded in $[0,1].$

### Properties of total variation
Denote by $(\mathcal{X},\Sigma)$ the underlying measurable space in our discussion. $P$ and $Q$ are two probability measures.

+ $d_{\mathrm{TV}}(P,Q) = \sup_{\mathcal{E}\in\Sigma} P(\mathcal{E}) - Q(\mathcal{E}),$ where the supremum is taken over all measurable sets.

  *Proof.* Since $P$ and $Q$ are two probability measure on $(\mathcal{X},\Sigma),$ $P-Q$ is a finite signed measure on $(\mathcal{X},\Sigma).$ From Hahn decomposition theorem, there exists two measurable sets $\mathcal{P}$ and $\mathcal{N}$ such that
  1. $\mathcal{P}\cup\mathcal{N}=\mathcal{X}$ and $\mathcal{P}\cap\mathcal{N}=\emptyset,$
  2. $P(\mathcal{E}) - Q(\mathcal{E}) \geq 0$ for any $\mathcal{E}\in\Sigma$ with $\mathcal{E}\subset\mathcal{P},$ and
  3. $P(\mathcal{E}) - Q(\mathcal{E}) \leq 0$ for any $\mathcal{E}\in\Sigma$ with $\mathcal{E}\subset\mathcal{N}.$

  For any $\mathcal{E}\in\Sigma,$ since $\mathcal{P}$ and $\mathcal{N}$ is a disjoint cover of $\mathcal{X},$ we have
  <p>$$\begin{align*}
    P(\mathcal{E}) - Q(\mathcal{E}) &= \int_{\mathcal{E}} \mathrm{d}[P-Q]\\
    &= \int_{\mathcal{E}\cap\mathcal{P}} \mathrm{d}[P-Q] + \int_{\mathcal{E}\cap\mathcal{N}} \mathrm{d}[P-Q]\\
    &\leq \int_{\mathcal{E}\cap\mathcal{P}} \mathrm{d}[P-Q]\\
    &\leq \int_{\mathcal{P}} \mathrm{d}[P-Q].
  \end{align*}$$</p>

  Moreover, we have for the total variation that
  <p>$$\begin{align*}
    d_{\mathrm{TV}}(P,Q) &= \frac{1}{2}\int_{\mathcal{P}}\mathrm{d}[P-Q] - \frac{1}{2}\int_{\mathcal{N}}\mathrm{d}[P-Q]\\
    &= \int_{\mathcal{P}}\mathrm{d}[P-Q].
  \end{align*}$$</p>

  Hence $d_{\mathrm{TV}}(P,Q) = \sup_{\mathcal{E}\in\Sigma} P(\mathcal{E}) - Q(\mathcal{E}),$ where the supremum is achieved when $\mathcal{E}=\mathcal{P}.$

+ Let $\mathcal{F}=\lbrace f:\mathcal{X}\to\mathbb{R},\Vert f\Vert_\infty \leq 1\rbrace.$ Then
  <p>$$d_{\mathrm{TV}}(P,Q) = \frac{1}{2}\sup_{f\in\mathcal{F}} \int f\mathrm{d}P-f\mathrm{d}Q.$$</p>

  It can be viewed as  a direct corollary from Holder's inequality.
  
## Inequalities between $f$-divergences
### Pinsker's Inequality
The Pinsker's inequality gives a bound of total variation in terms of the Kullback-Leibler divergence.

**Theorem 2** (Pinsker's inequality). If $P$ and $Q$ are two probability measures on a measurable space $(\mathcal{X},\mathscr{F}),$ then
<p>$$d_{\mathrm{TV}}(P,Q) \leq \sqrt{2D_{\mathrm{KL}}(Q\Vert P)}$$</p>

*Proof.* For an arbitrary set $\mathcal{E},$ define $Y=\mathbb{1}\lbrace X\in\mathcal{E}\rbrace.$ Then we have from the data processing inequality that
<p>$$d_\mathrm{TV}$$</p>

**Theorem 3** (The Breyagnolle-Huber bound). If $P$ and $Q$ are two probability measures on a measurable space $(\mathcal{X},\mathscr{F}),$ then
<p>$$d_{\mathrm{TV}}(P,Q) \leq \sqrt{1 - \exp\left(-D_{\mathrm{KL}}(Q\Vert P)\right)}$$</p>
