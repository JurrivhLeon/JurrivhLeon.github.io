# f-Divergences
## Definition and Properties
**Definition.** Let $P$ and $Q$ be two probability measures on a non-empty set $\mathcal{X}.$ Then for any convex function $f:\mathbb{R}^+\to\mathbb{R}$ such that $f(1)=0$ and $f$ is strictly convex at $1.$ Suppose that $Q$ is absolutely continuous with respect to $P.$ Then the $f$-divergence of $Q$ with respect to $P$ is defined as
<p>$$D_f(Q\Vert P) := \int f\left(\frac{\mathrm{d}Q}{\mathrm{d}P}\right)\mathrm{d}P(x),$$</p>

where the notation $\frac{\mathrm{d}Q}{\mathrm{d}P}$ stands for the Radon-Nikodym derivative of $Q$ with respect to $P.$

**Remark.** In practice, there are two more convenient forms of $f$-divergence.
+ When $\mathcal{X}$ is discrete,
  <p>$$D_f(Q\Vert P) = \sum_{x\in\mathcal{X}}f\left(\frac{Q(x)}{P(x)}\right)P(x).$$</p>
  
+ When $P$ and $Q$ are specified by density functions $p$ and $q,$ respectively, then
  <p>$$D_f(q\Vert p) = \int f\left(\frac{q(x)}{p(x)}\right)p(x)\mathrm{d}x.$$</p>

### Properties of $f$-divergences
+ **(Non-negativity).** $D_f(Q\Vert P)\geq 0$ with equality holds if and only if $P=Q.$
  
  *Proof.* From Jensen's inequality, the convexity of $f$ implies
  <p>$$D_f(Q\Vert P) = \mathbb{E}_P\left[f\left(\frac{\mathrm{d}Q}{\mathrm{d}P}\right)\right] \geq f\left(\mathbb{E}_P\left[\frac{\mathrm{d}Q}{\mathrm{d}P}\right]\right) = f(1) = 0.$$</p>
  Plus the strict convexity of $f$ at $1,$ the equality holds if and only if $P=Q.$

+ **(Joint convexity).** $(Q,P)\mapsto D_f(Q\Vert P)$ is a jointly convex function.
+ **(Conditional increment).** Define the conditional $f$-divergence:
  <p>$$D_f(Q_{Y|X}\Vert P_{Y|X}|P_X):= \mathbb{E}_{X\sim P_X}\left[D_f(Q_{Y|X}\Vert P_{Y|X})\right].$$ </p>

  Let $P_Y = P_{Y\vert X}P_X,$ $Q_Y = Q_{Y\vert X}P_X,$  then
  <p>$$D_f(Q_Y\Vert P_Y)\leq D_f(Q_{Y|X}\Vert P_{Y|X}|P_X).$$</p>
  
## Inequalities between $f$-divergences
