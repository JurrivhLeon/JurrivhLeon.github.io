# Fano's Method
## Mutual Information
**Definition.** Given two random variables $X,Y$ and their joint distribution $P_ {XY},$ the mutual information between $X$ and $Y$ is defined as
<p>$$I(X;Y):=D_{\mathrm{KL}}(P_{XY}\Vert P_XP_Y),$$</p>

which is the KL divergence of $P_{XY}$ with respect to a hypothetical distribution assuming $X$ and $Y$ are independent.

**Properties of mutual information.** The following statements are true:
+ $I(X;Y) = D_ {\mathrm{KL}}(P_ {Y\vert X}\Vert P_ Y\vert P_ X) = \mathbb{E}_ {X\sim P_ X}[D_{\mathrm{KL}}(P_{Y\vert X}\Vert P_Y)];$
+ *(Symmetry)* $I(X;Y) = I(Y;X);$
+ *(Measure of dependency)* $I(X;Y)\geq 0.$ The equality holds if and only if $X\perp Y.$

Akin to the property of $f$-divergence, we have the data processing inequality for mutual information applied on Markov chains:

**Theorem 1.** Let $X\to Y\to Z$ form a Markov chain, i.e., $Z\perp X\ \vert\ Y.$ Then
<p>$$I(X;Z)\leq I(X;Y).$$</p>

*Proof.* From Markovian property, we have $P_ {Z\vert X} = P_ {Z\vert Y}P_ {Y\vert X}.$ Then it follows from the data processing inequality for Kullback-Leibler divergence that
<p>$$D_{\mathrm{KL}}(P_{Z\vert X}\Vert P_Z) \leq D_{\mathrm{KL}}(P_{Y\vert X}\Vert P_Y).$$</p>

Taking expectation with respect to $X\sim P_X$ on both sides of this equality yields the result.

## Fano's Inequality
Let $\mathcal{P}$ be a set of distributions, and let $\mathbf{X}=\lbrace X_ 1,\cdots,X_ n\rbrace$ be i.i.d. from some unknown $P\in\mathcal{P}.$ We are interested in recovering $P$ from our observation $\mathbf{X}.$

Now we consider a problem of hypothesis testing. $\mathcal{S}=\lbrace P_1,\cdots,P_m\rbrace\subset\mathcal{P}.$
