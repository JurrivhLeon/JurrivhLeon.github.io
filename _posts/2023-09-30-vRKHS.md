# Vector-Valued RKHS
## Motivation
**Review: Real (or Complex) RKHS.**  Let $\mathcal{X}$ be a non-empty set, and let $\mathcal{H}$ be a Hilbert space of functions $h:\mathcal{X}\to\mathbb{R}$ (or $\mathbb{C}$) equipped with inner product $\langle\cdot,\cdot\rangle_\mathcal{H}.$ Then $\mathcal{H}$ is a reproducing kernel Hilbert space, provided that the evaluation functional $\delta_x:\mathcal{H}\to\mathbb{R}$ (or $\mathbb{C}$), $f\mapsto f(x)$ is bounded for all $x\in\mathcal{X}.$

By the Riesz's representation theorem, for every $x\in\mathcal{X},$ the boundedness (or continuity, equivalently) of linear $\delta_x$ implies that there exists a unique $k_x\in\mathcal{H}$ such that $f(x) = \delta_x f = \langle f,k_x\rangle_\mathcal{H}$ for all $f\in\mathcal{H}.$ Then we define $k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ (or $\mathbb{C}$), $(x,x^\prime)\mapsto k_{x^\prime}(x).$ We can verify the following two properties of $k$:
+ For all $x\in\mathcal{X},$ the function $k(\cdot,x):\mathcal{X}\to\mathbb{R}$ (or $\mathbb{C}$) is in $\mathcal{H}.$ To see this, just note that $k(\cdot,x)=k_x.$
+ For all $f\in\mathcal{H}$ and $x\in\mathcal{X},$ $\langle f,k(\cdot,x)\rangle_\mathcal{H} = \langle f,k_x\rangle = \delta_x f = f(x).$ This property is referred to as the *reproducing property*.

We call the function $k:\mathcal{X}\to\mathcal{X}\to\mathbb{R}$ (or $\mathbb{C}$) a kernel on $\mathcal{X}.$ It possesses the following properties (For brevity we only discuss $\mathbb{C}$):
+ (Symmetry / Conjugate symmetry). $k(x,x^\prime) = \overline{k(x^\prime,x)}$ for all $x,x^\prime\in\mathcal{X}.$
+ (Positive definiteness). For all $n\in\mathbb{N},\ x_1,\cdots,x_n\in\mathcal{X},\ \alpha_1,\cdots,\alpha_n\in\mathbb{C},$ we have
  <p>$$\sum_{i=1}^n\sum_{j=1}^n \alpha_i\overline{\alpha_j}k(x_i,x_j) \geq 0.$$</p>

In our discussion, we only consider the case for real and complex-valued functions. However, RHKS can be defined for more classes of functions.

**Definition 1** (Vector-valued RKHS, vRKHS). Let $\mathcal{H}$ be a Hilbert space of functions $h:\mathcal{X}\to\mathcal{G}$ equipped with inner product $\langle\cdot,\cdot\rangle_\mathcal{H},$ where $\mathcal{G}$ is also a Hilbert space equipped with inner product $\langle\cdot,\cdot\rangle_\mathcal{G}.$ Then $\mathcal{H}$ is a RKHS, provided thay the linear functional $\delta_{x,g}:\mathcal{H}\to\mathbb{C},\ f\mapsto\langle g,f(x)\rangle_\mathcal{G}$ is bounded for all $x\in\mathcal{X}$ and $g\in\mathcal{G}.$

By the Riesz's representation theorem, for every $x\in\mathcal{X}$ and $g\in\mathcal{G},$ the boundedness of linear $\delta_{x,g}$ implies that there exists a unique $k_{x,g}\in\mathcal{H}$ such that $\langle g,f(x)\rangle_\mathcal{G} = \delta_{x,g} f = \langle f,k_{x,g}\rangle_\mathcal{H}$ for all $f\in\mathcal{H}.$ Moreover, it is easy to see that $k_{x,g}$ is linear with respect to $g.$ Then for each $x\in\mathcal{X},$ we define $k_x:\mathcal{G}\to\mathcal{H}, g\mapsto k_{x,g}.$

Denote by $\mathcal{L}(\mathcal{G})$ the space of all bounded linear operators from $\mathcal{G}$ to $\mathcal{G}.$ We define the kernel $k:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{G})$ as
<p>$$k(x,x^\prime)g = (k_{x^\prime} g)(x)\in\mathcal{G}.$$</p>

All aforementioned properties of RKHS holds:
+ (Compatibility). For all $x\in\mathcal{X},$ $k(\cdot,x)=k_x\in\mathcal{H}.$
+ (Reproducing property). For all $f\in\mathcal{H},\ x\in\mathcal{X}$ and $g\in\mathcal{G},$ $\langle f,k(\cdot,x)g\rangle_\mathcal{H} = \langle f,k_xg\rangle_\mathcal{H} = \langle g,f(x)\rangle_\mathcal{G}.$
+ (Adjoint symmetry). For all $x,x^\prime\in\mathcal{X},$ $k(x,x^\prime) = k(x^\prime, x)^*,$ the Hermitian adjoint operator. To see this, just note that
  <p>$$\langle k(x,x^\prime)g,g^\prime\rangle_\mathcal{G} = \overline{\langle g^\prime,(k_{x^\prime}g)(x)\rangle_\mathcal{G}} = \overline{\langle k_{x^\prime}g,k_{x}g^\prime\rangle_\mathcal{H}} = \langle k_{x}g^\prime,k_{x^\prime}g\rangle_\mathcal{H} = \langle g,k(x^\prime,x)g^\prime\rangle_\mathcal{G}.$$</p>
+ (Positive definiteness). For all $n\in\mathcal{N},\ x_1,\cdots,x_n\in\mathcal{X}$ and  $g_1,\cdots,g_n\in\mathcal{G},$
  <p>$$\sum_{i=1}^n\sum_{j=1}^n \langle g_i,k(x_i,x_j)g_j\rangle_\mathcal{G} = \sum_{i=1}^n\sum_{j=1}^n \langle k_{x_j}g_j,k_{x_i}g_i\rangle_\mathcal{H}\geq 0.$$</p>

Like the Moore-Aronszajn theorem, we can also establish an one-to-one correspondence between the positive kernel and the vector-valued RKHS. A formal statement is given below.

**Theorem 1** (Micchelli-Pontil). Let $k:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{G})$ be a positive definite kernel. Then there exists a unique (up to an isometry) RKHS which admits $k$ as the reproducing kernel.

Furthermore, we can construct a vRKHS as the closure of $\mathrm{span}\lbrace k_x g:x\in\mathcal{X},g\in\mathcal{G}\rbrace.$

## Properties
Like in real and complex-valued RKHS's, we have some inequalities between different norms for vRKHS.
**Proposition.** The following statements are true:
+ $\Vert k_x\Vert = \Vert k(x,x)\Vert^{1/2}\ \forall x\in\mathcal{X}.$
+ $\Vert k(x,x^\prime)\Vert \leq \Vert k(x,x)\Vert^{1/2}\Vert k(x^\prime,x^\prime)\Vert^{1/2}\ \forall x,x^\prime\in\mathcal{X}.$
+ $\Vert f(x)\Vert_\mathcal{G} \leq \Vert f\Vert_\mathcal{H}\Vert k(x,x)\Vert^{1/2}\ \forall f\in\mathcal{H},\ x\in\mathcal{X}.$
+ Suppose $\sup_{x\in\mathcal{X}} \Vert k(x,x)\Vert \leq B.$ Then $\Vert f(x)\Vert_\mathcal{G}\leq\sqrt{B}\Vert f\Vert_\mathcal{H}.$

*Proof.* (i) Fix $x\in\mathcal{X}.$ Then for any $g\in\mathcal{G},$ $\Vert k_xg\Vert_\mathcal{H}^2=\langle k_xg,k_xg\rangle_\mathcal{H} = \langle g,k(x,x)g\rangle_\mathcal{G}$
<p>$$\begin{align*}\Vert k_x\Vert = \sup_{g\in\mathcal{G}\backslash\lbrace 0\rbrace}\frac{\Vert k_xg\Vert_\mathcal{H}}{\Vert g\Vert_\mathcal{G}} &= \sup_{g\in\mathcal{G}\backslash\lbrace 0\rbrace}\frac{\sqrt{\langle g,k(x,x)g\rangle_\mathcal{G}}}{\Vert g\Vert_\mathcal{G}}\\
  &\leq \sup_{g\in\mathcal{G}\backslash\lbrace 0\rbrace}\sqrt{\frac{\Vert k(x,x)g\Vert_\mathcal{G}}{\Vert g\Vert_\mathcal{G}}} = \Vert k(x,x)\Vert^{1/2}.\end{align*}$$</p>

Meanwhile,
<p>$$\begin{align*}\Vert k(x,x)g\Vert_\mathcal{G}^2 &= \langle k(x,x)g, (k_xg)(x)\rangle_\mathcal{G} = \langle k_xg,k_x k(x,x)g\rangle_\mathcal{H}\\
  &\leq\Vert k_xg\Vert_\mathcal{H}\Vert k_xk(x,x)g\Vert_\mathcal{H}\leq\Vert k_x\Vert^2\Vert g\Vert_\mathcal{G}\Vert k(x,x)g\Vert_\mathcal{G}.\end{align*}$$</p>

Then
<p>$$\Vert k(x,x)\Vert = \sup_{g\in\mathcal{G}\backslash\lbrace 0\rbrace} \frac{\Vert k(x,x)g\Vert_\mathcal{G}}{\Vert g\Vert_\mathcal{G}} \leq \Vert k_x\Vert^2,$$</p>

which concludes the proof.

(ii) Note that $\forall x,x^\prime\in\mathcal{X},\ g\in\mathcal{G},$
<p>$$\begin{align*}\Vert k(x,x^\prime)g\Vert_\mathcal{G}^2 &= \langle k(x,x^\prime)g, (k_{x^\prime}g)(x)\rangle_\mathcal{G} = \langle k_{x^\prime}g, k_xk(x,x^\prime)g\rangle_\mathcal{H}\\
  &\leq \Vert k_{x^\prime}g\Vert_\mathcal{H}^2\Vert k_xk(x,x^\prime)g\Vert_\mathcal{H}^2 \leq \Vert k_x\Vert\Vert k_{x^\prime}\Vert\Vert g\Vert_\mathcal{G}\Vert k(x,x)g\Vert_\mathcal{G}\end{align*}.$$</p>

Then
<p>$$\Vert k(x,x^\prime)\Vert = \sup_{g\in\mathcal{G}\backslash\lbrace 0\rbrace} \frac{\Vert k(x,x^\prime)g\Vert_\mathcal{G}}{\Vert g\Vert_\mathcal{G}} \leq \Vert k_x\Vert\Vert k_{x^\prime}\Vert \leq \Vert k(x,x)\Vert^{1/2}\Vert k(x^\prime,x^\prime)\Vert^{1/2}.$$</p>

(iii) For any $f\in\mathcal{H}$ and $x\in\mathcal{X},$ we have
<p>$$\begin{align*}\langle f(x),f(x)\rangle_\mathcal{G} = \langle f,k_xf(x)\rangle_\mathcal{H} \leq \Vert f\Vert_\mathcal{H}\Vert k_xf(x)\Vert_\mathcal{H}\leq \Vert f\Vert_\mathcal{H}\Vert k_x\Vert\Vert f(x)\Vert_\mathcal{G}.\end{align*}$$</p>

Then
<p>$$\Vert f(x)\Vert_\mathcal{G} \leq \Vert f\Vert_\mathcal{H}\Vert k_x\Vert = \Vert f\Vert_\mathcal{H}\Vert k(x,x)\Vert^{1/2}.$$</p>

(iv) immediately follows from (iii).

## Vector-valued Regression
Now suppose we are given an observational dataset $\lbrace(x_i,g_i)\rbrace_{i=1}^n.$ It is possible to perform a regression in the RKHS setting. Suppose $\mathcal{H}$ is the RKHS corresponding to the kernel $k:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{G}).$ Inspired by the kernel ridge regression, we propose the following risk functional:
<p>$$\widehat{\mathcal{R}}_\lambda[f] = \sum_{i=1}^n \Vert g_i - f(x_i)\Vert_\mathcal{G}^2 + \lambda\Vert f\Vert_\mathcal{H}^2,\ \lambda > 0.$$</p>

The first term stands for the empirical error, and the second is for regularization. 

We have the following representer theorem:

**Theorem.** If $f^*$ minimizes $\widehat{\mathcal{R}}_\lambda$ in $\mathcal{H},$ then it is unique and has the form,
<p>$$f^* = \sum_{j=1}^n k_{x_j}c_j,$$</p>

where the coefficients $\lbrace c_ j\rbrace_ {j\in[n]}\subset\mathcal{G}$ are the unique solution of the linear system
<p>$$\sum_{j=1}^n \left(k(x_i,x_j) + \delta_{ij}\lambda\right)c_j = g_i,\ i\in[n].$$</p>

*Proof.* By definition, the fitted value is $f^{\*}(x_ i) = \sum_ {j=1}^n k(x_ i,x_ j)c_ j,$ with $g_ i - f^{\*}(x_ i) = \lambda c_ i.$

Let $f$ be any element of $\mathcal{H}.$ Set $h=f-f^*,$ we have
<p>$$\begin{align*}\langle g_i-f^*(x_i), h(x_i)\rangle_\mathcal{G} &= \lambda\left\langle c_i, f(x_i) - \sum_{j=1}^n k(x_i,x_j)c_j\right\rangle_\mathcal{G},\\
\langle h,f^*\rangle_\mathcal{H} &= \sum_{j=1}^n\langle f,k_{x_j}c_j\rangle_\mathcal{H} - \sum_{i=1}^n\sum_{j=1}^n\langle k_{x_i}c_i, k_{x_j}c_j\rangle_\mathcal{H}\\
  &= \sum_{j=1}^n\langle c_i,f(x_i)\rangle_\mathcal{G} - \sum_{i=1}^n\sum_{j=1}^n\langle c_i,k(x_i,x_j)c_j\rangle_\mathcal{G}.\end{align*}$$</p>
  
For brevity, we apply a matrix representation. Let $\mathbf{K} = \lbrace k(x_ i,x_ j)\rbrace_ {i,j=1}^n,$ $\mathbf{c} = (c_1,\cdots,c_n)^\top,$ $\mathbf{f} = (f(x_1),\cdots,f(x_n))^\top.$ Then
<p>$$\sum_{i=1}^n \langle g_i-f^*(x_i), h(x_i)\rangle_\mathcal{G} = \lambda\langle h,f^*\rangle_\mathcal{H} = \lambda(\mathbf{f}^*\mathbf{c} - \mathbf{c}^*\mathbf{Kc}).$$</p>

Now consider the risk functional:
<p>$$\begin{align*}\widehat{\mathcal{R}}_\lambda(f) - \widehat{\mathcal{R}}_\lambda(f^*) &= -2\sum_{i=1}^n \mathrm{Re}(\langle g_i - f^*(x_i), h(x_i)\rangle_\mathcal{G}) + \sum_{i=1}^n\Vert h(x_i)\Vert_\mathcal{H}^2 + 2\lambda\mathrm{Re}(\langle h,f^*\rangle_\mathcal{H}) + \lambda\Vert h\Vert_\mathcal{H}^2\\
  &= \sum_{i=1}^n\Vert h(x_i)\Vert_\mathcal{H}^2 + \lambda\Vert h\Vert_\mathcal{H}^2\geq 0.\end{align*}$$</p>

The equality holds if and only if $h\equiv 0.$ Thus we conclude the proof.

## References
+ Micchelli, C. A. and Pontil, M. On learning vector-valued functions. (2005). In *Neural Computation.* 17(1):177–204.
+ Grünewälder, S., Lever G., Baldassarre L., Patterson S., Gretton A., and Pontil M.. (2012). Conditional mean embeddings as regressors. In *Proceedings of the 29th International Coference on International Conference on Machine Learning (ICML'12)*. 1803–1810.
