# Vector-Valued RKHS
## Motivation
**Review: Real (or Complex) RKHS.**  Let $\mathcal{X}$ be a non-empty set, and let $\mathcal{H}$ be a Hilbert space of functions $h:\mathcal{X}\to\mathbb{R}$ (or $\mathbb{C}$) equipped with inner product $\langle\cdot,\cdot\rangle_\mathcal{H}.$ Then $\mathcal{H}$ is a reproducing kernel Hilbert space, provided that the evaluation functional $\delta_x:\mathcal{H}\to\mathbb{R}$ (or $\mathbb{C}$), $f\mapsto f(x)$ is bounded for all $x\in\mathcal{X}.$

By the Riesz's representation theorem, for every $x\in\mathcal{X},$ the boundedness (or continuity, equivalently) of linear $\delta_x$ implies that there exists a unique $k_x\in\mathcal{H}$ such that $f(x) = \delta_x f = \langle f,k_x\rangle_\mathcal{H}$ for all $f\in\mathcal{H}.$ Then we define $k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ (or $\mathbb{C}$), $(x,x^\prime)\mapsto k_{x^\prime}(x).$ We can verify the following two properties of $k$:
+ For all $x\in\mathcal{X},$ the function $k(\cdot,x):\mathcal{X}\to\mathbb{R}$ (or $\mathbb{C}$) is in $\mathcal{H}.$ To see this, just note that $k(\cdot,x)=k_x.$
+ For all $f\in\mathcal{H}$ and $x\in\mathcal{X},$ $\langle f,k(\cdot,x)\rangle_\mathcal{H} = \langle f,k_x\rangle = \delta_x f = f(x).$ This property is referred to as the *reproducing property*.

We call the function $k:\mathcal{X}\to\mathcal{X}\to\mathbb{R}$ (or $\mathbb{C}$) a kernel on $\mathcal{X}.$ It possesses the following properties (For brevity we only discuss $\mathbb{C}$):
+ (Symmetry / Conjugate symmetry). $k(x,x^\prime) = \overline{k(x^\prime,x)}$ for all $x,x^\prime\in\mathcal{X}.$
+ (Positive definiteness). For all $n\in\mathbb{N},\ x_1,\cdots,x_n\in\mathcal{X},\ \alpha_1,\cdots,\alpha_n\in\mathbb{C},$ we have
  <p>$$\sum_{i=1}^n\sum_{j=1}^n \alpha_i\overline{\alpha_j}k(x_i,x_j) \geq 0.$$</p>

In our discussion, we only consider the case for real and complex-valued functions. However, RHKS can be defined for more classes of functions.

**Definition 1** (Vector-valued RKHS). Let $\mathcal{H}$ be a Hilbert space of functions $h:\mathcal{X}\to\mathcal{G}$ equipped with inner product $\langle\cdot,\cdot\rangle_\mathcal{H},$ where $\mathcal{G}$ is also a Hilbert space equipped with inner product $\langle\cdot,\cdot\rangle_\mathcal{G}.$ Then $\mathcal{H}$ is a RKHS, provided thay the linear functional $\delta_{x,g}:\mathcal{H}\to\mathbb{C},\ f\mapsto\langle g,f(x)\rangle_\mathcal{G}$ is bounded for all $x\in\mathcal{X}$ and $g\in\mathcal{G}.$

By the Riesz's representation theorem, for every $x\in\mathcal{X}$ and $g\in\mathcal{G},$ the boundedness of linear $\delta_{x,g}$ implies that there exists a unique $k_{x,g}\in\mathcal{H}$ such that $\langle g,f(x)\rangle_\mathcal{G} = \delta_{x,g} f = \langle f,k_{x,g}\rangle_\mathcal{H}$ for all $f\in\mathcal{H}.$ Moreover, it is easy to see that $k_{x,g}$ is linear with respect to $g.$ Then for each $x\in\mathcal{X},$ we define $k_x:\mathcal{G}\to\mathcal{H}, g\mapsto k_{x,g}.$

Denote by $\mathcal{L}(\mathcal{G})$ the space of all bounded linear operators from $\mathcal{G}$ to $\mathcal{G}.$ We define the kernel $k:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{G})$ as
<p>$$k(x,x^\prime)g = (k_{x^\prime} g)(x)\in\mathcal{G}.$$</p>

All aforementioned properties of RKHS holds:
+ (Compatibility). For all $x\in\mathcal{X},$ $k(\cdot,x)=k_x\in\mathcal{H}.$
+ (Reproducing property). For all $f\in\mathcal{H},\ x\in\mathcal{X}$ and $g\in\mathcal{G},$ $\langle f,k(\cdot,x)g\rangle_\mathcal{H} = \langle f,k_xg\rangle_\mathcal{H} = \langle g,f(x)\rangle_\mathcal{G}.$
+ (Adjoint symmetry). For all $x,x^\prime\in\mathcal{X},$ $k(x,x^\prime) = k(x^\prime, x)^*,$ the Hermitian adjoint operator. To see this, just note that
  <p>$$\langle k(x,x^\prime)g,g^\prime\rangle_\mathcal{G} = \overline{\langle g^\prime,(k_{x^\prime}g)(x)\rangle_\mathcal{G}} = \overline{\langle k_{x^\prime}g,k_{x}g^\prime\rangle_\mathcal{H}} = \langle k_{x}g^\prime,k_{x^\prime}g\rangle_\mathcal{H} = \langle g,k(x^\prime,x)g^\prime\rangle_\mathcal{G}.$$</p>
+ (Positive definiteness). For all $n\in\mathcal{N},\ x_1,\cdots,x_n\in\mathcal{X}$ and  $g_1,\cdots,g_n\in\mathcal{G},$
  <p>$$\sum_{i=1}^n\sum_{j=1}^n \langle g_i,k(x_i,x_j)g_j\rangle_\mathcal{G} = \sum_{i=1}^n\sum_{j=1}^n \langle k_{x_j}g_j,k_{x_i}g_i\rangle_\mathcal{H}\geq 0.$$</p>


## Kernel Ridge Regression
