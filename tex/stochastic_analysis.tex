\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{extarrows}
\usepackage{float} 
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{inconsolata}
\usepackage{listings}
\usepackage{makecell}
\usepackage{mathrsfs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{subfigure} 
\usepackage{threeparttable}
\usepackage{ulem}
\usepackage[many]{tcolorbox}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta}
\setitemize[1]{itemsep=1pt,partopsep=0.8pt,parsep=\parskip,topsep=0.8pt}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
%% Number of equations.
\numberwithin{equation}{section}
%% New symbols.
\newcommand{\rmb}{\mathrm{b}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\bfw}{\mathbf{w}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbN}{\mathbb{N}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\scr}{\mathscr}
\renewcommand{\cal}{\mathcal}
\newcommand{\loc}{\mathrm{loc}}
\newcommand{\ol}{\overline}
\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}
\DeclareFontFamily{U}{mathx}{}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}
\DeclareMathOperator{\id}{Id}
\DeclareMathOperator{\gr}{Gr}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Le}{Le}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\conv}{Conv}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\esssup}{ess\,sup}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\argmin}{argmin}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}
\renewcommand{\i}{\mathrm{i}}
\renewcommand{\proofname}{\textit{Proof}}
\renewcommand*{\thesubfigure}{(\arabic{subfigure})}
\renewcommand{\baselinestretch}{1.20}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{remark}{Remark}
\title{\bf Lecture Notes for Stochastic Analysis}
\usepackage{geometry}
\geometry{a4paper, scale=0.80}
\author{\textsc{Jyunyi Liao}}
\date{}
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Basic Measure Theory}
\subsection{Measurable Spaces, Sierpiński-Dynkin $\pi$-$\lambda$ System and Monotone Classes}
Let $\Omega$ be a nonempty set. Denote by $2^\Omega$ the set of all subsets of $\Omega$, namely, $2^\Omega=\{A:A\subset\Omega\}$. Any subset $\mathscr{A}\subset 2^\Omega$ is called a collection of subsets of $\Omega$.

\begin{definition}[$\sigma$-algebra and measurable space]\label{def:1.1}
Let $\mathscr{F}$ be a collection of subsets of $\Omega$. Then $\mathscr{F}$ is said to be a \textit{$\sigma$-algebra} (or \textit{$\sigma$-field}) if the following conditions are satisfied:
\begin{itemize}
	\item[(i)] $\Omega\in\mathscr{F}$;
	\item[(ii)] For all $A\in\mathscr{F}$, the \textit{complement} $A^c:=\Omega\backslash A\in \mathscr{F}$;
	\item[(iii)] For all sequences $(A_n)_{n=1}^\infty$ in $\mathscr{F}$, $\bigcup_{n=1}^\infty A_n\in\mathscr{F}$.
\end{itemize}

A pair $(\Omega,\mathscr{F})$ consisting of a set $\Omega$ and a $\sigma$-algebra of subsets of $\Omega$ is called a \textit{measurable space}. A set $A\in\mathscr{F}$ is said to be $\mathscr{F}$-measurable.
\end{definition}

\begin{remark} Clearly, $\{\emptyset,\Omega\}$ and $2^\Omega$ are two trivial $\sigma$-algebras of subsets of $\Omega$. Furthermore, we can show that a $\sigma$-algebra $\mathscr{F}$ also satisfies the following:
\begin{itemize}
	\item[(i)] $\emptyset\in\mathscr{F}$;
	\item[(ii)] For all $A,B\in\mathscr{F}$, $A\backslash B,A\cup B,A\cap B\in\mathscr{F}$;
	\item[(iii)] For all sequences $(A_n)_{n=1}^\infty$ in $\mathscr{F}$, $\bigcap_{n=1}^\infty A_n\in\mathscr{F}$.
\end{itemize}
Moreover, the intersection of any collection of $\sigma$-algebras is again a $\sigma$-algebra.
\end{remark}

\begin{definition}\label{def:1.2}
Let $\mathscr{A}$ be a collection of subsets of $\Omega$. The \textit{$\sigma$-algebra generated by $\mathscr{A}$} is the minimal $\sigma$-algebra of subsets of $\Omega$ that contains $\mathscr{A}$:
\begin{align*}
	\sigma(\mathscr{A}) = \bigcap\left\{\mathscr{F}:\mathscr{F}\ \textit{is a $\sigma$-algebra and}\ \mathscr{F}\supset\mathscr{A}\right\}.
\end{align*}
\end{definition}

\begin{remark} We can generate the minimal $\sigma$-algebra $\mathscr{F}$ from $\mathscr{A}$ as follows: (i) Complete $\mathscr{A}$: $\mathscr{F}\leftarrow\mathscr{A}\cup\{\Omega,\emptyset\}$; (ii) For all $A\in\mathscr{A}$, add $A^c$ to $\mathscr{F}$ if necessary; (iii) For all sequences of sets in $\mathscr{F}$, include their union in $\mathscr{F}$.
\end{remark}

\begin{definition}\label{def:1.3} 
Let $(X,\mathscr{T})$ be a topological space. The \textit{Borel $\sigma$-algebra} on $X$ is defined as the $\sigma$-algebra generated by all open sets in $X$. We write $\mathscr{B}(X)=\sigma(\mathscr{T})$.
\end{definition}
\begin{remark} One of the most commonly used $\sigma$-algebra is the Borel $\sigma$-algebra on $\mathbb{R}$. By definition, $\mathscr{B}(\mathbb{R})$ contains all open subsets, closed subsets, finite subsets and countable subsets of $\mathbb{R}$. Also, $\mathscr{B}(\mathbb{R})$ contains all $G_\delta$-sets (a countable intersection of open sets) and all $F_\sigma$-sets (a countable union of closed sets) in $\mathbb{R}$.
\end{remark}

\begin{definition}[$\pi$-system]\label{def:1.4} Let $\mathscr{P}$ be a collection of subsets of $\Omega$. If $A\cap B\in\mathscr{P}$ for all $A,B\in\mathscr{P}$, then $\mathscr{P}$ is said to be a \textit{$\pi$-system}.
\end{definition}

\begin{definition}[$\lambda$-system] Let $\mathscr{L}$ be a collection of subsets of $\Omega$. Then $\mathscr{L}$ is said to be a \textit{$\lambda$-system} (or \textit{Dynkin system}) if it satisfies the following conditions:
\begin{itemize}
	\item[(i)] $\Omega\in\mathscr{L}$;
	\item[(ii)] For all $A,B\in\mathscr{L}$ such that $A\subset B$, it holds $B\backslash A\in \mathscr{L}$;
	\item[(iii)] For all increasing sequences $A_1\subset A_2\subset\cdots\subset A_n\subset A_{n+1}\subset\cdots$ in $\mathscr{L}$, it holds $\bigcup_{n=1}^\infty A_n\in\mathscr{L}$.
\end{itemize}
\end{definition}
\begin{remark} Another equivalent formulation of $\lambda$-system is stated below:
\begin{itemize}
	\item[(i)] $\Omega\in\mathscr{L}$;
	\item[(ii)] For all $A\in\mathscr{L}$, it holds $A^c\in \mathscr{L}$;
	\item[(iii)] For all sequences $(A_n)_{n=1}^\infty$ of disjoint sets in $\mathscr{L}$, it holds $\coprod_{n=1}^\infty A_n\in\mathscr{L}$.
\end{itemize}
We also observe that the intersection of any collection of $\lambda$-systems is again a $\lambda$-system. Therefore, similar to \hyperref[def:1.2]{Definition 1.2}, we can define the minimal $\lambda$-system generated by a collection $\mathscr{A}$ of subsets of $\Omega$:
\begin{align*}
	\lambda(\mathscr{A})=\bigcap\left\{\mathscr{L}:\mathscr{L}\ \textit{is a $\lambda$-system and}\ \mathscr{L}\supset \mathscr{A}\right\}.
\end{align*}
\end{remark}

In general, a $\lambda$-system is not a $\sigma$-algebra, since it is not always closed under countable unions unless they are disjoint. For instance, let $\Omega=\{0,1,2,3\}$, and consider $\mathscr{L}=\left\{\Omega,\{0,1\},\{2,3\},\{0,2\},\{1,3\},\{1,2\},\{0,3\},\emptyset\right\}$.

\begin{lemma}\label{lemma:1.6} $\mathscr{F}$ is a $\sigma$-algebra if and only if $\mathscr{F}$ is a $\pi$-system and $\lambda$-system.
\end{lemma}
\begin{proof}
By definition, a $\sigma$-algebra is a $\pi$-system and $\lambda$-system. Conversely, if $\mathscr{F}$ is a $\pi$-system and $\lambda$-system, we only need to verify \hyperref[def:1.1]{Definition 1.1 (iii)}. Let $(A_n)_{n=1}^\infty$ be a sequence in $\mathscr{F}$. Define $$B_n=\bigcup_{j=1}^n A_j,\ \forall n\in\mathbb{N}.$$
Clearly, $B_1\in\mathscr{F}$. Moreover, if $B_{n-1}\in\mathscr{F}$, we have $C_n=B_{n-1}\cap A_n\in\mathscr{F}$ since $\mathscr{F}$ is a $\pi$-system, and
\begin{align*}
	B_n = A_n\cup B_{n-1} = \underbrace{(A_n\backslash C_n)}_{\in\mathscr{F}}\cup\underbrace{(B_{n-1}\backslash C_n)}_{\in\mathscr{F}}\cup\,C_n.\tag{1.1}\label{eq:1.1}
\end{align*}
Note that $\hyperref[eq:1.1]{(1.1)}$ is a union of disjoint sets in $\mathscr{F}$, which is a $\lambda$-system. Hence $B_n$ is an increasing sequence in $\mathscr{F}$, which implies $\bigcup_{n=1}^\infty A_n=\bigcup_{n=1}^\infty B_n\in\mathscr{F}$.
\end{proof}

We introduce the Sierpiński-Dynkin $\pi$-$\lambda$ theorem, which is a powerful tool in measure-theoretic analysis.
\begin{theorem}[Sierpiński-Dynkin $\pi$-$\lambda$ theorem]\label{thm:1.7} Let $\mathscr{P}$ and $\mathscr{L}$ be two collections of subsets of $\Omega$ such that $\mathscr{P}\subset\mathscr{L}$. If $\mathscr{P}$ is a $\pi$-system, and $\mathscr{L}$ is a $\lambda$-system, then $\sigma(\mathscr{P})\subset\mathscr{L}$.
\end{theorem}
\begin{proof}
We first claim that $\lambda(\mathscr{P})$ is a $\sigma$-algebra. By \hyperref[lemma:1.6]{Lemma 1.6}, it suffices to show that $\lambda(\mathscr{P})$ is a $\pi$-system.

\item\textit{Step I:} We show that for all $A\in\lambda(\mathscr{P})$, the collection
\begin{align*}
	\lambda_A:=\left\{B\subset\Omega:A\cap B\in\lambda(\mathscr{P})\right\}
\end{align*}
is a $\lambda$-system. Clearly, $\lambda_A$ contains $\Omega$ and is closed under countable disjoint unions. For any $B\in\lambda_A$,
\begin{align*}
	A\cap B^c = A\backslash (A\cap B)\in\lambda(\mathscr{P}),
\end{align*}
because it is the proper difference of sets in $\lambda(\mathscr{P})$. Hence $\lambda_A$ is a $\lambda$-system.

\item\textit{Step II:} We show that $A\cap B\in\lambda(\mathscr{P})$ for all $A\in\mathscr{P}$ and all $B\in\lambda(\mathscr{P})$. Fix $A\in\mathscr{P}$. Since $\mathscr{P}$ is a $\pi$-system and $\mathscr{P}\subset\lambda(\mathscr{P})$, we have $\mathscr{P}\subset\lambda_A$. Note that $\lambda(\mathscr{P})$ is the minimal $\lambda$-system generated by $\mathscr{P}$, we have $\lambda(\mathscr{P})\subset\lambda_A$.

\item\textit{Step III:} We show that $\lambda(\mathscr{P})$ is a $\sigma$-algebra. Let $B\in\lambda(\mathscr{P})$. By Step I, $\lambda_B$ is a $\lambda$-system. If $E\in\mathscr{P}$, using Step II, we have $E\cap B\in\lambda(\mathscr{P})$, which implies $E\in\lambda_B$. Then $\mathscr{P}\in\lambda_B$, and $\lambda(\mathscr{P})\subset\lambda_B$. Hence for all $A\in\lambda(\mathscr{P})\subset\lambda_B$, $A\cap B\in\lambda(\mathscr{P})$. As a result, $\lambda(\mathscr{P})$ is a $\pi$-system.

Since $\lambda(\mathscr{P})$ is a $\sigma$-algebra, we have $\sigma(\mathscr{P})\subset\lambda(\mathscr{P})\subset\mathscr{L}$. In fact, we can prove that $\sigma(P)=\lambda(P)$: The other direction holds because $\sigma(\mathscr{P})$ is a $\lambda$-system, which implies $\lambda(\mathscr{P})\subset\sigma(\mathscr{P})$.
\end{proof}

Now we introduce the monotone class theorem.

\begin{definition}[Monotone class]\label{def:1.8} A collection $\mathscr{M}$ of subsets of $\Omega$ is said to be a \textit{monotone class} if the following hold: (i) For all increasing sequence $(A_n)_{n=1}^\infty$ in $\mathscr{M}$, it holds $\bigcup_{n=1}^\infty A_n\in\mathscr{M}$; (ii) For all decreasing sequence $(B_n)_{n=1}^\infty$ in $\mathscr{M}$, it holds $\bigcap_{n=1}^\infty B_n\in\mathscr{M}$.
\end{definition}

\begin{remark}
Note that the intersection of a collection of monotone classes is also a monotone class. Again, we can define the monotone class generated by a collection $\mathscr{A}$ of subsets of $\Omega$:
\begin{align*}
	m(\mathscr{A})=\bigcap\left\{\mathscr{M}:\mathscr{M}\ \textit{is a monotone class and}\ \mathscr{M}\supset \mathscr{A}\right\}.
\end{align*}
\end{remark} 

\begin{definition}[Algebra]\label{def:1.9} Let $\mathscr{A}$ be a collection of subsets of $\Omega$. Then $\mathscr{A}$ is said to be an \textit{algebra} if the following hold: (i) $\Omega\in\mathscr{A}$; (ii) $A^c\in \mathscr{A}$ for all $A\in\mathscr{A}$; (iii) $A\cup B\in\mathscr{A}$ for all $A,B\in\mathscr{A}$.
\end{definition}

\begin{remark}
By (ii) and (iii), we can show that an algebra $\mathscr{A}$ is closed under finite unions and finite intersections. In fact, another formulation of algebra uses the ring.
\end{remark}

\paragraph{Definition 1.9*\label{def:1.9*}} (Ring and algebra). A \textit{ring} (or \textit{pre-algebra}) is a collection $\mathscr{R}$ of subsets of $\Omega$ such that $A\backslash B,A\cap B,A\cup B\in\mathscr{R}$ for all $A,B\in\mathscr{R}$. Following this, an \textit{algebra} is a ring that contain $\Omega$. 

\begin{lemma}\label{lemma:1.10} If $\mathscr{F}$ is an algebra that is also a monotone class, then $\mathscr{F}$ is a $\sigma$-algebra.
\end{lemma}
\begin{proof}
It suffices to check \hyperref[def:1.1]{Definition 1.1 (iii)}. For any sequence $(A_n)_{n=1}^\infty$ in algebra $\mathscr{F}$, the partial unions $B_n:=\bigcup_{k=1}^n A_k$ form an increasing sequence in $\mathscr{F}$. Since $\mathscr{F}$ is a monotone class,
$\bigcup_{n=1}^\infty A_n = \bigcup_{n=1}^n B_n \in \mathscr{F}.$
\end{proof}

\begin{theorem}[Monotone class theorem]\label{thm:1.11} Let $\mathscr{A}$ be a algebra of subsets of $\Omega$. Then the monotone class generated by $\mathscr{A}$ coincides with the $\sigma$-algebra generated by $\mathscr{A}$.
\end{theorem}
\begin{proof}
Clearly, a $\sigma$-algebra is a monotone class. If we can show that the monotone class $m(\mathscr{A})$ generated by $\mathscr{A}$ is a $\sigma$-algebra, then $m(\mathscr{A})=\sigma(\mathscr{A})$. Following \hyperref[lemma:1.10]{Lemma 1.10}, it suffices to show that $m(\mathscr{A})$ is an algebra. For any $E\in m(\mathscr{A})$, define
\begin{align*}
	\mathscr{M}_E = \left\{F\in m(\mathscr{A}): E\backslash F,F\backslash E,E\cup F\in m(\mathscr{A})\right\}
\end{align*}

We claim that $\mathscr{M}_E=m(\mathscr{A})$ for all $E\in m(\mathscr{A})$. For any increasing sequence $F_n$ in $\mathscr{M}_E$, the sequence $E\backslash F_n$ is decreasing in $m(\mathscr{A})$, and the sequences $F_n\backslash E$ and $E\cup F_n$ are increasing in $m(\mathscr{A})$. Then
\begin{align*}
	E\backslash\biggl(\bigcup_{n=1}^\infty F_n\biggr) = \bigcap_{n=1}^\infty (E\backslash F_n),\ \biggl(\bigcup_{n=1}^\infty F_n\biggr)\backslash E = \bigcup_{n=1}^\infty (F_n\backslash E),\ E\cup \biggl(\bigcup_{n=1}^\infty F_n\biggr)=\bigcup_{n=1}^\infty (E\cup F_n)
\end{align*} 
are all contained in $m(\mathscr{A})$, and $\bigcup_{n=1}^\infty F_n\in \mathscr{M}_E$. A similar statement holds for decreasing sequences in $\mathscr{M}_E$. Hence $\mathscr{M}_E$ is a monotone class. 

Assume $E\in\mathscr{A}$. Since $\mathscr{A}$ is an algebra, we have $\mathscr{A}\subset\mathscr{M}_E$, which implies $m(\mathscr{A})\subset\mathscr{M}_E$. Then for all $E\in\mathscr{A}$ and all $F\in m(\mathscr{A})$, we have $F\in\mathscr{M}_E$, which holds if and only if $E\in\mathscr{M}_F$. As a result, we have $\mathscr{A}\subset\mathscr{M}_F$ for all $F\in m(\mathscr{A})$, which again implies $m(\mathscr{A})\subset\mathscr{M}_F$. Hence for all $E,F\in m(\mathscr{A})$, we have $E\backslash F,F\backslash E,E\cup F\in m(\mathscr{A})$. Clearly, $\Omega\in m(\mathscr{A})$. Hence $m(\mathscr{A})$ is an algebra, as desired.
\end{proof}
\begin{remark} There is an equivalent statement of \hyperref[thm:1.11]{Theorem 1.11}: If $\mathscr{A}$ is an algebra, and $\mathscr{M}$ is a monotone class such that $\mathscr{M}\supset\mathscr{A}$, then $\sigma(\mathscr{A})\subset\mathscr{M}$.
\end{remark}

\newpage
\subsection{Measures, Pre-measures and Carathéodory's Extension}
\begin{definition}[Measure]\label{def:1.12}
Let $(\Omega,\mathscr{F})$ be a measurable space. A \textit{(nonnegative)  measure} $\mu$ on $(\Omega,\mathscr{F})$ is a function $\mu:\mathscr{F}\to\overline{\mathbb{R}}_+:=[0,\infty]$ that satisfies the following:
\begin{itemize}
	\item[(i)] $\mu(\emptyset)=0$;
	\item[(ii)] (Countable additivity). If $(A_n)_{n=1}^\infty$ is a sequence of pairwise disjoint sets in $\mathscr{F}$, then
	\begin{align*}
		\mu\left(\bigcup_{n=1}^\infty A_n\right) = \sum_{n=1}^\infty \mu(A_n).
	\end{align*}
\end{itemize}
The triple $(\Omega,\mathscr{F},\mu)$ is called a \textit{measure space}. Furthermore, 
\begin{itemize}
\item [(i)] $\mu$ is called a \textit{finite measure} if $\mu(\Omega)<\infty$; \item [(ii)] $\mu$ is called a \textit{$\sigma$-finite measure} if there exists $\{\Omega_n\}_{n=1}^\infty\subset\mathscr{F}$ such that $\Omega=\bigcup_{n=1}^\infty\Omega_n$ and $\mu(\Omega_n)<\infty$ for all $n$; and $\mu$ is called a \textit{semi-finite measure} if every positive measure set $E$ have a finite measure subset.
\item[(iii)] $\mu$ is called a \textit{probability measure} if $\mu(\Omega)=1$, and $(\Omega,\mathscr{F},\mu)$ is called a \textit{probability space}.
\end{itemize}
\end{definition}

\begin{remark}
A measure $\mu$ also has the following properties:
\begin{itemize}
	\item For all $A,B\in\mathscr{F}$ such that $A\subset B$, it holds $\mu(B\backslash A)=\mu(B)-\mu(A)\leq\mu(B)$.
	\item For all $A,B\in\mathscr{F}$, it holds $\mu(A\cup B) = \mu(A)+\mu(B)-\mu(A\cap B)$.
	\item Using the following \hyperref[lemma:1.13]{Lemma 1.13}, we obtain countable subadditivity of $\mu$:
	\begin{align*}
		\mu\left(\bigcup_{n=1}^\infty A_n\right) = \lim_{n\to\infty}\mu\left(\bigcup_{k=1}^{n}A_k\right) \leq \lim_{n\to\infty}\sum_{k=1}^n\mu(A_k) = \sum_{n=1}^\infty \mu(A_n),\ \  \forall \textit{sequences $(A_n)_{n=1}^\infty$ in $\mathscr{F}$}.
	\end{align*}
\end{itemize}
\end{remark}

\begin{lemma}\label{lemma:1.13} Let $(\Omega,\mathscr{F},\mu)$ be a measurable space.
	\begin{itemize}
		\item[(i)] If $(A_n)_{n=1}^\infty$ is an increasing sequence in $\mathscr{F}$, then
		\begin{align*}
			\mu\left(\bigcup_{n=1}^\infty A_n\right) = \lim_{n\to\infty}\mu(A_n);
		\end{align*}
		\item[(ii)] If $(A_n)_{n=1}^\infty$ is an decreasing sequence in $\mathscr{F}$ such that $\mu(A_1)<\infty$, then
		\begin{align*}
			\mu\left(\bigcap_{n=1}^\infty A_n\right) = \lim_{n\to\infty}\mu(A_n);
		\end{align*}
\item[(iii)] More generally, if $(A_n)_{n=1}^\infty$ is a sequence in $\scr{F}$, then
\begin{align*}
	\mu\left(\bigcup_{n=1}^\infty\bigcap_{k=n}^\infty A_k\right)\leq\liminf_{n\to\infty}\mu(A_n),
\end{align*}
and if in addition $\mu(\bigcup_{n=1}^\infty A_n)<\infty$, then
\begin{align*}
	\mu\left(\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty A_k\right)\geq\limsup_{n\to\infty}\mu(A_n).
\end{align*}
\end{itemize}
\end{lemma}
\begin{proof}
(i) Define $B_1=A_1$, and $B_n=A_n\backslash A_{n-1}$ for $n\geq 2$. Then $(B_n)_{n=1}^\infty$ is a disjoint sequence of sets in $\mathscr{F}$. By countable additivity of $\mu$, we have $\mu(A_n) = \sum_{k=1}^n\mu(B_k)$. Hence
\begin{align*}
	\lim_{n\to\infty}\mu(A_n) = \sum_{n=1}^\infty\mu(B_n) = \mu\left(\bigcup_{n=1}^\infty B_n\right) = \mu\left(\bigcup_{n=1}^\infty A_n\right).
\end{align*}
\item (ii) Choose an increasing sequence $C_n=A_1\backslash A_n$. By (i), we have
\begin{align*}
	\lim_{n\to\infty}\mu(C_n) = \mu\left(\bigcup_{n=1}^\infty C_n\right).\label{eq:1.2}\tag{1.2}
\end{align*}
Since $\mu(A_1)<\infty$, and since $\bigcup_{n=1}^\infty C_n = A_1\backslash\left(\bigcap_{n=1}^\infty A_n\right)$, the identity becomes \hyperref[eq:1.2]{(1.2)} $$\mu(A_1)-\lim_{n\to\infty}\mu(A_n)=\mu(A_1) - \mu\left(\bigcap_{n=1}^\infty A_n\right).$$
(iii) The set $B_n=\bigcap_{k=n}^\infty A_k$ is an increasing sequence in $\scr{F}$, and we have $\mu(B_n)\leq\inf_{n\geq k}\mu(A_k)$. By (i),
\begin{align*}
	\mu\left(\bigcup_{n=1}^\infty\bigcap_{k=n}^\infty A_k\right)=\lim_{n\to\infty}\mu(B_n)\leq\liminf_{n\to\infty}\mu(A_k).
\end{align*}
Also, the set $B_n=\bigcup_{k=n}^\infty A_k$ is a decreasing sequence in $\scr{F}$, and we have $\mu(B_n)\geq\sup_{n\geq k}\mu(A_k)$. By (ii),
\begin{align*}
	\mu\left(\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty A_k\right)=\lim_{n\to\infty}\mu(B_n)\geq\limsup_{n\to\infty}\mu(A_k).
\end{align*}
Then we conclude the proof.
\end{proof}
\begin{remark}
The condition $\mu(A_1)<\infty$ in (ii) cannot be removed. For example, let $A_n=[n,\infty)$, and let $\mu$ be the Lebesgue measure. Then $\mu(E_n)=\infty$ for all $n\in\mathbb{N}$, but $$\mu\left(\bigcup_{n=1}^\infty A_n\right)=\mu(\emptyset)=0.$$
\end{remark}

\begin{theorem}[First Borel-Cantelli Lemma]\label{thm:1.14} 
Let $(\Omega,\mathscr{F},\mu)$ be a measure space. If $(A_n)_{n=1}^\infty$ is a sequence of sets in $\mathscr{F}$ such that $\sum_{n=1}^\infty \mu(A_n)<\infty$, then
\begin{align*}
	\mu\left(\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty A_k\right) = 0.
\end{align*}
In other words, almost all $\omega\in\Omega$ belongs to at most finitely many $A_k$'s.
\end{theorem}
\begin{proof}
Let $B_n=\bigcup_{k=n}^\infty A_k$ for all $n$. Then $B_n$ is a decreasing sequence in $\mathscr{F}$, and $\mu(B_1)\leq\sum_{k=1}^\infty\mu(A_k)<\infty$. By \hyperref[lemma:1.13]{Lemma 1.13},\vspace{0.3cm}

$\displaystyle\hspace{2cm}0\leq\mu\left(\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty A_k\right) = \mu\left(\bigcup_{n=1}^\infty B_n\right) = \lim_{n\to\infty}\mu(B_n)\leq\lim_{n\to\infty}\sum_{k=n}^\infty\mu(A_k)=0.\vspace{0.3cm}$
\end{proof}

We are going to construct a measure from a ring of subsets of $\Omega$.

\begin{definition}[Pre-measure]\label{def:1.15} Let $\mathscr{A}$ be a collection of subsets of $\Omega$ such that $\emptyset\in\mathscr{A}$. A \textit{pre-measure} on $\mathscr{A}$ is a function $\mu:\mathscr{A}\to\overline{\mathbb{R}}_+$ satisfying the following:
\begin{itemize}
	\item[(i)] $\mu(\emptyset)=0$;
	\item[(ii)] (Countable additivity). If $(A_n)_{n=1}^\infty$ is a sequence of pairwise disjoint sets in $\mathscr{A}$ with $\coprod_{n=1}^\infty A_n\in \mathscr{A}$, then
	\begin{align*}
		\mu\left(\bigcup_{n=1}^\infty A_n\right) = \sum_{n=1}^\infty \mu(A_n).
	\end{align*}
\end{itemize}
\end{definition}

\begin{definition}[Outer measure]\label{def:1.16} 
An \textit{outer measure} on $\Omega$ is a set function $\mu^*:2^\Omega\to\overline{\mathbb{R}}_+$ satisfying the following properties:
\begin{itemize}
	\item[(i)] $\mu(\emptyset)=0$;
	\item[(ii)] (Monotonicity). If $A\subset B$ then $\mu^*(A)\leq\mu^*(B)$;
	\item[(iii)] (Countable subadditivity). For any sequence $(A_n)_{n=1}^\infty$ of subsets of $\Omega$, we have
	\begin{align*}
		\mu^*\left(\bigcup_{n=1}^\infty A_n\right) \leq \sum_{n=1}^\infty \mu^*(A_n).
	\end{align*}
\end{itemize}
\end{definition}

\begin{lemma}[Induced outer measure]\label{lemma:1.17}
Let $\mathscr{R}$ be a ring of subsets of $\Omega$, and let $\mu:\mathscr{R}\to\overline{\mathbb{R}}_+$ be a pre-measure on $\mathscr{R}$. Define $\mu^*:2^\Omega\to\overline{\mathbb{R}}_+$ by
\begin{align*}
	\mu^*(E)=\inf\left\{\sum_{n=1}^\infty\mu(A_n):\left\{A_n\right\}_{n=1}^\infty\subset\mathscr{R},\ E\subset\bigcup_{n=1}^\infty A_n\right\},\ \forall E\in 2^\Omega.
\end{align*}
Define $\inf\emptyset=\infty$. Then $\mu^*$ is an outer measure on $\Omega$, and $\mu^*|_\mathscr{R}=\mu$.
\end{lemma}
\begin{proof}
It is easy to check that $\mu^*$ is an outer measure. For all $E\in\mathscr{R}$, take $A_1=E$ and $A_2=A_3=\cdots=\emptyset$. Then we know $\mu^*(E)\leq\mu(E)$. Hence it remains to show $\mu^*(E)\geq\mu(E)$.

For an arbitrary sequence $(A_n)_{n=1}^\infty$ such that $E\subset\bigcup_{n=1}^\infty A_n$, take $B_1=A_1$ and $B_n=A_n\backslash \left(\bigcup_{k=1}^{n-1} B_k\right)$ for $n\geq 2$. Then $(B_n)_{n=1}^\infty$ is a disjoint sequence in $\mathscr{R}$, and we have
\begin{align*}
	E\subset \bigcup_{n=1}^\infty A_n= \bigcup_{n=1}^\infty B_n = \bigcup_{n=1}^\infty(E\cap B_n)\ \Rightarrow\ \mu(E)=\sum_{n=1}^\infty\mu(E\cap B_n)\leq\sum_{n=1}^\infty\mu(B_n)\leq\sum_{n=1}^\infty\mu(A_n).
\end{align*}
Hence $\mu^*(E)\geq\mu(E)$.
\end{proof}

\begin{definition}[Carathéodory condition]\label{def:1.18} Let $\mathscr{R}$ be a ring of subsets of $\Omega$, and let $\mu:\mathscr{R}\to\overline{\mathbb{R}}_+$ be a pre-measure on $\mathscr{R}$. Let $\mu^*$ be the outer measure induced by $\mu$. A subset $E\subset\Omega$ is said to be \textit{$\mu^*$-measurable} if
\begin{align*}
	\mu^*(A)=\mu^*(A\cap E) + \mu^*(A\backslash E),\ \forall A\subset\Omega.\tag{1.3}\label{eq:1.3}
\end{align*}
Denote by $\mathscr{R}^*$ the collection of all $\mu^*$-measurable sets on $\Omega$.
\end{definition}

\paragraph{Remark.} To check \hyperref[eq:1.3]{(1.3)}, it suffices to check $\mu^*(A)\geq\mu^*(A\cap E) + \mu^*(A\backslash E)$, since the opposite holds by definition. Moreover, for all $E\subset X$ with $\mu^*(E)=0$, the Carathéodory condition is automatically satisfied.

\begin{proposition}\label{prop:1.19} The collection $\mathscr{R}^*$ given in Definition \ref{def:1.18} is a $\sigma$-algebra.
\end{proposition}
\begin{proof}
It is clear that $\Omega,\emptyset\in\mathscr{R}^*$ and that $E^c\in\mathscr{R}^*$ for all $E\in\mathscr{R}^*$.

\item \textit{Step I:} We claim that $\mathscr{R}^*$ is an algebra. Let $E,F\in\mathscr{R}^*$. Then for each $A\subset\Omega$,
\begin{align*}
	\mu^*(A) &= \mu^*(A\cap E)+\mu^*(A\cap E^c)\\
	&= \mu^*(A\cap E\cap F) + \mu^*(A\cap E\cap F^c)+\mu^*(A\cap E^c)\\
	&\geq \mu^*(A\cap E\cap F) + \mu^*\bigl(\underbrace{(A\cap E\cap F^c)\cup(A\cap E^c)}_{=A\,\cap\,(E^c\,\cup\,F^c)}\bigr)\tag{By subadditivity of $\mu^*$}\\
	&= \mu^*(A\cap E\cap F) + \mu^*(A\cap (E\cap F)^c) \ \Rightarrow\ E\cap F\in\mathscr{R}^*.
\end{align*}

Hence $\mathscr{R}^*$ is closed under finite intersections. Note that $\mathscr{R}^*$ is closed under complements, it is also closed under finite unions. Thus $\mathscr{R}^*$ is an algebra, as desired.

\item \textit{Step II:} Following \hyperref[lemma:1.10]{Lemma 1.10}, it remains to show $\mathscr{R}^*$ is a monotone class. Let $(E_n)_{n=1}^\infty$ be a increasing sequence in $\mathscr{R}^*$. We want to show that $G:=\bigcup_{n=1}^\infty E_n\in\mathscr{R}^*$.

Take $F_1=E_1$ and $F_n=E_n\backslash E_{n-1}$ for $n\geq 2$. Then $(F_n)_{n=1}^\infty$ is a disjoint sequence in $\mathscr{R}^*$, and $G=\bigcup_{n=1}^\infty F_n$. For all $A\subset X$ and all $n\in\mathbb{N}$,
\begin{align*}
	\mu^*(A) = \mu^*(A\cap E_n) + \mu^*(A\cap E_n^c) &\geq \mu^*(A\cap E_n) + \mu^*(A\cap G^c)\\
	&=\mu^*(A\cap E_{n-1}) + \mu^*(A\cap F_n) + \mu^*(A\cap G^c) \tag{by $F_n\in\mathscr{R}^*$}\\
	&=\cdots =\sum_{k=1}^n\mu^*\left(A\cap F_k\right) + \mu^*(A\cap G^c)\tag{by $F_{n-1},\cdots,F_2\in\mathscr{R}^*$}.
\end{align*}

Therefore $\mu^*(A)\geq\,\sum_{n=1}^\infty\mu^*(A\cap F_k)+\mu^*(A\cap G^c) = \mu^*(A\cap G)+\mu^*(A\cap G^c)$, and $G\in\mathscr{R}^*$, as desired. (To show that decreasing sequences in $\mathscr{R}^*$ have their limits in $\mathscr{R}^*$, take the complement.)
\end{proof}

\begin{proposition}\label{prop:1.20} $\mu^*$ is a measure on $(\Omega,\mathscr{R}^*)$.
\end{proposition}
\begin{proof}
It suffices to show countable additivity. Let $(A_n)_{n=1}^\infty$ be a sequence of disjoint sets in $\mathscr{R}^*$, and let $B_n=\bigcup_{k=1}^n A_k$. Then for all $n\geq\mathbb{N}$,
\begin{align*}
	\mu^*\left(\bigcup_{n=1}^\infty A_n\right) \geq \mu^*(B_n) \overset{A_n\in\mathscr{R}^*}{=} \mu^*(A_n) + \mu^*(B_{n-1}) \overset{A_{n-1}\in\mathscr{R}^*}{=} \cdots \overset{A_2\in\mathscr{R}^*}{=} \sum_{k=1}^n \mu^*(A_k).
\end{align*}

Hence $\mu^*\left(\bigcup_{n=1}^\infty A_n\right) \geq\sum_{n=1}^\infty \mu^*(A_n)$. Since the opposite inequality holds by countable subadditivity of outer measure $\mu^*$, the equality of countable additivity follows.
\end{proof}

Now we introduce the Carathéodory's extension theorem.

\begin{theorem}[Carathéodory's extension theorem]\label{thm:1.21} Let $\mathscr{R}$ be a ring of subsets of $\Omega$, and let $\mu:\mathscr{R}\to\overline{\mathbb{R}}_+$ be a pre-measure on $\mathscr{R}$. Let $\mu^*$ and $\mathscr{R}^*$ be given as in Definition \ref{def:1.18}. Then $(\Omega,\mathscr{R}^*,\mu^*)$ is a measure space, and $\mu^*|_\mathscr{R}=\mu$. Furthermore, $\mathscr{R}\subset\mathscr{F}:=\sigma(\mathscr{R})\subset\mathscr{R}^*$. As a result, $\mu^*|_\mathscr{F}$ is an extension of $\mu$, which is called the \textbf{Carathéodory's extension}.
\end{theorem}
\begin{proof}
It suffices to show $\mathscr{R}\subset\mathscr{R}^*$. Fix $E\in\mathscr{R}$, we want to show that
\begin{align*}
	\mu^*(A)\geq\mu^*(A\cap E) + \mu^*(A\cap E^c),\ \forall A\subset\Omega.
\end{align*}
We can certainly assume $\mu^*(A)<\infty$. Then for all $\epsilon>0$, there exists a sequence $(F_n)_{n=1}^\infty$ in $\mathscr{R}$ such that $A\subset\bigcup_{n=1}^\infty F_n$ and that
\begin{align*}
	\sum_{n=1}^\infty\mu(F_n)\leq\mu^*(A)+\epsilon.
\end{align*}

Take a disjoint sequence $(G_n)_{n=1}^\infty$ of sets in $\mathscr{R}$ such that $G_1=F_1$ and $G_n=F_n\backslash\left(\bigcup_{k=1}^{n-1}F_k\right)$ for all $n\geq 2$. Then $\bigcup_{n=1}^\infty G_n = \bigcup_{n=1}^\infty F_n\supset A$, and
\begin{align*}
	\mu^*(A)+\epsilon\geq\sum_{n=1}^\infty\mu(G_n) &= \sum_{n=1}^\infty\mu(G_n\cap E) + \sum_{n=1}^\infty\mu(G_n\backslash E)\\
	&\geq \mu(A\cap E) + \mu(A\backslash E).
\end{align*}
Since $\epsilon>0$ is arbitrary, the result follows.
\end{proof}
\begin{remark}
$(\Omega,\mathscr{R}^*,\mu^*)$ is a complete measure space, since all $E\subset X$ such that $\mu^*(E)=0$ is contained in $\mathscr{R}^*$.
\end{remark}

\paragraph{} We have proved existence of an extension of the pre-measure on a ring. Now we discuss uniqueness.

\begin{lemma}\label{lemma:1.22} Let $\mu$ and $\nu$ be two measures on a measurable space $(\Omega,\mathscr{F})$. Let $\mathscr{P}\subset\mathscr{F}$ be a $\pi$-system such that $\sigma(\mathscr{P})=\mathscr{F}$ and that $\mu|_\mathscr{P}=\nu|_\mathscr{P}$.
\begin{itemize}
	\item[(i)] If $\mu(\Omega)=\nu(\Omega)$, then $\mu=\nu$;
	\item[(ii)] If there exists an increasing sequence $(\Omega_n)_{n=1}^\infty$ of sets in $\mathscr{P}$ such that $\Omega=\bigcup_{n=1}^\infty\Omega_n$ and $\mu(\Omega_n)=\nu(\Omega_n)$ for all $n\in\mathbb{N}$, then $\mu=\nu$.
\end{itemize}
\end{lemma}
\begin{proof}
(i) Let $\mathscr{L}=\{A\in\mathscr{F}:\mu(A)=\nu(A)\}$. Then $\mathscr{L}$ is a $\lambda$-system that contains $\pi$-system $\mathscr{P}$. By \hyperref[thm:1.7]{Theorem 1.7}, $\mathscr{F}\subset\sigma(\mathscr{P})\subset\mathscr{L}\subset\mathscr{F}$. Hence $\mathscr{L}=\mathscr{F}$, as desired.

(ii) Denote $\mu_n=\mu|_{\Omega_n}$. Using (i), we have $\mu_n=\nu_n$ for all $n\in\mathbb{N}$. Then \vspace{0.3cm}

$\hspace{2.75cm}\mu(A) = \lim_{n\to\infty}\mu_n\left(A\cap\Omega_n\right) = \lim_{n\to\infty}\nu_n\left(A\cap\Omega_n\right) = \nu(A),\ \forall A\in\mathscr{F}.\vspace{0.2cm}$
\end{proof}

\begin{theorem}[Uniqueness of extension]\label{thm:1.23} Let $\mathscr{R}$ be a ring of subsets of $\Omega$, and let $\mu:\mathscr{R}\to\overline{\mathbb{R}}_+$ be a pre-measure on $\mathscr{R}$. If $\mu$ is \uwave{$\sigma$-finite,} then its extension on $\mathscr{F}=\sigma(\mathscr{R})$ is unique.
\end{theorem}
\begin{proof}
Any ring $\mathscr{R}$ of subsets of $\Omega$ is a $\pi$-system. Apply \hyperref[lemma:1.22]{Lemma 1.22}.
\end{proof}

\begin{remark}
	Define the collection $\mathscr{A}$ of subsets of $\mathbb{R}$ which are finite unions of intervals of the following forms: $(-\infty,b],\ (a,b],\ (a,\infty),\ (-\infty,\infty)$, where $a<b$. Then $\mathscr{A}$ is an algebra. For each $A\in\mathscr{A}$, define $\ell(A)$ to be the length of $A$. Then $(\mathbb{R},\mathscr{A},\ell)$ is a $\sigma$-finite pre-measure space. Indeed, the \textit{Lebesgue measure} on $\mathbb{R}$ is obtained by the extension procedure described above.
\end{remark}

\begin{definition}[Semi-ring]\label{def:1.24} A \textit{semi-ring} is a $\pi$-system $\mathscr{S}$ of subsets of $\Omega$ such that for all $A,B\in\mathscr{S}$, there exists finite collection $\{A_k\}_{k=1}^n\subset\mathscr{S}$ of pairwise disjoint sets such that $A\backslash B=\coprod_{k=1}^n A_k$.
\end{definition}
\begin{remark}
We can expand a semi-ring $\mathscr{S}$ to a ring by including all finite disjoint unions of sets in $\mathscr{S}$:
\begin{align*}
	\mathscr{R}:=\left\{\coprod_{k=1}^n A_k: n\in\mathbb{N},\  A_1,\cdots,A_n\in\mathscr{S}\ \textit{are pairwise disjoint}\right\}.
\end{align*}

Clearly, $\mathscr{R}$ is closed under finite disjoint unions. For all $A,B\in\mathscr{S}$, we have $A\backslash B,B\backslash A\in\mathscr{R}$, and their union $A\cup B=(A\backslash B)\amalg(A\cap B)\amalg(B\backslash A)\in\mathscr{R}$. Suppose the union of any $n-1$ sets in $\mathscr{S}$ lies in $\mathscr{R}$. Then for all $A_1,\cdots,A_n\in\mathscr{S}$,
\begin{align*}
	A_1\cup\cdots\cup A_n &= \bigl((A_1\cup\cdots\cup A_{n-1})\cup A_n\bigr)\amalg \bigl((A_1\cup\cdots\cup A_{n-1})\backslash A_n\bigr)  \amalg\bigl(A_n\backslash (A_1\cup\cdots\cup A_{n-1})\bigr)\\
	&=\underbrace{\biggl(\bigcup_{k=1}^{n-1}(A_k\cap A_n)\biggr)}_{(a)}\amalg \underbrace{\biggl(\bigcup_{k=1}^{n-1}(A_k\backslash A_n)\biggr)}_{(b)} \amalg \underbrace{\biggl(\bigcap_{k=1}^{n-1}(A_n\backslash A_k)\biggr)}_{(c)}.
\end{align*}

Note that both (a) and (b) are $(n-1)$-unions of sets in $\mathscr{S}$, that (c) is finite intersection of sets in $\mathscr{S}$, and (a), (b), (c) are disjoint sets, we have $\bigcup_{k=1}^n A_k\in\mathscr{R}$. By induction, any finite union of sets in $\mathscr{S}$ is in $\mathscr{R}$. Hence $\mathscr{R}$ is closed under finite unions. To show that $\mathscr{R}$ is a ring, it remains to show that $\mathscr{R}$ is closed under finite intersections and differences:
\begin{align*}
	\left(\coprod_{k=1}^n A_k\right)\cap\left(\coprod_{l=1}^m B_l\right) = \bigcup_{k=1}^n\bigcup_{l=1}^m\underbrace{(A_k\cap B_l)}_{\in\mathscr{S}}\in\mathscr{R},\ \forall\ \textit{disjoint}\  \{A_k\}_{k=1}^n,\{B_l\}_{l=1}^m\subset\mathscr{S},\\
	\left(\coprod_{k=1}^n A_k\right)\backslash\left(\coprod_{l=1}^m B_l\right) = \bigcup_{k=1}^n\bigcap_{l=1}^m\underbrace{(A_k\backslash  B_l)}_{\in\mathscr{R}}\in\mathscr{R},\ \forall\ \textit{disjoint}\  \{A_k\}_{k=1}^n,\{B_l\}_{l=1}^m\subset\mathscr{S}.
\end{align*}

Hence $\mathscr{R}$ is a ring of subsets of $\Omega$. Furthermore, we can extend a pre-measure $\mu:\mathscr{S}\to\overline{\mathbb{R}}_+$ to $\mathscr{R}$ by defining
\begin{align*}
	\mu^*\left(\bigcup_{k=1}^n A_k\right) = \sum_{k=1}^n \mu(A_k),\ \forall\ \textit{pairwise disjoint}\ A_1,\cdots,A_n\in\mathscr{S}.
\end{align*}
Then $\mu^*$ is a pre-measure on $\mathscr{R}$, and $\mu^*|_\mathscr{S}=\mu$. Applying Carathéodory's extension procedure discussed above, we can also extend a pre-measure space $(\Omega,\mathscr{S},\mu)$ on a semi-ring to a complete measure space $(\Omega,\sigma(\mathscr{S}),\mu^*)$.
\end{remark}

\paragraph{} We are going to discuss product measures.
\begin{theorem}[Product measure]
\label{thm:1.25} Let $(\Omega_1,\mathscr{F}_1,\mu_1)$ and $(\Omega_2,\mathscr{F}_2,\mu_2)$ be two \uwave{$\sigma$-finite} measure spaces.
\begin{align*}
	\mathscr{F}_1\times\mathscr{F}_2 := \left\{A_1\times A_2:A_1\in\mathscr{F}_1,A_2\in\mathscr{F}_2\right\}
\end{align*}
is a collection of measurable rectangles in $\Omega_1\times\Omega_2$. Define $\mathscr{F}_1\otimes\mathscr{F}_2:=\sigma(\mathscr{F}_1\times\mathscr{F}_2)$, which is a $\sigma$-algebra of subsets of $\Omega_1\times\Omega_2$. Then there exists a unique measure $\mu$ on $(\Omega_1\times\Omega_2,\mathscr{F}_1\otimes\mathscr{F}_2)$ such that
\begin{align*}
	\mu(A_1\times A_2) = \mu_1(A_1)\mu_2(A_2),\ \forall A_1\in\mathscr{F}_1,\ A_2\in\mathscr{F}_2.
\end{align*}

The measure $\mu_1\otimes\mu_2:=\mu$ is called the \textbf{product measure} on $(\Omega_1\times\Omega_2,\mathscr{F}_1\otimes\mathscr{F}_2)$. Moreover, the triple $(\Omega_1\times\Omega_2,\mathscr{F}_1\otimes\mathscr{F}_2,\mu_1\otimes\mu_2)$ forms a product measure space.
\end{theorem}
\begin{proof}
We use the Carathéodory's extension theorem to prove this. We check that (i) $\mathscr{F}_1\times\mathscr{F}_2$ is a semi-ring; and (ii) $\mu_1\times\mu_2:A_1\times A_2\mapsto \mu_1(A_1)\mu_2(A_2)$ is a pre-measure on $\mathscr{F}_1\times\mathscr{F}_2$. If (i) and (ii) are satisfied, the existence of an extension on $\mathscr{F}_1\otimes\mathscr{F}_2$ is ensured.\vspace{0.1cm}

(i) Let $A=A_1\times A_2,B=B_1\times B_2\in\mathscr{F}_1\times\mathscr{F}_2$. Then $A\cap B=(A_1\cap B_1)\times(A_2\cap B_2)\in\mathscr{F}_1\times\mathscr{F}_2$, and $\mathscr{F}_1\times\mathscr{F}_2$ is a $\pi$-system. Moreover, $(B_1\times B_2)^c=(B_1^c\times\Omega_2)\amalg(B_1\times B_2^c)$, and
\begin{align*}
	A\backslash B = \underbrace{\left((A_1\cap B_1^c)\times A_2\right)}_{\in\mathscr{F}_1\times\mathscr{F}_2}\amalg\underbrace{\left((A_1\cap B_1)\times(A_2\cap B_2^c)\right)}_{\in\mathscr{F}_1\times\mathscr{F}_2}
\end{align*}
Hence $\mathscr{F}_1\times\mathscr{F}_2$ is a semi-ring.\vspace{0.1cm}

(ii) Clearly, $(\mu_1\times\mu_2)(\emptyset)=0$. Then we need to verify the countable additivity of $\mu$. Let $E\times F\in\mathscr{F}_1\times\mathscr{F}_2$, and assume there exists disjoint sets $\{E_n\times F_n\}_{n=1}^\infty$ such that $E\times F=\coprod_{n=1}^\infty(E_n\times F_n)$. In other words,
\begin{align*}
	\chi_{E}(x)\chi_F(y)=\sum_{n=1}^\infty\chi_{E_n}(x)\chi_{F_n}(y),\ \forall x\in\Omega_1,\ y\in\Omega_2.\label{eq:1.4}\tag{1.4}
\end{align*}

Fix $y\in\Omega_2$. By monotone convergence theorem (MCT, \hyperref[thm:1.40]{Theorem 1.40}), we integrate both sides of \hyperref[eq:1.4]{(1.4)} with respect to $x$ on $\Omega_1$. Then we obtain $\mu_1(E)\chi_F(y)=\sum_{n=1}^\infty\mu_1(E_n)\chi_{F_n}(y)$. Again by MCT, we have $\mu_1(E)\mu_2(F)=\sum_{n=1}^\infty\mu_1(E_n)\mu_2(F_n)$. Hence $\mu_1\times\mu_2$ is a pre-measure on $\mathscr{F}_1\times\mathscr{F}_2$.\vspace{0.1cm}

Now we show that $\mu_1\times\mu_2$ is $\sigma$-finite, so uniqueness of extension then follows from \hyperref[thm:1.23]{Theorem 1.23}. By $\sigma$-finiteness of $\mu_1$ and $\mu_2$, there exist $\{A_n\}_{n=1}^\infty\subset\mathscr{F}_1$ and $\{B_n\}_{n=1}^\infty\subset\mathscr{F}_2$ such that $\bigcup_{n=1}^\infty A_n=\Omega_1$, $\bigcup_{n=1}^\infty B_n=\Omega_2$ and $\mu_1(A_n),\mu_2(B_n)<\infty$ for all $n$. Clearly, $\Omega_1\times\Omega_2=\bigcup_{(j,k)\in\mathbb{N}^2} (A_j\times B_k)$, and $(\mu_1\times\mu_2)(A_j\times B_k)$ is finite for all $(j,k)\in\mathbb{N}^2$. Since $\mathbb{N}^2$ is countable, $\mu_1\times\mu_2$ is $\sigma$-finite.
\end{proof}

\paragraph{Remark I.} In general, the set of measurable rectangles $\mathscr{F}_1\times\mathscr{F}_2$ is not a $\sigma$-algebra, since it is possibly not closed under complements countable intersections. For example, consider $(\mathbb{R}^2,\mathscr{B}(\mathbb{R})^2)$, where $\mathscr{B}(\mathbb{R})^2=\mathscr{B}(\mathbb{R})\times\mathscr{B}(\mathbb{R}))$. The union of $(0,1)\times(0,1)$ and $(-1,0)\times (-1,0)$ is not in $\mathscr{B}(\mathbb{R})^2$.

\paragraph{Remark II.} Let $(X_1,\mathscr{T}_1)$ and $(X_2,\mathscr{T}_2)$ are two \uwave{second-countable} topological spaces. The \textit{product topology} $\mathscr{T}_1\otimes\mathscr{T}_2$ is the topology generated by all open rectangles $\mathscr{T}_1\times\mathscr{T}_2$. Let $\mathscr{B}_1=\sigma(\mathscr{T}_1)$ and $\mathscr{B}_2=\sigma(\mathscr{T}_2)$ be the Borel $\sigma$-algebras generated by $\mathscr{T}_1$ and $\mathscr{T}_2$, respectively. Then the $\sigma$-algebras generated by the product topology $\mathscr{T}_1\otimes\mathscr{T}_2$ and by Borel rectangles $\mathscr{B}_1\times\mathscr{B}_2$ coincide. In a nutshell, $\sigma(\mathscr{T}_1)\otimes\sigma(\mathscr{T}_2)=\sigma(\mathscr{T}_1\otimes\mathscr{T}_2)$.
\begin{proof}
Given $A\in\mathscr{B}_1$, let $\mathscr{V}_A$ be the collection of all $B\subset X_2$ such that $A\times B\in\sigma(\mathscr{T}_1\otimes\mathscr{T}_2)$. Clearly, $\mathscr{V}_A$ is a $\sigma$-algebra of subsets of $X_2$, and it contains all open sets in $X_2$. Hence $\mathscr{B}_2\subset\mathscr{V}_A$. Similarly, for $B\in\mathscr{B}_1$, the collection $\mathscr{U}_B$ of all $A\subset X_1$ such that $A\times B\in\sigma(\mathscr{T}_1\otimes\mathscr{T}_2)$ is a $\sigma$-algebra containing $\mathscr{B}_1$. As a result, $\sigma(\mathscr{T}_1\otimes\mathscr{T}_2)$ contains all Borel rectangles $\mathscr{B}_1\times\mathscr{B}_2$, hence contains $\sigma(\mathscr{B}_1\times\mathscr{B}_2)$. 

In the other direction, let $\{U_m\}_{m\in\mathbb{N}}$ be a topological basis for $X_1$, and $\{V_n\}_{n\in\mathbb{N}}$ a topological basis for $X_2$. Then the collection $\mathscr{A}=\{U_m\times V_n\}_{m,n\in\mathbb{N}}$ is a topological basis for the product space $X_1\times X_2$. Furthermore, any open set in $X_1\times X_2$ is a union of these basis elements, which must be countable. Hence the $\sigma$-algebra generated by $\mathscr{A}$ contains all open sets in $X_1\times X_2$, and $\sigma(\mathscr{A})\supset\sigma(\mathscr{T}_1\otimes\mathscr{T}_2)$. On the other hand, note that $\mathscr{A}\subset\mathscr{T}_1\times\mathscr{T}_2$, which is the set of all open rectangles in $X_1\times X_2$, we have $\sigma(\mathscr{A})\subset\sigma(\mathscr{T}_1\times\mathscr{T}_2)\subset\sigma(\mathscr{T}_1\otimes\mathscr{T}_2)$. Furthermore, since $\mathscr{A}\subset\mathscr{B}_1\times\mathscr{B}_2$, we have $\sigma(\mathscr{A})\subset\sigma(\mathscr{B}_1\times\mathscr{B}_2)$.

To summarize, $\sigma(\mathscr{T}_1)\otimes\sigma(\mathscr{T}_2)=\sigma(\mathscr{B}_1\times\mathscr{B}_2)=\sigma(\mathscr{A})=\sigma(\mathscr{T}_1\otimes\mathscr{T}_2)$.
\end{proof}

Since the real line $\mathbb{R}$ given the standard topology is second-countable, we have $\mathscr{B}(\mathbb{R}^2)=\mathscr{B}(\mathbb{R})\otimes\mathscr{B}(\mathbb{R})$. The same conclusion applies for all Euclidean spaces $\bbR^n$, where $n\in\bbN$. The Lebesgue measure is a completion of the Borel measure.

\subsection{Measurable Functions and Simple Function Approximation}
\begin{definition}[Inverse image]\label{def:1.26} Given a function $T:\Omega_1\to\Omega_2$ and a subset $A\subset\Omega_2$, define
\begin{align*}
	T^{-1}A=\left\{\omega\in\Omega_1:T\omega\in A\right\}
\end{align*}
to be the \textit{inverse image} of $A$. If $\mathscr{A}$ is a collection of subsets of $\Omega_2$, define $T^{-1}\mathscr{A}=\{T^{-1}A:A\in\mathscr{A}\}$. 
\end{definition}
\begin{proposition}\label{prop:1.27} Let $T:\Omega_1\to\Omega_2$. It is easy to verify the following properties of $T$.
\begin{itemize}
	\item[(i)] $T^{-1}\Omega_2=\Omega_1$, $T^{-1}\emptyset=\emptyset$;
	\item[(ii)] For all $A\subset\Omega_2$, $T^{-1}(\Omega_2\backslash A)=\Omega_1\backslash T^{-1}A$.
	\item[(iii)] If $\{A_\alpha\}_{\alpha\in J}$ is a collection of subsets of $\Omega_2$, then $T^{-1}\left(\bigcup_{\alpha\in J}A_\alpha\right) = \bigcup_{\alpha\in J}T^{-1}A_\alpha$.
	\item[(iv)] If $\mathscr{F}$ is a $\sigma$-algebra of subsets of $\Omega_2$, then $T^{-1}\mathscr{F}$ is again a $\sigma$-algebra.
\end{itemize}
\end{proposition}

\begin{definition}[Measurable functions]\label{def:1.28} 
Let $(\Omega_1,\mathscr{F}_1)$ and $(\Omega_2,\mathscr{F}_2)$ be two measurable spaces. A function $T:(\Omega_1,\mathscr{F}_1)\to(\Omega_2,\mathscr{F}_2)$ is said to be a \textit{measurable function} if $T^{-1}\mathscr{F}_2\subset\mathscr{F}_1$. In other words, the inverse image of every $\mathscr{F}_2$-measurable set in $\Omega_2$ is $\mathscr{F}_1$-measurable.
\end{definition}

\begin{remark} By definition, we can immediately verify that the composition $T\circ S$ of two measurable functions $(\Omega_1,\mathscr{F}_1)\overset{S}{\to}(\Omega_2,\mathscr{F}_2)\overset{T}{\to}(\Omega_3,\mathscr{F}_3)$ is measurable.
\end{remark}

\begin{lemma}[Pushforward measure]\label{lemma:1.29} Let $(\Omega_1,\mathscr{F}_1)$ and $(\Omega_2,\mathscr{F}_2)$ be two measurable spaces. If $\mu:\mathscr{F}_1\to\overline{\mathbb{R}}_+$ is a measure on $(\Omega_1,\mathscr{F}_1)$, and $T:\Omega_1\to\Omega_2$ is a measurable function, then $T_*\mu:\mathscr{F}_2\to\overline{\mathbb{R}}_+,\ A\mapsto \mu(T^{-1}A)$ is a measure on $(\Omega_2,\mathscr{F}_2)$, called the \textbf{pushforward} of $\mu$.
\end{lemma}
\begin{proof}
This lemma immediately follows from \hyperref[prop:1.27]{Proposition 1.27 (i) and (iii)}.
\end{proof}
\begin{remark}
A function $T:(\Omega_1,\mathscr{F}_1,\mu_1)\to(\Omega_2,\mathscr{F}_2,\mu_2)$ is said to be \textit{measure preserving} if $\mu_2=T_*\mu_1$. In other words, the measure of any measurable set $A\in\mathscr{F}_2$ does not change after inverse transformation.
\end{remark}

\newpage
\begin{definition}\label{def:1.30} Let $(\Omega,\mathscr{F})$ be a measurable space.
\begin{itemize}
	\item[(i)] A real-valued function $f:\Omega\to\mathbb{R}$ is said to be \textit{measurable} if $f^{-1}(B)\in\mathscr{F}$ for all $B\in\mathscr{B}(\mathbb{R})$. In other words, the function $f:(\Omega,\mathscr{F})\to(\mathbb{R},\mathscr{B}(\mathbb{R}))$ is measurable.
	\item[(ii)] An extended real-valued function $f:\Omega\to\overline{\mathbb{R}}:=\mathbb{R}\cup\{-\infty,\infty\}$ is said to be \textit{measurable} if the sets $\{\omega:f(\omega)=-\infty\}$ and $\{\omega:f(\omega)=\infty\}$ are measurable, and the following real-valued function is measurable:
	\begin{align*}
		f_1(\omega)=\begin{cases}
			f(\omega),\ &\textit{if}\ f(\omega)\in\mathbb{R};\\
			0,\ &\textit{otherwise}
		\end{cases}
	\end{align*}
\end{itemize}
\end{definition}
\paragraph{Remark.} We can generalize (i) to any topological space $(X,\mathscr{T})$, where a Borel $\sigma$-algebra can be defined.

\begin{proposition}[Characterization of real-valued measurable functions]\label{prop:1.31} Let $(\Omega,\mathscr{F})$ be a measurable space, and $f:\Omega\to\mathbb{R}$. The following are equivalent: 
\begin{itemize}
	\item[(i)] $\{\omega:f(\omega)>\alpha\}$ is measurable for all $\alpha\in\mathbb{R}$; 
	\item[(ii)] $\{\omega:f(\omega)\geq\alpha\}$ is measurable for all $\alpha\in\mathbb{R}$; 
	\item[(iii)] $\{\omega:f(\omega)<\alpha\}$ is measurable for all $\alpha\in\mathbb{R}$; 
	\item[(iv)] $\{\omega:f(\omega)\leq\alpha\}$ is measurable for all $\alpha\in\mathbb{R}$; 
	\item[(v)] $f$ is a measurable function.
\end{itemize}
\end{proposition}
\begin{proof}
Clearly (i) and (iii) are equivalent. It is easy to see that (i) and (ii) are equivalent, since
\begin{align*}
	\{\omega:f(\omega)>\alpha\} = \bigcap_{n=1}^\infty\left\{\omega:f(\omega)\geq\alpha+\frac{1}{n}\right\},\ \{\omega:f(\omega)\geq\alpha\} = \bigcap_{n=1}^\infty\left\{\omega:f(\omega)>\alpha-\frac{1}{n}\right\}.
\end{align*}
Similarly (iii) and (iv) are equivalent. Then it remains to show (i)-(iv) $\Rightarrow$ (v).\vspace{0.1cm}

Let $\mathscr{A} = \left\{A\subset\mathbb{R}:f^{-1}(A)\in\mathscr{F}\right\}$. Clearly, $\mathscr{A}$ is a $\sigma$-algebra. Then it suffices to show that $\mathscr{A}$ contains all open intervals: for all $\alpha<\beta$, $f^{-1}((\alpha,\beta)) = \{\omega:f(\omega)<\beta\}\cap\{\omega:f(\omega)>\alpha\}\in\mathscr{F}$.
\end{proof}
\paragraph{Remark.} By definition, all constant functions, indicator functions, continuous functions (the inverse images of open sets remain open) and monotone functions on $\mathbb{R}$ are measurable. Furthermore, this proposition remains true for extended real-valued functions $f:\Omega\to\overline{\mathbb{R}}$.

\begin{definition}\label{def:1.32} Given a function $f:\Omega\to\mathbb{R}$, define $f^+=\max\{f,0\}$ to be the \textit{positive part} of $f$, and define $f^-=\max\{-f,0\}$ to be the \textit{negative part} of $f$. Then we have
\begin{align*}
	f=f^+ - f^-,\ \vert f\vert=f^+ + f^-.
\end{align*}
\end{definition}

\begin{proposition}\label{prop:1.33} Let $(\Omega,\mathscr{F})$ be a measurable space. Let $f$ and $g$ be two real-valued measurable functions. Let $\alpha\in\mathbb{R}$. The following functions are measurable: $f^+,\ f^-,\ \vert f\vert,\ \alpha f,\ f+g,\ fg.$
\end{proposition}
\begin{proof}
Clearly, $f^+,f^-,\vert f\vert,\vert f\vert^2$ and $\alpha f$ are measurable. To show $f+g$ is measurable, note that
\begin{align*}
	\left\{\omega:f(\omega)+g(\omega)>\alpha\right\} = \bigcup_{r_n\in\mathbb{Q}}\{\omega:f(\omega)>r_n\}\cap\{\omega:g(\omega)>\alpha - r_n\}\in\mathscr{B}(\mathbb{R}).
\end{align*}
To show $fg$ is measurable, note that $(f+g)^2 - \vert f\vert^2 - \vert g\vert^2=2fg$ is measurable.
\end{proof}
\begin{remark}
	The proposition also holds for extended real-valued $f$ and $g$. (Note $f+g$ should be well-defined.)
\end{remark}

\begin{proposition}\label{prop:1.34} Given a measurable space $(\Omega,\mathscr{F})$ and a sequence of measurable functions $f_n:\Omega\to\overline{\mathbb{R}}$, $n\in\mathbb{N}$, then following functions are also measurable:
\begin{align*}
	g_1(\omega)=\sup_{n\geq 1}f_n(\omega),\ \ g(\omega)=\limsup_{n\to \infty}f_n(\omega),\ \ h_1(\omega)=\inf_{n\geq 1}f_n(\omega),\ \ h(\omega)=\liminf_{n\to \infty}f_n(\omega).
\end{align*}
\end{proposition}
\begin{proof}
Define $g_k(\omega)=\sup_{n\geq k}f_n(\omega)$, $k\in\mathbb{N}$. Then $(g_k)_{k=1}^\infty$ is a decreasing sequence. For all $\alpha\in\mathbb{R}$,
\begin{align*}
	\{\omega:g_k(\omega)\geq\alpha\} = \bigcup_{n=k}^\infty\{\omega:f_n(\omega)\geq\alpha\}\in\mathscr{F}.
\end{align*}
Hence $g_k$ is measurable. Similarly, $h_k(\omega)=\inf_{n\geq k}f_n(\omega)$ is an increasing sequence of measurable functions. Furthermore,
\begin{align*}
	g(\omega) = \lim_{k\to\infty} g_k(\omega) = \inf_{k\geq 1} g_k(\omega),\ h(\omega) = \lim_{k\to\infty} h_k(\omega) = \sup_{k\geq 1} h_k(\omega)
\end{align*}
are also measurable.
\end{proof}
\begin{remark}
	Following the result above, If $\{f_n:\Omega\to\overline{\mathbb{R}},\ n\in\mathbb{N}\}$ is a sequence of measurable functions that converges pointwise to a function $f:\Omega\to\overline{\mathbb{R}}$, then $f$ is also measurable.
\end{remark}

Sometimes we are also interested in the measurability of vector-valued functions.

\begin{theorem}\label{thm:1.35} Let $(\Omega,\mathscr{F})$ be a measurable space. Let $X$ and $Y$ be two second-countable measurable spaces. A vector-valued function $f=(f_X,f_Y):\Omega\to X\times Y$ is measurable if and only if its two components $f_X$ and $f_Y$ are both measurable.
\end{theorem}
\begin{proof}
If $f=(f_X,f_Y)$ is measurable, consider the projection map $\pi_X:X\times Y\to Y,\ (x,y)\mapsto x$. Clearly, $\pi_X$ is continuous, hence is measurable. Then $f_X=\pi_X\circ f$ is measurable. The same holds for $f_Y$.

Conversely, let $\{U_m\}_{n=1}^\infty$ be a topological basis for $X$, and $\{V_n\}_{n=1}^\infty$ a topological basis for $Y$. For an open set $W$ in $X\times Y$, it can be written as a countable union of some basis elements:
\begin{align*}
	W=\bigcup_{k=1}^\infty U_{m_k}\times V_{n_k}\ \Rightarrow\ f^{-1}(W)=\bigcup_{k=1}^\infty\left(f_X^{-1}(U_{m_k})\cap f_Y^{-1}(V_{n_k})\right).
\end{align*}
Since $f_X$ and $f_Y$ are measurable, $f^{-1}(W)\in\mathscr{F}$ for all open set $W\subset X\times Y$. Since $f$ preserves set operations (intersection, union and complement), we have $f^{-1}(W)\in\mathscr{F}$ for all Borel set $W$ in $X\times Y$.
\end{proof}

\paragraph{Remark.} By induction, a real-vector-valued function $f=(f_1,\cdots,f_n)$ is measurable if and only if each of its components $f_k$ is measurable.

\begin{theorem}[Simple function approximation]\label{thm:1.36}. Let $(\Omega,\mathscr{F})$ be a measurable space. A \textbf{(measurable) simple function} $\varphi$ is a finite linear combination of indicator functions of measurable sets. That is, there exists $A_1,\cdots,A_n\in\mathscr{F}$ and $c_1,\cdots,c_n\in\mathbb{R}$ such that
\begin{align*}
	\varphi = \sum_{k=1}^n c_k\chi_{A_k}. \tag{1.5}\label{eq:1.5}
\end{align*}

Let $(\Omega,\mathscr{F})$ be a measurable space, and let $f:\Omega\to\mathbb{R}$ be a nonnegative measurable function. Then there exists an increasing sequence $(\varphi_n)_{n=1}^\infty$ of measurable functions such that $f(\omega)=\lim_{n\to\infty}\varphi_n(\omega)$ for all $\omega\in\Omega$. Namely, $\varphi_n$ converges pointwise to $f$. Furthermore, if there exists $M>0$ such that $f(\omega)\leq M$ for all $\omega\in\Omega$, then we are able to choose $\varphi_n$ that converges uniformly to $f$.
\end{theorem}
\begin{proof}
For each $n\in\mathbb{N}$ and $0\leq k< 4^n$, define
\begin{align*}
	E_{n,k} = \left\{\omega:2^{-n}k\leq f(\omega) < 2^{-n}(k+1)\right\},\ E_{n,4^n}=\left\{\omega:f(\omega)\geq 2^n\right\}.
\end{align*}
Then choose a nonnegative measurable simple function $\varphi_n$ as follows:
\begin{align*}
	\varphi_n=\sum_{k=0}^{4^n}\frac{k}{2^n}\chi_{E_{n,k}}\ \Rightarrow\ \varphi_n(\omega) = \max_{k\in\{0,1,\cdots,4^n\}}\left\{2^{-n}k:2^{-n}k\leq f(\omega)\right\},\ \forall\omega\in\Omega.
\end{align*}
Clearly, $\varphi_n$ is increasing, and $\varphi_n(\omega)\to f(\omega)$ for all $\omega\in\Omega$. If there exists $M>0$ such that $f(\omega)\leq M$ for all $\omega\in\Omega$, then $E_{n,4^n}=\emptyset$ once $2^n> N$, and $\vert f(\omega)-\varphi_n(\omega)\vert < 2^{-n}$ for all $\omega\in\Omega$.
\end{proof}

\begin{remark} If $f$ is a measurable function, we can extract its positive part $f^+=\max\{f,0\}$ and negative part $f^-=\max\{-f,0\}$. By approaching $f^+$ and $f^-$ respectively, we obtain a simple function approximation for $f$.
\end{remark}

The following theorem shows that a pointwise convergent function sequence almost converges uniformly. It is also known as the second statement of the Littlewood's three principles for real analysis.

\begin{theorem}[Egoroff]\label{thm:1.37} Let $(\Omega,\mathscr{F},\mu)$ be a \uwave{finite} measure space. Let $f_n:\Omega\to\mathbb{R},\ n\in\mathbb{N}$ be a sequence of measurable functions that converges $\mu$-almost everywhere to $f:\Omega\in\mathbb{R}$. Then for all $\epsilon>0$, there exists $E\in\mathscr{F}$ such that $\mu(\Omega\backslash E)<\epsilon$ and that $f_n$ converges to $f$ uniformly on $E$.
\end{theorem}
\begin{proof}
Choose $\Omega_0\in\mathscr{F}$ such that $\mu(\Omega\backslash\Omega_0)=0$ and $f_n(x)\to f(x)$ everywhere on $\Omega_0$. For all $n,k\in\mathbb{N}$, define
\begin{align*}
	A_{k,n}:=\left\{\omega:\vert f_n(\omega)-f(\omega)\vert\geq\frac{1}{k}\right\},\ \ B_{k,n}:=\bigcup_{j=n}^\infty A_{k,j},\ \ A_k:=\bigcap_{n=1}^\infty B_{k,n}.
\end{align*}

If $\omega_0\in\Omega_0$, there exists $N>0$ such that $\vert f_n(\omega_0)-f(\omega_0)\vert < k^{-1}$ for all $n\geq N$. Then $\omega_0\notin B_{k,N}$, and $\omega_0\notin A_k$ for all $k\in\mathbb{N}$. This implies $\bigcup_{k=1}^\infty A_k\subset \Omega\backslash\Omega_0$. Since $\mu$ is finite, we have
\begin{align*}
	\lim_{n\to\infty} \mu(B_{k,n}) = \mu\left(\bigcap_{n=1}^\infty B_{k,n}\right) = \mu(A_k) = 0\ \Rightarrow\ \exists N_k>0\ \textit{such that}\ \mu\left(B_{k,N_k}\right)< 2^{-k}\epsilon.
\end{align*}

Let $E=\Omega\backslash\left(\bigcup_{k=1}^\infty B_{k,N_k}\right)\in\mathscr{F}$. Then $\mu(\Omega\backslash E)<\epsilon$. Furthermore, for all $\omega\in E$, $\omega\notin B_{k,N_k}$ for all $k\in\mathbb{N}$. In other words, given any $k\in\mathbb{N}$, we have $\vert f_n(\omega)-f(\omega)\vert < k^{-1}$ for all $n\geq N_k$ and all $\omega\in E$. Hence $E$ is the desired set on which $f_n$ converges uniformly to $f$.
\end{proof}

\newpage
\subsection{Lebesgue Integration}
If not specifically indicated, our discussion is based on the measure space $(\Omega,\mathscr{F},\mu)$.
\begin{definition}[Lebesgue integral for nonnegative measurable functions]\label{def:1.38} 
A simple function $\varphi:\Omega\to\mathbb{R}$ takes only finitely many values $a_1,\cdots,a_n\in\mathbb{R}$. Hence it has the unique \textit{standard expression}:
\begin{align*}
	\varphi=\sum_{k=1}^n a_k\chi_{A_k},\ \textit{where}\ A_k=\{\omega:\varphi(\omega)=a_k\}. \tag{1.6}\label{eq:1.6}
\end{align*}
For any nonnegative simple function $\varphi:\Omega\to\mathbb{R}_+$ with standard expression \hyperref[eq:1.6]{(1.6)}, we define
\begin{align*}
	S(\varphi) = \sum_{k=1}^n a_n\mu(A_n).
\end{align*}
Given a nonnegative measurable function $f:\Omega\to\overline{\mathbb{R}}_+$, define its \textit{Lebesgue integral} on $\Omega$ as follows:
\begin{align*}
	\int f\,\d\mu = \sup\left\{S(\varphi):0\leq\varphi\leq f,\ \varphi\ \textit{is a measurable simple function}\right\}.
\end{align*}
In addition, given $A\in\mathscr{F}$, define
\begin{align*}
	\int_A f\,\d\mu = \int f\chi_A\,\d\mu.
\end{align*}
\end{definition}
\begin{remark} Recall the convention $\infty\cdot 0=0$. These integrals are well defined but may take value $\infty$. Moreover, we can immediately check that $\int\varphi\,\d\mu=S(\varphi)$ for all measurable simple functions $\varphi$.
\end{remark}

\begin{proposition}\label{prop:1.39} If $f$ and $g$ are nonnegative measurable functions such that $f\leq g$, then $\int f\,\d\mu\leq\int g\,\d\mu$. It is to check by definition of Lebesgue integral.
\end{proposition}

\paragraph{} We have the following important theorem, which ensures the interchangeability of limit and integration.

\begin{theorem}[Monotone convergence theorem/Levi's theorem]\label{thm:1.40} Let $(f_n)_{n=1}^\infty$ be a monotone increasing sequence of nonnegative measurable functions, and let $f(\omega)=\lim_{n\to\infty} f_n(\omega)$ for all $\omega\in\Omega$. Then
\begin{align*}
	\int f\,\d\mu = \lim_{n\to\infty}\int f_n\,\d\mu.
\end{align*}
\end{theorem}
\begin{proof}
By \hyperref[prop:1.39]{Proposition 1.39}, $\int f_n\,\d\mu$ is monotone increasing, and 
\begin{align*}
	\lim_{n\to\infty}\int f_n\,\d \mu = \sup_{n\geq 1}\int f_n\,\d \mu\leq\int f\,\d \mu.\tag{1.7}\label{eq:1.7}
\end{align*}

Now we prove the opposite. Let $0<\alpha<1$, and let $\varphi$ be any simple function such that $0\leq\varphi\leq f$. Take $A_n:=\{\omega:f_n(\omega)\geq\alpha\varphi(\omega)\}$, which is an increasing sequence in $\mathscr{F}$ such that $\Omega=\bigcup_{n=1}^\infty A_n$. Note that $f_n$ is nonnegative, and $\varphi$ is simple. Then
\begin{align*}
	\int f_n\,\d \mu\geq\int_{A_n}f_n\,\d \mu\geq\alpha\int_{A_n}\varphi\,\d \mu\ \ \overset{n\to\infty}{\Rightarrow}\ \ \lim_{n\to\infty}\int f_n\,\d \mu \geq\alpha\int\varphi\,\d \mu\ \ \overset{\alpha\to 1}{\Rightarrow}\ \ \lim_{n\to\infty}\int f_n\,\d \mu \geq\int\varphi\,\d \mu.
\end{align*}

Since the simple function $0\leq\varphi\leq f$ is arbitrary, by definition of Lebesgue integral, we complete the proof of the opposite of \hyperref[eq:1.7]{(1.7)}.
\end{proof}

\begin{proposition}\label{prop:1.41} For all nonnegative measurable functions $f$ and $g$ and all $\alpha,\beta\in\mathbb{R}_+$,
\begin{align*}
	\int(\alpha f + \beta g)\,\d \mu = \alpha\int f\,\d \mu + \beta\int g\,\d \mu.
\end{align*}
\end{proposition}
\begin{proof}
The equality is clear when $f$ and $g$ is simple. In general case, use simple function approximation and monotone convergence theorem.
\end{proof}

\begin{proposition}\label{prop:1.42} Let $f$ and $g$ be nonnegative measurable functions. Then
\begin{align*}
	\int f\,\d \mu = 0\ \Leftrightarrow\ f=0\ a.e..
\end{align*}
Furthermore, if $f=g\ a.e.$, then 
\begin{align*}
	\int f\,\d \mu = \int g\,\d \mu.
\end{align*}
\end{proposition}
\begin{proof}
Let $\varphi=\sum_{k=1}^na_k\chi_{A_k}$ be a simple function such that $0\leq\varphi\leq f$. If $f=0\ a.e.$, then either $\mu(A_k)=0$ or $a_k=0$ for each $k$, which implies $S(\varphi)=0$. By definition of Lebesgue integral, $\int f\,\d \mu=0$.\vspace{0.1cm}

Now assume $\int f\,\d \mu=0$. Take $E=\left\{\omega:f(\omega)>0\right\}$ and $E_n=\left\{\omega:f(\omega)>n^{-1}\right\}$ for all $n\in\mathbb{N}$. Then we have $E=\bigcup_{n=1}^\infty E_n$, and
\begin{align*}
	0\leq\mu(E)=\lim_{n\to\infty}\mu(E_n) \leq \lim_{n\to\infty} n\int_{E_n}f\,\d \mu \leq \lim_{n\to\infty} n\int f\,\d \mu =0\ \ \Rightarrow\ \ \mu(E)=0,\ f=0\ a.e..
\end{align*}

Finally assume $f=g\ a.e.$. Take $h=\max\{f,g\}$, then $h-f$ is a nonnegative measurable function, and $h-f=0\ a.e.$. As a result, $\int h\,\d \mu=\int f\,\d \mu$. Similarly, we have $\int h\,\d \mu=\int g\,\d \mu$.
\end{proof}

The Fatou's lemma is useful when we do not know whether limit and integration are interchangeable.

\begin{theorem}[Fatou's lemma]\label{thm:1.43} Let $(f_n)_{n=1}^\infty$ be a sequence of nonnegative measurable functions, and let $f(\omega)=\liminf_{n\to\infty} f_n(\omega)$ for all $\omega\in\Omega$. Then
\begin{align*}
	\int f\,\d \mu \leq \liminf_{n\to\infty}\int f_n\,\d \mu.\tag{1.8}\label{eq:1.8}
\end{align*}
\end{theorem}
\begin{proof}
Let $g_n(\omega)=\inf_{k\geq n} f_k(\omega)$. Then $g$ is measurable, and \begin{align*}
	\int g_n\,\d \mu\leq\int f_k\,\d \mu,\ \forall k\geq n\ \Rightarrow\ \int g_n\,\d \mu\leq\inf_{k\geq n}\int f_k\,\d \mu.
\end{align*}
Furthermore, $(g_n)_{n=1}^\infty$ is a monotone increasing sequence converging to $f$. By monotone convergence theorem,
\begin{align*}
	\int f\,\d \mu = \lim_{n\to\infty}\int g_n\,\d \mu \leq \lim_{n\to\infty} \inf_{k\geq n}\int f_k\,\d \mu.
\end{align*}
This is indeed the inequality \hyperref[eq:1.8]{(1.8)}.
\end{proof}
\paragraph{Remark.} Even though $f=\lim_{n\to\infty} f_n$ (pointwise), limit and integration are not interchangeable in general. For example, let $f_n=n\chi_{[0,n^{-1}]}$. Then $f=\lim_{n\to\infty} f_n=\infty\chi_{\{0\}}$, but
\begin{align*}
	0=\int f\,\d \mu < \lim_{n\to\infty}\int f_n d\mu = 1.
\end{align*}

\begin{definition}[Lebesgue integrable functions]\label{def:1.44} A measurable function $f$ is said to be \textit{integrable} if
\begin{align*}
	\int f^+\,\d \mu < \infty\ \ \textit{and}\ \ \int f^-\,\d \mu<\infty.
\end{align*}
We denote by $L^1(\Omega,\mathscr{F},\mu)$ the set of all integrable functions. The \textit{Lebesgue integral} of $f$ is defined as
\begin{align*}
	\int f\,\d \mu = \int f^+\,\d \mu - \int f^-\,\d \mu \in\mathbb{R}.
\end{align*}
In addition, given $A\in\mathscr{F}$, define
\begin{align*}
	\int_A f\,\d \mu = \int f\chi_A\,\d \mu.
\end{align*}
\end{definition}
\begin{remark}
A measurable function $f$ is said to be \textit{quasi-integrable} if at least one of $f^+$ and $f^-$ is integrable. In this case, the Lebesgue integral of $f$ takes value in $\overline{\mathbb{R}}=\mathbb{R}\cup\{-\infty,\infty\}$.
\end{remark}

\begin{proposition}[Linearity of Lebesgue integral]\label{prop:1.45} For all $f,g\in L^1(\Omega,\mathscr{F},\mu)$ and all $\alpha,\beta\in\mathbb{R}$,
\begin{align*}
	\int(\alpha f + \beta g)\,\d \mu = \alpha\int f\,\d \mu + \beta\int g\,\d \mu.
\end{align*}
\end{proposition}
\begin{proof}
By \hyperref[prop:1.41]{Proposition 1.41},
\begin{align*}
	\int\alpha f\,\d \mu = \int(\alpha f)^+\,\d \mu - \int(\alpha f)^-\,\d \mu = \alpha\int f^+\,\d \mu - \alpha\int f^-\,\d \mu = \alpha\int f\,\d \mu.
\end{align*}
Let $A=\{f\geq 0,\ g\geq 0\}$, $B=\{f< 0,\ g< 0\}$, $P_1=\{f\geq 0, g<0,\ f+g\geq 0\}$, $P_2=\{f<0, g\geq 0,\ f+g\geq 0\}$, $N_1=\{f< 0, g\geq 0,\ f+g< 0\}$, $N_2=\{f\geq 0, g<0,\ f+g< 0\}$. Then $\Omega=A\cup B\cup P_1\cup P_2\cup N_1\cup N_2$, and
\begin{align*}
	\int (f+g)\,\d \mu &= \int (f+g)^+\,\d \mu - \int (f+g)^-\,\d \mu\\
	&= \int_A (f^+ + g^+)\,\d \mu + \int_{P_1} (f^+ - g^-)\,\d \mu + \int_{P_2} (g^+ - f^-)\,\d \mu \\
	&\quad - \int_B (f^- + g^-)\,\d \mu - \int_{N_1} (f^- - g^+)\,\d \mu - \int_{N_2} (g^- - f^+)\,\d \mu\\
	&= \int_A f^+\,\d \mu + \int_{P_1} f^+\,\d \mu + \int_{N_2} f^+\,\d \mu - \int_B f^-\,\d \mu - \int_{P_2} f^-\,\d \mu - \int_{N_1} f^-\,\d \mu\\
	&\quad + \int_A g^+\,\d \mu + \int_{P_2} g^+\,\d \mu + \int_{N_1} g^+\,\d \mu - \int_B g^-\,\d \mu - \int_{P_1} g^-\,\d \mu - \int_{N_2} g^-\,\d \mu\\
	&= \int f^+\,\d \mu - \int f^-\,\d \mu + \int g^+\,\d \mu - \int g^-\,\d \mu = \int f\,\d \mu + \int g\,\d \mu.
\end{align*}
Hence the linearity follows.
\end{proof}

\begin{proposition}\label{prop:1.46} Let $f$ be a measurable function. Then $f\in L^1(\Omega,\mathscr{F},\mu)$ if and only if $\vert f\vert\in L^1(\Omega,\mathscr{F},\mu)$. In that case,
\begin{align*}
	\left\vert\int f\,\d \mu\right\vert \leq \int\vert f\vert\,\d \mu.\tag{1.9}\label{eq:1.9}
\end{align*}
\end{proposition}
\begin{proof}
Note that $f=f^+-f^-$, $\vert f\vert=f^+ + f^-$. Then $f$ and $\vert f\vert$ is integrable if and only if $f^+$ and $f^-$ is integrable. Moreover, \hyperref[eq:1.9]{(1.9)} follows from the triangle inequality.
\end{proof}

The Lebesgue's dominated convergence theorem ensures interchangeability of limit and integration.

\begin{theorem}[Lebesgue's dominated convergence theorem]\label{thm:1.47} Let $(f_n)_{n=1}^\infty$ be a sequence of measurable functions such that $f_n\to f\ a.e.$, where $f$ is also a measurable function. If there exists $g\in L^1(\Omega,\mathscr{F},\mu)$ such that $\vert f_n\vert\leq g$ for all $n\in\mathbb{N}$, then all functions $f_n$ and $f$ are integrable, and
\begin{align*}
	\int f\,\d \mu = \lim_{n\to\infty}\int f_n\,\d \mu.
\end{align*}
\end{theorem}
\begin{proof}
We may assume $f_n\to f$ pointwise by redefining $f$ and $f_n$ on a set of measure zero. For all $n\in\mathbb{N}$, we have $\vert f_n\vert\leq g$, then $\vert f\vert\leq g$. By \hyperref[prop:1.46]{Proposition 1.46}, all functions $f_n$ and $f$ are integrable.

Since $g+f_n\geq 0$, and $g-f_n\geq 0$, apply Fatou's lemma (\hyperref[thm:1.43]{Theorem 1.43}):
\begin{align*}
	&\int (g+f)\,\d \mu \leq \liminf_{n\to\infty}\int (g+f_n)\,\d \mu = \int g\,\d \mu + \liminf_{n\to\infty}\int f_n\,\d \mu,\\
	&\int (g-f)\,\d \mu \leq \liminf_{n\to\infty}\int (g-f_n)\,\d \mu = \int g\,\d \mu - \limsup_{n\to\infty}\int f_n\,\d \mu.
\end{align*}
Hence we have
\begin{align*}
	\limsup_{n\to\infty}\int f_n\,\d \mu\leq\int f\,\d \mu\leq \liminf_{n\to\infty}\int f_n\,\d \mu,
\end{align*}
and the result follows.
\end{proof}
\paragraph{Remark.} If $(\Omega,\mathscr{F},\mu)$ is a complete measure space, then $f$ is automatically measurable. Inspired by this proof, we summarize another commonly used version of Fatou's lemma as follows.

\begin{corollary}[Fatou's lemma]\label{cor:1.48} Let $(f_n)_{n=1}^\infty$ be a sequence of integrable functions.
\begin{itemize}
\item[(i)] If there exists an integrable function $g$ such that $f_n\geq g$ for all $n\in\mathbb{N}$, then
\begin{align*}
	\int\liminf_{n\to\infty} f_n\,\d \mu \leq \liminf_{n\to\infty}\int f_n\,\d \mu
\end{align*}
\item[(ii)] If there exists an integrable function $g$ such that $f_n\leq g$ for all $n\in\mathbb{N}$, then
\begin{align*}
	 \limsup_{n\to\infty}\int f_n\,\d \mu\leq\int\limsup_{n\to\infty} f_n\,\d \mu.
\end{align*}
\end{itemize}
\end{corollary}

\paragraph{} Now we discuss integral transform among different measure spaces.

\begin{theorem}[Integral transform]\label{thm:1.49} Let $(\Omega_1,\mathscr{F}_1,\mu_1)$ and $(\Omega_2,\mathscr{F}_2,\mu_2)$ be two measure spaces. If function $T: (\Omega_1,\mathscr{F}_1,\mu_1)\to (\Omega_2,\mathscr{F}_2,\mu_2)$ is measure-preserving, i.e. $\mu_2=T_*\mu_1$ is the pushforward of $\mu_1$, then
\begin{align*}
	\int f\circ T\,\d \mu_1 = \int f\,\d \mu_2,\ \forall f\in L^1(\Omega_2,\mathscr{F}_2,\mu_2). \label{eq:1.10}\tag{1.10}
\end{align*}
\end{theorem}
\begin{proof}
For all $A\in\mathscr{F}_2$, we have
\begin{align*}
	\int \chi_A\circ T\,\d \mu_1 = \mu_1\left(T^{-1}A\right) = \mu_2(A) = \int\chi_A\,\d \mu_2.
\end{align*}
Then \hyperref[eq:1.10]{(1.10)} holds for all nonnegative measurable simple functions $f$. Similar to the procedure of defining Lebesgue integral (\hyperref[def:1.38]{Definition 1.38} and \hyperref[def:1.44]{Definition 1.44}), it holds for all $f\in L^1(\Omega_2,\mathscr{F}_2,\mu_2)$.
\end{proof}

Now we discuss Lebesgue integral on product spaces and the interchange of integrals. We first present the general conclusion in \hyperref[thm:1.50]{Theorem 1.50}.

\begin{theorem}[Fubini's theorem]\label{thm:1.50} Let $f\in L^1(\Omega_1\times\Omega_2,\mathscr{F}_1\otimes\mathscr{F}_2,\mu_1\otimes\mu_2)$.
\begin{itemize}
	\item[(i)] For all $\omega_1\in\Omega_1$, the function $\omega_2\mapsto f(\omega_1,\omega_2)$ is integrable.
	\item[(ii)] The function $\omega_1\mapsto\int_{\Omega_2} f(\omega_1,\omega_2)\,\d \mu_2(\omega_2)$ is integrable.
	\item[(iii)] For all $\omega_2\in\Omega_1$, the function $\omega_1\mapsto f(\omega_1,\omega_2)$ is integrable.
	\item[(iv)] The function $\omega_2\mapsto\int_{\Omega_1} f(\omega_1,\omega_2)\,\d \mu_1(\omega_1)$ is integrable.
	\item[(v)] The following integrals are equivalent:
	\begin{align*}
		\int_{\Omega_1\times\Omega_2} f\,\d (\mu_1\otimes\mu_2) = \int_{\Omega_1}\left(\int_{\Omega_2} f(\omega_1,\omega_2)\,\d \mu_2(\omega_2)\right)\d\mu_1(\omega_1) = \int_{\Omega_2}\left(\int_{\Omega_1} f(\omega_1,\omega_2)\,\d \mu_1(\omega_1)\right)\d\mu_2(\omega_2).
	\end{align*}
\end{itemize}
\end{theorem}

\paragraph{} The proof of Fubini's theorem uses Tonelli theorem. We first prove the following proposition.

\begin{proposition}\label{prop:1.51} Let $(\Omega_1,\mathscr{F}_1,\mu_1)$ and $(\Omega_2,\mathscr{F}_2,\mu_2)$ be $\sigma$-finite measure spaces, and denote their product space by $(\Omega_1\times\Omega_2,\mathscr{F}_1\otimes\mathscr{F}_2,\mu_1\otimes\mu_2)$. For all $A\in\mathscr{F}_1\otimes\mathscr{F}_2$, we define the \textit{slices}
\begin{align*}
	A_{\omega_1} = \left\{\omega_2\in\Omega_2:(\omega_1,\omega_2)\in A\right\}\ \textit{and}\ A^{\omega_2} = \left\{\omega_1\in\Omega_1:(\omega_1,\omega_2)\in A\right\}.
\end{align*}
Then the following hold for all $A\in\mathscr{F}_1\otimes\mathscr{F}_2$:
\begin{itemize}
	\item[(i)] $A_{\omega_1}\in\mathscr{F}_2$ for all $\omega_1\in\Omega_1$;
	\item[(ii)] The function $f_A:\omega_1\mapsto\mu_2(A_{\omega_1})$ is measurable;
	\item[(iii)] $(\mu_1\otimes\mu_2)(A)=\int_{\Omega_1}\mu_2(A_{\omega_1})\,\d \mu_1(\omega_1):=\int_{\Omega_1} f_A\,\d \mu_1$.
\end{itemize}
Symmetric statements also holds for slices $A^{\omega_2}$.
\end{proposition}
\begin{proof}
Denote by $\mathscr{M}$ the collection of all subsets of $\Omega_1\times\Omega_2$ which satisfy (i), (ii) and (iii). We prove that $\mathscr{M}\supset\mathscr{F}_1\otimes\mathscr{F}_2$. Clearly, all measurable rectangles in $\Omega_1\times\Omega_2$ satisfy (i), (ii) and (iii). Hence $\mathscr{M}\supset\mathscr{F}_1\times\mathscr{F}_2$. \vspace{0.1cm}

\item\textit{Step I:} We prove that for any increasing sequence $(A_n)_{n=1}^\infty$ of sets in $\mathscr{M}$, it holds $A:=\bigcup_{n=1}^\infty A_n\in\mathscr{M}$.

(i) For all $\omega_1\in\Omega_1$, we have
\begin{align*}
	A_{\omega_1}=\left(\bigcup_{n=1}^\infty A_n\right)_{\omega_1} = \bigcup_{n=1}^\infty (A_n)_{\omega_1}\in\mathscr{F}_2;
\end{align*}

(ii) Note that $(A_n)_{\omega_1}$ is an increasing sequence, we have
\begin{align*}
	\mu_2(A_{\omega_1}) = \lim_{n\to\infty}\mu_2\left((A_n)_{\omega_1}\right)\ \ \Rightarrow\ \ f_A=\lim_{n\to\infty} f_n\ \textit{is measurable}.
\end{align*}

(iii) Note that $f_{A_n}$ is monotone increasing, by monotone convergence theorem,
\begin{align*}
	(\mu_1\otimes\mu_2)(A) = \lim_{n\to\infty}(\mu_1\otimes\mu_2)(A_n) = \lim_{n\to\infty}\int_{\Omega_1}f_{A_n}\,\d \mu_1 = \int_{\Omega_1} f\,\d \mu_1.
\end{align*}

\item\textit{Step II:} Similar to Step I, we can prove that for any decreasing sequence $(B_n)_{n=1}^\infty$ of sets in $\mathscr{M}$ such that $\mu_1((B_1)_{\omega_1})<\infty$ for all $\omega_1\in\Omega_1$ and $(\mu_1\otimes\mu_2)(B_1)<\infty$, it holds  $B:=\bigcap_{n=1}^\infty B_n\in\mathscr{M}$.\vspace{0.1cm}

\item\textit{Step III:} We prove that for any sequence $(E_n)_{n=1}^\infty$ of disjoint sets in $\mathscr{M}$, it holds $\bigcup_{n=1}^\infty E_n\in\mathscr{M}$. Clearly, if $E,F$ are disjoint sets in $\mathscr{M}$, we have $E\cup F\in\mathscr{M}$. Then our result immediately follows from Step I by choosing increasing sequence $A_n:=\bigcup_{k=1}^n E_k$ in $\mathscr{M}$. \vspace{0.1cm}

\item\textit{Step IV:} Denote by $\mathscr{A}$ the collection of all finite unions of measurable rectangles in $\Omega_1\times\Omega_2$. Then $\mathscr{A}$ is an algebra. By $\sigma$-finiteness of $\Omega_1$, choose an increasing sequence $(X_n)_{n=1}^\infty$ such that $\mu_1(X_n)<\infty$ for all $n\in\mathbb{N}$ and $\Omega_1=\bigcup_{n=1}^\infty X_n$. Similarly choose an increasing sequence $(Y_n)_{n=1}^\infty$ for $\Omega_2$.

Let $\mathscr{A}_n=\{A\in\mathscr{A}:A\subset X_n\times Y_n\}$, and $\mathscr{M}_n=\{M\in\mathscr{M}:M\subset X_n\times Y_n\}$. Then $\mathscr{A}_n\subset\mathscr{M}_n$. Clearly, $\mathscr{A}_n$ is an algebra, and $\mathscr{M}_n$ is a monotone class by Steps I and II. By monotone class theorem, $\sigma(\mathscr{A}_n)\subset\mathscr{M}_n$. Since $\sigma(\mathscr{A}_n)$ contains all measurable subsets of $X_n\times Y_n$, so does $\mathscr{M}$. As a result, $\mathscr{M}\supset\mathscr{M}_n$ contains all measurable subsets of $X_n\times Y_n$ for all $n\in\mathbb{N}$. Since $\mathscr{M}$ is closed under countable unions, $\mathscr{M}\supset\mathscr{F}_1\otimes\mathscr{F}_2$.
\end{proof}

The Tonelli's theorem gives the integral of nonnegative functions on product measure spaces.
\begin{theorem}[Tonelli's theorem]\label{thm:1.52} Let $f:(\Omega_1\times\Omega_2,\mathscr{F}_1\otimes\mathscr{F}_2,\mu_1\otimes\mu_2)\to\overline{\mathbb{R}}_+$ be a measurable function.
\begin{itemize}
	\item[(i)] For all $\omega_1\in\Omega_1$, the function $\omega_2\mapsto f(\omega_1,\omega_2)$ is measurable.
	\item[(ii)] The function $\omega_1\mapsto\int_{\Omega_2} f(\omega_1,\omega_2)\,\d \mu_2(\omega_2)$ is measurable.
	\item[(iii)] For all $\omega_2\in\Omega_1$, the function $\omega_1\mapsto f(\omega_1,\omega_2)$ is measurable.
	\item[(iv)] The function $\omega_2\mapsto\int_{\Omega_1} f(\omega_1,\omega_2)\,\d \mu_1(\omega_1)$ is measurable.
	\item[(v)] The following integrals are equivalent:
	\begin{align*}
		\int_{\Omega_1\times\Omega_2} f\,\d (\mu_1\otimes\mu_2) = \int_{\Omega_1}\left(\int_{\Omega_2} f(\omega_1,\omega_2)\,\d \mu_2(\omega_2)\right)\d\mu_1(\omega_1) = \int_{\Omega_2}\left(\int_{\Omega_1} f(\omega_1,\omega_2)\,\d \mu_1(\omega_1)\right)\d\mu_2(\omega_2).
	\end{align*}
\end{itemize}
\end{theorem}
\begin{proof}
By \hyperref[prop:1.51]{Theorem 1.51}, the theorem holds for all indicator functions $\chi_A$, where $A\in\mathscr{F}_1\times\mathscr{F}_2$. Consequently, it holds for all nonnegative simple functions $\varphi$. For a general nonnegative measurable function $f$, choose a monotone increasing sequence $\varphi_n$ of nonnegative simple functions such that $f=\lim_{n\to\infty}\varphi_n$. Applying monotone convergence theorem, we know that the theorem holds for $f$.
\end{proof}
\renewcommand{\proofname}{Proof of Fubini's theorem}
\begin{proof}
Since $f=f^+-f^-$, using Tonelli's theorem to $f^+$ and $f^-$ completes the proof.
\end{proof}
\renewcommand{\proofname}{Proof}

\paragraph{Remark.} If $f\notin L^1(\Omega_1\times\Omega_2,\mathscr{F}_1\otimes\mathscr{F}_2,\mu_1\otimes\mu_2)$, we cannot change the order of integration. For example, consider the function on $f:[0,1]\times[0,1]\to\overline{\mathbb{R}}$:
\begin{align*}
	f(x,y)=\frac{x^2-y^2}{(x^2+y^2)^2} = -\frac{\partial^2}{\partial x\partial y}\arctan\left(\frac{y}{x}\right).
\end{align*}
Then
\begin{align*}
	&\int_{[0,1]}\left(\int_{[0,1]} \frac{x^2-y^2}{(x^2+y^2)^2}\,\d y\right)\d x = \int_{[0,1]}\frac{1}{1+x^2}\,\d x = \frac{\pi}{4};\\
	&\int_{[0,1]}\left(\int_{[0,1]} \frac{x^2-y^2}{(x^2+y^2)^2}\,\d x\right)\d y = \int_{[0,1]}\frac{-1}{1+y^2}\,\d y = -\frac{\pi}{4};
\end{align*}
and
\begin{align*}
	\int_{[0,1]}\int_{[0,1]}\left\vert\frac{x^2-y^2}{(x^2+y^2)^2}\right\vert \d x\,\d y=\infty.
\end{align*}

\newpage
\subsection{Signed Measures, Jordan Decomposition and Radon-Nikodym Theorem}
\begin{definition}[Signed measure]\label{def:1.53} Let $(\Omega,\mathscr{F})$ be a measurable space. A \textit{signed measure} $\mu$ on $(\Omega,\mathscr{F})$ is a set function $\mu:\mathscr{F}\to\overline{\mathbb{R}}$ that satisfies the following:
\begin{itemize}
	\item[(i)] $\mu(\emptyset)=0$;
	\item[(ii)] (Countable additivity). If $(A_n)_{n=1}^\infty$ is a sequence of disjoint sets in $\mathscr{F}$, then
	\begin{align*}
		\mu\left(\bigcup_{n=1}^\infty A_n\right) = \sum_{n=1}^\infty \mu(A_n).\tag{1.11}\label{eq:1.11}
	\end{align*}
    When the left-hand side of \hyperref[eq:1.11]{(1.11)} is finite, the right-hand side converges absolutely.
\end{itemize}
A signed measure $\mu$ is said to be \textit{finite} if it only takes values in $\mathbb{R}$. A signed measure $\mu$ is said to be \textit{$\sigma$-finite} if there exists $\{\Omega_n\}_{n=1}^\infty$ such that $\Omega=\bigcup_{n=1}^\infty\Omega_n$, and $-\infty<\mu(\Omega_n)<\infty$ for all $n\in\mathbb{N}$.
\end{definition}
\begin{remark} One immediate consequence of (ii) is that a signed measure $\mu$ may take $\infty$ or $-\infty$ as a value, but it cannot take both, because the expression $\infty - \infty$ is undefined.
\end{remark}

\begin{theorem}[Hahn decomposition theorem]\label{thm:1.54} Let $\mu$ be a signed measure on a measurable space $(\Omega,\mathscr{F})$. Then there exist measurable sets $P,N\in\mathscr{F}$ that satisfy the following:
\begin{itemize}
	\item[(i)] $P\amalg N$ is a division of $\Omega$. (In other words, $P\cup N=\Omega$ and $P\cap N=\emptyset$.)
	\item[(ii)] For all $A\in\mathscr{F}$ with $A\subset P$, $\mu(A)\geq 0$. (In other words, $P$ is a positive set.)
	\item[(iii)] For all $B\in\mathscr{F}$ with $B\subset N$, $\mu(B)\leq 0$. (In other words, $N$ is a negative set.)
\end{itemize}
\end{theorem}
\begin{proof}
	We may assume that $\mu$ does not take $\infty$ as a value. Otherwise apply the following proof on $-\mu$.
	
	Denote by $\mathscr{P}$ the collection of all positive sets in $\mathscr{F}$, then $\emptyset\in\mathscr{P}$. Let $M=\sup_{A\in\mathscr{P}}\mu(A)$, and choose $\{A_n\}_{n=1}^\infty\subset\mathscr{P}$ such that $\mu(A_n)\to M$. Clearly, $P=\bigcup_{n=1}^\infty A_n$ is a positive set, and $\mu(P)=M$. 
	
	We prove that $N:=\Omega\backslash P$ is a negative set. If not, there exists a measurable set $E\subset N$ with $\mu(E)>0$. Clearly, $E$ is not a positive set. (Otherwise, $P\cup E\in\mathscr{P}$, but $\mu(P\cup E)=\mu(P)+\mu(E)>M=\sup_{A\in\mathscr{P}}\mu(A)$, a contradiction!) Hence there exists $B\subset E$ with $\mu(B)<0$. We choose the smallest positive integer $k_1$ such that there exists $B_1\subset E$ with $\mu(B_1)<-k_1^{-1}$. Since $k_1$ is the smallest, once $k_1>1$, any measurable subset $A$ of $E$ satisfies $\mu(A)\geq-(k_1-1)^{-1}$.
	
	Again, $E\backslash B_1$ is not positive. Then we choose the smallest $k_2\in\mathbb{N}$ such that there exists $B_2\subset E\backslash B_1$ with $\mu(B_2)<-k_2^{-1}$. Repeat this procedure, we obtain a sequence $k_n\in\mathbb{N}$ and $B_n\subset\mathscr{F}$ such that 
	\begin{itemize}
		\vspace{0.1cm}
		\item $B_n\subset E\backslash\left(\bigcup_{k=1}^{n-1} B_k\right)$ and $\mu(B_n)<-k_n^{-1}$, and
		\item Once $k_n>1$, any measurable subset $A$ of $E\backslash\left(\bigcup_{k=1}^{n-1} B_k\right)$ satisfies $\mu(A)\geq-(k_n-1)^{-1}$.
	\end{itemize}
	
	Take $C=E\backslash\left(\bigcup_{n=1}^\infty B_n\right)$. By assumption that $\mu$ does not take $\infty$,
	\begin{align*}
		\mu(C) = \mu(E) - \sum_{n=1}^\infty\mu(B_n) = \mu(E) + \sum_{n=1}^\infty\frac{1}{k_n} < \infty\ \Rightarrow\ k_n\to\infty\ \textit{as}\ n\to\infty.
	\end{align*}

	Since any measurable subset $A$ of $C$ satisfies $\mu(E)\geq-\lim_{n\to\infty}(k_n-1)^{-1}=0$, $C$ is a positive set disjoint from $P$. However $\mu(P\cup C)=\mu(P)+\mu(E)+\sum_{n=1}^\infty k_n^{-1}>M=\sup_{A\in\mathscr{P}}\mu(A)$, again a contradiction!
\end{proof}
\paragraph{Remark.} We called a set $E\in\mathscr{F}$ a $\mu$-null set if $\mu(A)=0$ for any measurable subset $A$ of $E$. Following this proof, the Hahn decomposition $P\amalg N$ is unique up to adding to/subtracting $\mu$-null sets from $P$ and $N$: 

Given a Hahn decomposition $P^\prime\amalg N^\prime$, the set $P\cap N^\prime$ is a positive set and also a negative set. The same applies to $N\cap P^\prime$. Then $P\triangle P^\prime = N\triangle N^\prime = (P\cap N^\prime)\cup(N\cap P^\prime)$is a $\mu$-null set.


\begin{corollary}[Jordan decomposition]\label{cor:1.55} Given a signed measure $\mu$ on a measurable space $(\Omega,\mathscr{F})$, take the Hahn decomposition $\Omega=P\amalg N$ on $\mu$. Define
\begin{align*}
	\mu^+(A)=\mu(A\cap P),\ \mu^-(A)=-\mu(A\cap N),\ \forall A\in\mathscr{F}.
\end{align*}
Then $\mu^+$ and $\mu^-$ are two (finite) measures on $(\Omega,\mathscr{F})$, and we have the \textbf{Jordan decomposition} $\mu=\mu^+ - \mu^-$. Since the Hahn decomposition is unique up to the difference of a $\mu$-null set, the Jordan decomposition is unique. The measure $\vert\mu\vert:=\mu^+ + \mu^-$ is called the \textbf{variation} of $\mu$. Its maximum value $\Vert\mu\Vert=\vert\mu\vert(\Omega)$ is called  the \textbf{total variation} of $\mu$. 
\end{corollary}

\paragraph{} Now we discuss the relationship between signed measures and Lebesgue integration. We first introduce the absolute continuity and singularity of signed measures.

\begin{definition}\label{def:1.56} Let $\mu$ be a measure on a measurable space $(\Omega,\mathscr{F})$. 
\begin{itemize}
	\item[(i)] (Absolute continuity). A signed measure $\nu$ is said to be \textit{absolutely continuous with respect to $\mu$}, denoted by $\nu\ll\mu$, if $\nu(A)=0$ for all $A\in\mathscr{F}$ such that $\mu(A)=0$.
	\item[(ii)] (Singularity). A signed measure $\nu$ is said to be \textit{singular with respect to $\mu$}, denoted by $\nu\perp\mu$, if there exists $A\in\mathscr{F}$ such that $\mu(A)=0$ and $\nu(\Omega\backslash A)=0$.
\end{itemize} 
\end{definition}
\paragraph{} The following theorem tells that every measurable function $f$ is associated with a signed measure.

\begin{theorem}\label{thm:1.57} Let $f\in L^1(\Omega,\mathscr{F},\mu)$. Define $\nu:\mathscr{F}\to\mathbb{R}$ by
\begin{align*}
	\nu(A)=\int_A f\,\d \mu,\ \forall A\in\mathscr{F}.
\end{align*}

Then $\nu$ is a (finite) signed measure on $(\Omega,\mathscr{F})$. Furthermore, for any $\epsilon>0$, there exists $\delta>0$ such that for all $A\in\mathscr{F}$ with $\mu(A)<\delta$, we have $\nu(A)<\epsilon$. In particular, $\nu$ is absolutely continuous with respect to $\mu$.
\end{theorem}
\begin{proof}
We may assume $f\geq 0$, and the result follows from $f=f^+-f^-$.

Clearly $\nu(\emptyset)=0$. Let $(A_n)_{n=1}^\infty$ be a sequence of disjoint sets in $\mathscr{F}$. Then $\sum_{k=1}^n f\chi_{A_k}$ is a monotone increasing sequence of nonnegative measurable functions that converges pointwise to $f\chi_A$, where $A=\bigcup_{n=1}^\infty A_n$. By monotone convergence theorem,
\begin{align*}
	\nu(A) = \int f\chi_A\,\d \mu = \lim_{n\to\infty}\sum_{k=1}^n\int f\chi_{A_k}\,\d \mu = \sum_{n=1}^\infty\nu(A_n).
\end{align*}

Thus $\nu$ is a signed measure on $(\Omega,\mathscr{F})$. Now fix $\epsilon>0$. We define $E_n=\{\omega:f(\omega)>n\}$ for all $n\in\mathbb{N}$. Since $f$ is integrable, $\mu(E_n)\to 0$. Again by monotone convergence theorem,
\begin{align*}
	\int f\,\d \mu = \lim_{n\to\infty}\int f\chi_{\Omega\backslash E_n}\,\d \mu\ \Rightarrow\ \exists N>0\ \textit{such that}\ \int_{E_N} f\,\d \mu < \frac{\epsilon}{2}.
\end{align*}
Then for all $A\in\mathscr{F}$ with $\mu(A)<\epsilon/2N$, we have \begin{align*}
	\int_A f\,\d \mu = \int_A f\chi_{\Omega\backslash E_N}\,\d \mu + \int_A f\chi_{E_N}\,\d \mu \leq N\mu(A) + \int_{E_N}f\,\d \mu < \epsilon.
\end{align*}

For the last statement, note that if $\nu(A)>0$, then $\nu(A)\geq\epsilon$ for some $\epsilon>0$, and there exists $\delta>0$ such that $\mu(A)\geq\delta>0$. Hence $\mu(A)=0$ implies $\nu(A)=0$.
\end{proof}

In fact, the converse of \hyperref[thm:1.57]{Theorem 1.57} also holds true. It is the generalization of the fundamental theorem of calculus on measures, known as Radon-Nikodym theorem.

\begin{theorem}[Radon-Nikodym theorem]\label{thm:1.58} Let $\mu$ and $\nu$ be two $\sigma$-finite measures defined on a measurable space $(\Omega,\mathscr{F})$. If $\nu\ll\mu$, then there exists a nonnegative measurable function $f:\Omega\to\mathbb{R}_+$ such that
\begin{align*}
	\nu(A)=\int_A f\,\d \mu,\ \forall A\in\mathscr{F}.
\end{align*}

The function $\frac{d\nu}{d\mu}:=f$, called the \textbf{Radon-Nikodym derivative of $\nu$ with respect to $\mu$}, is uniquely determined up to a $\mu$-null set.
\end{theorem}
\begin{proof}
\textit{Step I:} We first assume that both $\mu$ and $\nu$ are finite. Denote by $\mathcal{F}$ the collection of all measurable functions $f:\Omega\to\overline{\mathbb{R}}$ such that (i) $f\geq 0\ a.e.$, and (ii) $\int_A f\,\d \mu\leq\nu(A)$ for all $A\in\mathscr{F}$. Then $f\equiv 0$ is in $\mathcal{F}$, and $\mathcal{F}$ is closed under finite maximum:
\begin{align*}
	\int_A\max\{f_1,f_2\}\,\d \mu = \int_{A\cap\{f_1\geq f_2\}} f_1\,\d \mu + \int_{A\cap\{f_1<f_2\}}f_2\,\d \mu\leq\nu(A\cap\{f_1\geq f_2\}) + \nu(A\cap\{f_1\geq f_2\}) = \nu(A).
\end{align*}

Define $M=\sup_{f\in\mathcal{F}}\int f\,\d \mu$, we prove that $M=\nu(\Omega)$. Let $f_n$ be a sequence in $\mathcal{F}$ such that $\int f_n\,\d \mu\to M$. We choose an increasing sequence $g_n=\max\{f_1,\cdots,f_n\}\in\mathcal{F}$. By monotone convergence theorem, the function $g=\lim_{n\to\infty} g_n$ lies in $\mathcal{F}$ and satisfies
\begin{align*}
	\int g\,\d \mu = \lim_{n\to\infty}\int g_n d\mu\geq\lim_{n\to\infty}\int f_n d\mu = M\ \overset{g\in\mathcal{F}}{\Rightarrow}\ \int g\,\d \mu = M.
\end{align*}

Argue by contradiction. If $M<\nu(\Omega)$, choose $0<\epsilon<\frac{\nu(\Omega)-M}{\mu(\Omega)}$ and define $\nu^\prime(A)=\int_A g\,\d \mu + \epsilon\mu(A)$. Then
\begin{align*}
	\delta:=\sup_{A\in\mathscr{F}}(\nu-\nu^\prime)(A) \geq \nu(\Omega) - \nu^\prime(\Omega) = \nu(\Omega) - M - \epsilon\mu(\Omega) > 0.
\end{align*}
Using Hahn decomposition theorem on signed measure $\nu-\nu^\prime$, there exists a positive set $P\in\mathscr{F}$ such that $\nu(P)-\nu^\prime(P)=\delta>0$. Since $\nu^\prime(A)\leq\nu(A)+\epsilon\mu(A)$ and $\nu\ll\mu$, we have $\nu^\prime\ll\mu$, and $\mu(P)>0$. By maximal property of $P$, we have $\nu^\prime(A)\leq\nu(A)$ for all $A\subset P$. (Otherwise $\nu(P\backslash A)-\nu^\prime(P\backslash A)>\delta$.) Then
\begin{align*}
	\int_A (g+\epsilon\chi_P)\,\d \mu = \int_A g\,\d \mu + \epsilon\mu(A\cap P) = \int_{A\backslash P}g\,\d \mu + \nu^\prime(A\cap P) \leq \nu(A\backslash P) + \nu(A\cap P) = \nu(A),\ \forall A\in\mathscr{F}.
\end{align*}
Hence $g+\epsilon\chi_P\in\mathcal{F}$. However, $\int (g+\epsilon\chi_P)\,\d \mu=\int g\,\d \mu + \epsilon\mu(P) > M$, a contradiction! As a result, we have $\int g\,\d \mu=M=\nu(\Omega)$. Since $g\in\mathcal{F}$, it holds
\begin{align*}
	0\leq\nu(A) - \int_A g\,\d \mu = \int_{\Omega\backslash A} g\,\d \mu - \nu(\Omega\backslash A)\leq 0,\ \forall A\in\mathscr{F}.
\end{align*}
Note that $g:\Omega\to\overline{\mathbb{R}}$ is integrable. The set $E=\{\omega:g(\omega)=\infty\}$ is $\mu$-null. Choose $f=g\chi_{\Omega\backslash E}$, then $f$ is the desired real-valued function.

\item\textit{Step II:} If $\mu$ and $\nu$ are $\sigma$-finite, take a disjoint sequence $(\Omega_n)_{n=1}^\infty$ such that $\Omega=\bigcup_{n=1}^\infty\Omega_n$ and $\mu(\Omega_n),\nu(\Omega_n)<\infty$ for all $n\in\mathbb{N}$. For each $n$, by the finite case, there exists a measurable function $f_n:\Omega_n\to\mathbb{R}_+$ such that
\begin{align*}
	\nu(A)=\int_A f_n\,\d \mu,\ \forall A\in\mathscr{F}\ \textit{with}\ A\subset\Omega_n.
\end{align*}

Let $f=\sum_{n=1}^n f_n$. Apply monotone convergence theorem to $\left(\sum_{k=1}^n f_k\right)_{n=1}^\infty$:
\begin{align*}
	\nu(A) = \sum_{n=1}^\infty\nu(\Omega_n\cap A) = \sum_{n=1}^\infty \int_{A} f_n\,\d \mu = \int_A f\,\d \mu,\ \forall A\in\mathscr{F}.
\end{align*}

\item\textit{Step III:} Finally we show that $f$ is uniquely determined up to a $\mu$-null set. Let $h:\Omega\to\mathbb{R}_+$ be another function satisfying the desired property. Then
\begin{align*}
	\int_A (f-h)\,\d \mu = 0,\ \forall A\in\mathscr{F}.
\end{align*}
Take $A=\{\omega:f(\omega)>h(\omega)\}$, we have $\int_X(f-h)^+\,\d \mu=0$, and $(f-h)^+=0\ a.e.$. Similarly $(f-h)^-=0\ a.e.$. Hence $f=h\ a.e.$, as desired.
\end{proof}

\begin{corollary}\label{cor:1.59} Let $\mu$ (resp. $\nu$) be a $\sigma$-finite measure (resp. finite signed measure) on a measurable space $(\Omega,\mathscr{F})$. If $\nu\ll\mu$, then there exists $f\in L^1(\Omega,\mathscr{F},\mu)$ such that
\begin{align*}
	\nu(A)=\int_A f\,\d \mu,\ \forall A\in\mathscr{F}.
\end{align*}

The \textbf{Radon-Nikodym derivative} $\frac{d\nu}{d\mu}:=f$ is uniquely determined up to a $\mu$-null set.
\end{corollary}
\begin{proof}
Use the Jordan decomposition of signed measure $\nu=\nu^+-\nu^-$. Then there exist measurable functions $g,h:\Omega\to\mathbb{R}_+$ such that
\begin{align*}
	\nu^+(A)=\int_A g\,\d \mu,\ \nu^-(A)=\int_A h\,\d \mu,\ \forall A\in\mathscr{F}.
\end{align*}
Since $\nu$ is finite, $g$ and $h$ are integrable. Then $f=g-h$ is the desired integrable function.
\end{proof}

\begin{theorem}[Lebesgue decomposition theorem]\label{thm:1.60} Let $\mu$ and $\nu$ be two $\sigma$-finite measures on a measurable space $(\Omega,\mathscr{F})$. Then there exist unique measures $\nu_0\ll\mu$ and $\nu_1\perp\mu$ such that $\nu=\nu_0+\nu_1$.
\end{theorem}
\begin{proof}
Define the measure $\lambda=\mu+\nu$, then $\mu,\nu\ll\lambda$, and $\lambda$ is $\sigma$-finite. By Radon-Nikodym theorem, there exists nonnegative measurable functions $f,g:\Omega\to\mathbb{R}_+$ such that
\begin{align*}
	\mu(A)=\int_A f\,\d \lambda,\ \nu(A)=\int_A g\,\d \lambda,\ \forall A\in\mathscr{F}.
\end{align*}

Let $E=\{\omega:f(\omega)=0\}$, and define $\nu_1(A)=\nu(A\cap E)$, $\nu_0=\nu(A\cap E^c)$ for all $A\in\mathscr{F}$. Clearly, $\nu_1\perp\mu$, since $\nu_1(X\backslash E)=\nu_1(\emptyset)=0=\mu(E)$. 

It remains to show $\nu_0\ll\mu$. If $\mu(A)=0$, we fix any $n\in\bbN$ and let $B_n=\{\omega\in A:f(\omega)>n^{-1}\}$. Then
\begin{align*}
	0\leq\lambda(B_n)\leq n\int_{B_n} f\,d\lambda\leq n\int_A f\,d\lambda=n\mu(A)=0.
\end{align*}
Then the set $B=A\cap E^c=\bigcup_{n=1}^\infty B_n$ has measure zero. Hence $0\leq\nu_0(A)=\nu(B)\leq\lambda(B)=0$.

Finally we prove uniqueness. If $\nu=\nu_0^\prime+\nu_1^\prime$ with $\nu_0^\prime\ll\mu$ and $\nu_1^\prime\perp\mu$, there exists $E^\prime\in\mathscr{F}$ such that $\nu_1^\prime(X\backslash E^\prime)=\mu(E^\prime)=0$. Then for all measurable $A\subset X\backslash(E\cup E^\prime)$, we have $\nu_0(A)=\mu(A)=\nu_0^\prime(A)$. Moreover, for all measurable $A\subset E\cup E^\prime$, since $\nu_0,\nu_0^\prime\ll\mu$, we have $\nu_0(A)=\nu_0^\prime(A)=\mu(A)=0$. Therefore $\nu_0^\prime=\nu_0$.
\end{proof}

We can apply \hyperref[thm:1.60]{Theorem 1.60} to the Jordan decomposition of a signed measure $\nu=\nu^+ - \nu^-$.

\begin{corollary}[Lebesgue]\label{cor:1.60} Let $\mu$ (resp. $\nu$) be a $\sigma$-finite measure (resp. $\sigma$-finite signed measure) on $(\Omega,\mathscr{F})$. Then there exist unique signed measures $\nu_0\ll\mu$ and $\nu_1\perp\mu$ such that $\nu=\nu_0+\nu_1$.
\end{corollary}
\paragraph{Remark.} When $\nu$ is not absolutely continuous with respect to $\mu$, we can apply Radon-Nikodym theorem to the pair $\nu_0\ll\mu$.

\newpage
\subsection{Convergence of Measurable Functions and Measures}
\label{sec:1.6}
\begin{definition}[Cauchy sequence in measure]\label{def:1.62} Let $(f_n)_{n=1}^\infty$ be a sequence of measurable functions on $(\Omega,\mathscr{F},\mu)$. If there exists a function $f$ such that for all $\epsilon>0$ and all $\eta>0$, there exists $N$ such that $\mu(\vert f_n-f_m\vert\geq\eta)<\epsilon$ for all $n,m\geq N$, then $f_n$ is said to be \textit{a Cauchy sequence in measure}.
\end{definition}
\begin{lemma}\label{lemma:1.63} If $(f_n)_{n=1}^\infty$ is a Cauchy sequence in measure, there exists a subsequence $(f_{n_k})_{k=1}^\infty$ that converges $a.e.$ to a measurable function $f$.
\end{lemma}
\begin{proof}
Since $f_n$ is a Cauchy sequence, we can choose a subsequence $f_{n_k}$ such that
\begin{align*}
	\mu\left(E_k\right) < \frac{1}{2^k},\ \textit{where}\ E_k=\left\{\vert f_{n_{k+1}}-f_{n_k}\vert\geq \frac{1}{2^k}\right\}.
\end{align*}

Let $F_N=\bigcup_{k=N}^\infty E_k$, and $E=\bigcap_{N=1}^\infty F_N$. Then $\mu(F_N)<2^{-N+1}$, and $\mu(E)=\lim_{N\to\infty}\mu(F_N)=0$. For each $\omega\in\Omega\backslash E$, we have $\omega\notin F_N$ for some $N\in\mathbb{N}$, which implies $\vert f_{n_{k+1}}(\omega)-f_{n_k}(\omega)\vert<2^{-k}$ for all $k\geq N$. Hence $f_{n_k}(\omega)$ is a Cauchy sequence, which converges to some $f(\omega)\in\mathbb{R}$. For $\omega\in E$, define $f(\omega)=0$. As a result, $f_n\to f\ a.e.$, which is measurable.
\end{proof}

\begin{definition}[Convergence in measure]\label{def:1.64} Let $(f_n)_{n=1}^\infty$ be a sequence of measurable functions on $(\Omega,\mathscr{F},\mu)$. If there exists a function $f$ such that for all $\eta>0$,
\begin{align*}
	\lim_{n\to\infty}\mu\left(\vert f_n-f\vert\geq\eta\right) = 0,
\end{align*}
then $f$ is said to \textit{converges to $f$ in measure}, and we write $f_n\overset{\mu}{\to}f$.
\end{definition}

\begin{theorem}\label{thm:1.65} A function sequence $(f_n)_{n=1}^\infty$ converges in measure if and only if it is a Cauchy sequence.
\end{theorem}
\begin{proof}
Given $\epsilon>0$ and $\eta>0$. If $f_n\overset{\mu}{\to}f$,  there exists $N$ such that $\mu\left(\vert f_n-f\vert\geq\eta/2\right)<\epsilon/2$ for all $n\geq N$. Then for all $m,n\geq N$, we have
\begin{align*}
	\mu(\vert f_n-f_m\vert\geq\eta) \leq \mu\left(\left\{\omega:\vert f_n(\omega)-f(\omega)\vert\geq\frac{\eta}{2}\right\}\cup\left\{\omega:\vert f_m(\omega)-f(\omega)\vert\geq\frac{\eta}{2}\right\}\right) < \epsilon.
\end{align*}

Conversely, if $f_n$ is a Cauchy sequence in measure, by \hyperref[lemma:1.63]{Lemma 1.63}, one of its subsequence $f_{n_k}$ converges a.e. to a measurable function $f$. Furthermore, if we choose $F_k$ in \hyperref[lemma:1.63]{Lemma 1.63}, for all $k\geq N$, we have
\begin{align*}
	\vert f_{n_k}(\omega)-f(\omega)\vert \leq \sum_{l=k}^\infty\vert f_l(\omega)-f_{l+1}(\omega)\vert \leq \frac{1}{2^{k-1}},\ \forall\omega\in\Omega\backslash F_k,
\end{align*}
which implies
\begin{align*}
\mu\left(\vert f_{n_k}-f\vert > \frac{1}{2^{k-1}}\right) \leq \mu(F_k) < \frac{1}{2^{k-1}}.
\end{align*}
Hence $f_{n_k}\overset{\mu}{\to}f$. Now given $\epsilon,\eta>0$, we choose $k>0$ such that $\mu(\vert f_{n_k}-f\vert>\eta/2)<\epsilon/2$, and choose $N>0$ such that $\mu(\vert f_n-f_m\vert>\eta/2)<\epsilon/2$ for all $n\geq N$. Then
\begin{align*}
	\mu(\vert f_n-f\vert\geq \eta) \leq \mu\left(\left\{\vert f_n-f_{n_k}\vert\geq\frac{\eta}{2}\right\}\cup\left\{\vert f_{n_k}-f\vert\geq\frac{\eta}{2}\right\}\right) < \epsilon
\end{align*}
for all $n\geq\max\{n_k,N\}$. Therefore $f_n\overset{\mu}{\to} f$.
\end{proof}

The following theorem shows that a pointwise convergent function sequence also converges in measure.
\begin{theorem}[Egoroff]\label{thm:1.66} Let $(\Omega,\mathscr{F},\mu)$ be a finite measure space. If a sequence of functions $(f_n)$ converges to a function $f$ $\mu$-a.e., then $f_n\overset{\mu}{\to}f$.
\begin{proof}
Let $\epsilon>0$ and $\eta>0$. By \hyperref[thm:1.37]{Theorem 1.37}, choose $E\in\mathscr{F}$ such that $\mu(E)<\epsilon$ and $f_n$ converges $f$ uniformly on $\Omega\backslash E$. Then there exists $N$ such that $\vert f_n-f\vert < \eta$ for all $n\geq N$ and all $\omega\in\Omega\backslash E$. Hence $\mu(\vert f_n-f\vert\geq\eta) < \mu(E)<\epsilon$ for all $n\geq N$. Since $\epsilon>0$ and $\eta>0$ are arbitrary, $f_n\overset{\mu}{\to} f$.
\end{proof}
\end{theorem}
Next, we are going to introduce $L^p$ spaces. 

\begin{definition}[$L^p$-spaces]\label{def:1.67} Let $(\Omega,\mathscr{F},\mu)$ be a measurable space. For $1\leq p<\infty$, define $\mathcal{L}^p(\Omega,\mathscr{F},\mu)$ to be the set of all measurable functions $f$ such that $\vert f\vert^p$ is integrable, i.e. $\int_X\vert f\vert^p d\mu < \infty$. We define
\begin{align*}
	\Vert f\Vert_p = \left(\int\vert f\vert^p d\mu\right)^{1/p},\ f\in\mathcal{L}^p(\Omega,\mathscr{F},\mu).
\end{align*}
By Minkowski's inequality, $\Vert\cdot\Vert_p$ is a seminorm on $\mathcal{L}^p(\Omega,\mathscr{F},\mu)$.  Let $f\sim g\overset{def}{\Leftrightarrow} f=g\ a.e.$ be a equivalence relation on $\mathcal{L}^p(\Omega,\mathscr{F},\mu)$. We define $L^p$-space as the quotient space
\begin{align*}
	L^p(\Omega,\mathscr{F},\mu)=\mathcal{L}^p(\Omega,\mathscr{F},\mu)/\sim,
\end{align*} 
and maintain the norm $\Vert[f]\Vert_p=\Vert f\Vert_p$. This is a well-defined norm, since $\Vert f\Vert_p = \Vert g\Vert_p$ if $f\sim g$. For simplicity, we drop the brackets and use $f$ to denote its corresponding equivalence class $[f]$ in $L^p(\Omega,\mathscr{F},\mu)$. Then the space $(L^p(\Omega,\mathscr{F},\mu),\Vert\cdot\Vert_p)$ is a normed space.
\end{definition}

\begin{theorem}[Chebyshev inequality]\label{thm:1.68} Let $1\leq p<\infty$, and $f\in L^p(\Omega,\mathscr{F},\mu)$. Then
\begin{align*}
	\mu(\vert f\vert\geq\eta)\leq \frac{1}{\eta^p}\Vert f\Vert_p^p,\ \forall\eta>0.
\end{align*}
\end{theorem}
\begin{proof}
Let $E=\left\{\omega:\vert f(\omega)\vert\geq\eta\right\}$. Then \vspace{0.35cm}

$\displaystyle\hspace{4.2cm}\Vert f\Vert_p^p = \int \vert f\vert^p\,\d \mu\geq\int_E\vert f\vert^p\,\d \mu \geq \eta^p\mu(E),\ \forall \eta>0.\vspace{0.2cm}$
\end{proof}

\begin{remark}
As a result of \hyperref[thm:1.68]{Theorem 1.68}, convergence in $L^p$-norm implies convergence in measure.
\end{remark}

\begin{theorem}[Riesz-Fisher]\label{thm:1.69} $L^p(\Omega,\mathscr{F},\mu)$ is a Banach space. That is, every Cauchy sequence $(f_n)$ in $L^p(\Omega,\mathscr{F},\mu)$ converges in $L^p$ norm.
\end{theorem}
\begin{proof}
By Chebyshev's inequality, $f_n$ is also a Cauchy sequence in measure, and there exists a subsequence $f_{n_k}$ that converges $a.e.$ to some measurable $f$. Given $\epsilon>0$, we choose $N$ such that $\Vert f_n - f_m\Vert_p < \epsilon$ for all $n,m\geq N$. By Fatou's lemma,
\begin{align*}
	\int \vert f-f_m\vert^p\,\d \mu = \int\lim_{k\to\infty}\vert f_{n_k}-f_m\vert^p\,\d \mu \leq\liminf_{k\to\infty}\int\vert f_{n_k}-f_m\vert^p\,\d \mu\leq\epsilon^p,\ \forall m\geq N.
\end{align*}
Hence $f-f_m\in L^p(\Omega,\mathscr{F},\mu)$, $f=f_m+(f-f_m)\in L^p(\Omega,\mathscr{F},\mu)$. Since $\epsilon>0$ is arbitrary, $\Vert f-f_m\Vert_p\to 0$.
\end{proof}

Now we introduce uniform integrability of function classes.

\begin{definition}\label{def:1.70} (Uniform Integrability). Let $L^1_+(\Omega,\mathscr{F},\mu) = \{g\in L^1(\Omega,\mathscr{F},\mu):g\geq 0\}$. A collection of integrable functions $\mathcal{F}\subset L^1(\Omega,\mathscr{F},\mu)$ is said to be \textit{uniformly integrable} \begin{align*}
	\inf_{g\in L_+^1(\Omega,\mathscr{F},\mu)}\sup_{f\in\mathcal{F}}\int_{\{\vert f\vert>g\}}\vert f\vert\,\d \mu=0.\tag{1.12}\label{eq:1.12}
\end{align*}
\end{definition}
Clearly, a collection of finitely many integrable functions is uniformly integrable. So we are interested in the case of infinitely many integrable functions. We have the following basic fact.
\begin{lemma}\label{lemma:1.71} If $\mathcal{F}\subset L^1(\Omega,\mathscr{F},\mu)$ is uniformly integrable, then
\begin{align*}
	\lim_{n\to\infty}\sup_{f\in\mathcal{F}}\int_{\{\vert f\vert > n\}}\vert f\vert\,\d \mu = 0.\label{eq:1.13}\tag{1.13}
\end{align*}
\end{lemma}
\begin{proof}
If \hyperref[eq:2.10]{(2.10)} holds, we fix $\epsilon>0$ and choose $g\in L^1_+(\Omega,\mathscr{F},\mu)$ and $n>0$ such that
\begin{align*}
	\sup_{f\in\mathcal{F}}\int_{\{\vert f\vert>g\}}\vert f\vert\,\d \mu < \frac{\epsilon}{2},\quad \int_{\{g>n\}}g\,\d \mu <\frac{\epsilon}{2}.
\end{align*}
Then for all $f\in\mathcal{F}$, we have
\begin{align*}
	\int_{\{\vert f\vert>n\}}\vert f\vert\,\d \mu \leq \int_{\{\vert f\vert>g\}}\vert f\vert\,\d \mu + \int_{\{\vert g\vert\geq f>n\}}\vert f\vert\,\d \mu \leq \int_{\{\vert f\vert>g\}}\vert f\vert\,\d \mu + \int_{\{\vert g\vert>n\}}g\,\d \mu < \epsilon.
\end{align*}
Hence \hyperref[eq:1;12]{(1.11)} implies \hyperref[eq:1.13]{(1.13)}.
\end{proof}
\begin{remark} In addition, if $(\Omega,\mathscr{F},\mu)$ is a finite measure space, \hyperref[eq:1.13]{(1.13)} is an equivalent definition of uniform integrability, since constant functions are always integrable in finite measure spaces, \hyperref[eq:1.13]{(1.13)} $\Rightarrow$ \hyperref[eq:1.12]{(1.12)}.
\end{remark}

The following theorem gives a depiction of uniformly integrable function classes in finite measure spaces, which has a similar form to the Arzelà-Ascoli theorem in functional analysis.

\begin{theorem}\label{thm:1.72} Let $(\Omega,\mathscr{F},\mu)$ be a finite measure space. A collection of functions $\mathcal{F}\subset L^1(\Omega,\mathscr{F},\mu)$ is uniformly integrable if and only if it satisfies the following:
\begin{itemize}
	\item[(i)] (Uniform $L^1$-boundedness). $\sup_{f\in\mathcal{F}}\Vert f\Vert_1 <\infty.$
	\item[(ii)] (Uniform absolute continuity). For all $\epsilon>0$, there exists $\delta>0$ such that for all $A\in\mathscr{F}$ with $\mu(A)<\delta$,
	\begin{align*}
		\int_A\vert f\vert\,\d \mu < \epsilon,\ \forall f\in\mathcal{F}.
	\end{align*}
\end{itemize}
\end{theorem}
\begin{proof}
Let $\mathcal{F}$ be uniform integrable. For all $A\in\mathscr{F}$ and $n>0$, we have
\begin{align*}
	\int_A\vert f\vert\,\d \mu \leq \int_{A\cap\{f>n\}}\vert f\vert\,\d \mu + \int_{A\cap\{\vert f\vert\leq n\}}\vert f\vert\,\d \mu \leq \int_{\{\vert f\vert>n\}}\vert f\vert\,\d \mu + n\mu(A),\ \forall f\in\mathcal{F}.
\end{align*}
\begin{itemize}
	\item[(i)] Choose $A=\Omega$. Since $\mu(\Omega)<\infty$, and $\mathcal{F}$ be uniform integrable, both terms are uniformly bounded.
	\item[(ii)] Given $\epsilon>0$, we choose $N$ such that $\sup_{f\in\mathcal{F}}\int_{\{\vert f\vert>N\}}\vert f\vert\,\d \mu<\epsilon/2$ and $\delta = \frac{\epsilon}{2N}$.\vspace{0.1cm}
\end{itemize}

Conversely, if $\mathcal{F}$ satisfies (i) and (ii), by Chebyshev inequality,
\begin{align*}
	\sup_{f\in\mathcal{F}}\mu(\vert f\vert\geq n) \leq \frac{1}{Nn}\sup_{f\in\mathcal{F}}\Vert f\Vert_1\to 0\ \textit{as}\ n\to\infty.
\end{align*}

Given $\epsilon>0$, we choose the $\delta$ mentioned in (ii), and choose $N$ such that $\mu(\vert f\vert\geq n)<\delta$ for all $n\geq N$ and all $f\in\mathcal{F}$. By uniform absolute continuity of $\mathcal{F}$, we have $\sup_{f\in\mathcal{F}}\int_{\{\vert f\vert > n\}}\vert f\vert\,\d \mu<\epsilon$ for all $n\geq N$. Since $\epsilon$ is arbitrary, $\mathcal{F}$ is uniformly integrable.
\end{proof}

With uniform integrability, we can deduce $L^1$-convergence using convergence in measure.

\begin{theorem}\label{thm:1.73} Let $(\Omega,\mathscr{F},\mu)$ be a finite measure space, and $f_n\in L^1(\Omega,\mathscr{F},\mu)$. Then $\Vert f_n-f\Vert_1\to 0$ if and only if $f_n\overset{\mu}{\to}f$ and $\{f_n\}_{n\in\mathbb{N}}$ is uniformly integrable.
\end{theorem}
\begin{proof}
Assume $\Vert f_n-f\Vert_1\to 0$, and fix $\epsilon>0$. By Chebyshev inequality, $f_n\overset{\mu}{\to} f$. Furthermore, by choosing $N$ such that $\Vert f_n-f\Vert_1<\frac{\epsilon}{2\mu(\Omega)}$ for all $n\geq N$, we have
\begin{align*}
	\sup_{n\in\mathbb{N}}\Vert f_n\Vert\leq\max\left\{\Vert f_1\Vert_1,\cdots,\Vert f_{N-1}\Vert_1,\Vert f\Vert+\frac{\epsilon}{2\mu(\Omega)}\right\}<\infty.
\end{align*}
Hence $\{f_n\}_{n\in\mathbb{N}}$ is uniformly $L^1$-bounded. Furthermore, for all $A\in\mathscr{F}$, we have 
\begin{align*}
	\sup_{n\in\mathbb{N}}\int_A\vert f_n\vert\,\d \mu\leq\max\left\{\int_A\vert f_1\vert\,\d \mu,\cdots,\int_A\vert f_{N-1}\vert\,\d \mu,\int_A\vert f_N\vert\,\d \mu+\frac{\epsilon}{2}\right\}.
\end{align*}

By \hyperref[thm:1.57]{Theorem 1.57}, for each $j\in\{1,\cdots,N\}$, we choose $\delta_j>0$ such that for all $A\in\mathscr{F}$ with $\mu(A)<\delta_j$, $\int_A\vert f_j\vert\,\d \mu<\frac{\epsilon}{2}$. Choose $\delta:=\min_{j\in\{1,\cdots,N\}}\delta_j$, we have $\sup_{n\in\mathbb{N}}\int_A\vert f_n\vert\,\d \mu<\epsilon$ for all $\mu(A)<\delta$. Hence $\{f_n\}_{n\in\mathbb{N}}$ is uniformly absolutely continuous. By \hyperref[thm:1.72]{Theorem 1.72}, it is uniformly integrable.\vspace{0.1cm}

Conversely, assume $f_n\overset{\mu}{\to} f$ and $\{f_n\}_{n\in\mathbb{N}}$ is uniformly integrable. Given $\epsilon>0$, we have
\begin{align*}
\Vert f_n - f\Vert_1 &\leq \epsilon\mu(\Omega) + \int_{\{\vert f_n-f\vert>\epsilon\}}\vert f_n-f\vert\,\d \mu\\
&\leq \epsilon\mu(\Omega) + \int_{\{\vert f_n-f\vert>\epsilon\}}\vert f_n\vert\,\d \mu + \int_{\{\vert f_n-f\vert>\epsilon\}}\vert f\vert\,\d \mu
\end{align*}

Since $f$ is integrable, $\{f_n\}_{n=1}^\infty$ is uniformly integrable, we choose $\delta>0$ such that $\sup_{n\in\mathbb{N}}\int_A\vert f_n\vert\,\d \mu<\epsilon$ and $\int_A\vert f\vert\,\d \mu<\epsilon$ for all $\mu(A)<\delta$. Since $f_n\overset{\mu}{\to}f$, we also choose $N$ such that $\mu(\vert f_n - f\vert>\epsilon)<\delta$ for all $n\geq N$. Then $\Vert f_n-f\Vert_1\leq\epsilon(\mu(\Omega)+2)$ for all $n\geq N$. Note that $\epsilon>0$ is arbitrary, we have $\Vert f_n-f\Vert_1\to 0$.
\end{proof}

\begin{remark} By \hyperref[thm:1.73]{Theorem 1.73}, if $(\Omega,\mathscr{F},\mu)$ is a finite measure space, $\{f_n\}_{n\in\mathbb{N}}\subset L^1(\Omega,\mathscr{F},\mu)$ is a uniformly integrable sequence, and $f_n\overset{\mu}{\to}f$, then
\begin{align*}
	\lim_{n\to\infty}\int f_n\,\d \mu = \int f\,\d \mu.
\end{align*}
This is a weaker condition than Lebesgue dominated convergence theorem.
\end{remark}

\begin{definition}[Weak convergence]\label{def:1.74} Let $\Omega$ be a metric space with its Borel $\sigma$-algebra $\mathscr{B}$.  Let $C_b(\Omega)$ be the set of all bounded continuous functions on $\Omega$. Let $\mu_n$ be a sequence of probability measures on $(\Omega,\mathscr{B})$. If there exists a probability measure $\mu$ on $(\Omega,\mathscr{B})$ such that
\begin{align*}
	\int f\,\d \mu_n \to \int f\,\d \mu,\quad \forall f\in C_b(\Omega),
\end{align*}
then $\mu_n$ is said to \textit{converge weakly to $\mu$}, and we write $\mu_n\overset{w}{\to}\mu$.
\end{definition}
\paragraph{Review: Semi-continuity.} Recall that a function $f:\Omega\to\overline{\mathbb{R}}$ is \textit{upper semi-continuous at $\omega_0$} if for any real $y>f(\omega_0)$ there exists a neighborhood $U$ of $\omega_0$ such that $f(x)<y$ for all $x\in U$. In a nutshell, $f$ does not take a much larger value than $f(\omega_0)$ at a point closed to $\omega_0$. 

Similarly, a function $f$ is said to be \textit{lower semi-continuous at $\omega_0$} if if for any real $y<f(\omega_0)$ there exists a neighborhood $U$ of $\omega_0$ such that $f(x)>y$ for all $x\in U$. In addition, If $f$ is upper (resp.\,lower) semi-continuous at each $\omega\in\Omega$, we say $f$ is \textit{upper (resp.\,lower) semi-continuous}.

\begin{lemma}\label{lemma:1.75} Let $\Omega$ be a metric space. For every nonnegative lower semi-continuous function $f:\Omega\to\overline{\mathbb{R}}_+$, there exists a sequence of nonnegative bounded Lipschitz continuous functions $f_n$ such that $f_n\nearrow f$ pointwise.
\end{lemma}
\begin{proof}
For every $n\in\mathbb{N}$, define $g_n(x)=\inf_{y\in\Omega}\left\{f(y)+nd(x,y)\right\}$. Clearly, we have $0\leq f_n\leq f_{n+1}\leq f$. Furthermore, for all $x,y\in\Omega$,
\begin{align*}
	g_n(x) - g_n(y) = \inf_{z\in\Omega}\left\{f(z)+nd(x,z)\right\} - f_n(y) \leq \inf_{z\in\Omega}\left\{f(z)+nd(x,y)+nd(y,z)\right\} - f_n(y) = nd(x,y).
\end{align*}
Symmetrically $g_n(y) - g_n(x)\leq nd(x,y)$. Hence $g_n$ is $n$-Lipschitz. It remains to show $g_n\nearrow f$ pointwise. \vspace{0.1cm}

Fix $x\in\Omega$, and choose $0<\epsilon<f(x)$. Since $f$ is lower semi-continuous, there exists $\delta>0$ such that $f(y)>f(x)-\epsilon$ for all $y\in O(x,\delta)$. Choose $N>f(x)/\delta$. If $n\geq N$, we have $f(y)+nd(x,y)\geq N\delta > f(x)$ for all $y\notin O(x,\delta)$, and $f(y)+nd(x,y) \geq f(y) > f(x)-\epsilon$ for all $y\in O(x,\delta)$. Hence $f(x)-\epsilon<g_n(x)\leq f(x)$ for all $n\geq N$. Since $0<\epsilon<f(x)$ is arbitrary, $g_n(x)\nearrow f(x)$. Then $f_n=\min\{g_n,n\}$ is the desired sequence.
\end{proof}

The following lemma states that the converging point of a weakly convergent sequence is unique.

\begin{lemma}\label{lemma:1.76} Let $\Omega$ be a metric space equipped with its Borel $\sigma$-algebra $\mathscr{B}$. Let $\mu_n$ be a sequence of probability measures on $(\Omega,\mathscr{B})$. If $\mu_n\overset{w}{\to}\mu$ and $\mu_n\overset{w}{\to}\mu^\prime$, then $\mu=\mu^\prime$.
\end{lemma}
\begin{proof}
	By definition of weak convergence, $\int f_n\,\d \mu=\int f_n\,\d \mu^\prime$ for all $f\in C_b(\Omega)$. 
	
	Let $G$ be a closed set, then $\chi_G$ is lower semi-continuous. By \hyperref[lemma:1.75]{Lemma 1.75}, we choose a sequence of bounded Lipschitz continuous functions $f_n\nearrow\chi_G$. By monotone convergence theorem,
	\begin{align*}
		\mu(G) = \lim_{n\to\infty}\int f_n\,\d \mu = \lim_{n\to\infty}\int f_n\,\d \mu^\prime = \mu^\prime(G).
	\end{align*}
	
	Let $\mathscr{T}$ be the topology on $\Omega$, i.e. $\mathscr{T}$ is the collection of all open subsets of $\Omega$. Then $\mu|_\mathscr{T}=\mu^\prime|_\mathscr{T}$. Since $\mathscr{T}$ is a $\pi$-system, and $\sigma(\mathscr{T})=\mathscr{B}$, by \hyperref[lemma:1.22]{Lemma 1.22}, $\mu=\mu^\prime$.
\end{proof}

Portmanteau lemma gives multiple equivalent definitions of weak convergence.

\begin{theorem}[Portmanteau lemma]\label{thm:1.77} Let $\Omega$ be a metric space with its Borel $\sigma$-algebra $\mathscr{B}$. Let $\mu_n$ be a sequence of probability measures on $(\Omega,\mathscr{B})$. The following are equivalent:
\begin{itemize}
	\item[(i)] $\int f\,\d \mu_n\to\int f\,\d \mu$ for all bounded continuous functions $f$. In other words, $\mu_n\overset{w}{\to}\mu$;
	\item[(ii)] $\int f\,\d \mu_n\to\int f\,\d \mu$ for all bounded Lipschitz continuous functions $f$;
	\item[(iii)] $\liminf_{n\to\infty}\int f\,\d \mu_n\geq\int f\,\d \mu$ for all lower semi-continuous function $f$ bounded from below;
	\item[(iv)] $\limsup_{n\to\infty}\int f\,\d \mu_n\leq\int f\,\d \mu$ for all upper semi-continuous function $f$ bounded from above;
	\item[(v)] $\liminf_{n\to\infty}\mu_n(G)\geq\mu(G)$ for every open sets $G$;
	\item[(vi)] $\limsup_{n\to\infty}\mu_n(F)\leq\mu(F)$ for every closed sets $F$;
	\item[(vii)] $\lim_{n\to\infty}\mu_n(B)\to\mu(B)$ for all Borel sets $B$ with $\mu(\partial B)=0$, where $\partial B=\overline{B}\,\backslash\mathring{B}$ is the boundary of $B$. 
\end{itemize}
\end{theorem}
\begin{remark}
A Borel set $B$ is said to be a \textit{$\mu$-continuity set} if $\mu(\partial B)=0$. Conversely, if a Borel set $B$ is not a $\mu$-continuity set, it is said to be a \textit{$\mu$-discontinuity set}.
\end{remark}
\begin{proof}
(i) $\Rightarrow$ (ii) is clear. (iii) $\Leftrightarrow$ (iv) follows by taking negation. (v) $\Leftrightarrow$ (vi) follows by taking complements.

\item (ii) $\Rightarrow$ (iii): Without loss of generality, assume $f\geq 0$ is lower semi-continuous. By \hyperref[lemma:1.75]{Lemma 1.75}, choose a sequence $f_k$ of nonnegative bounded Lipschitz continuous functions such that $f_k\nearrow f$ pointwise.

Since $f_k$ is Lipschitz and $f_k\leq f$, by (ii) and monotone convergence theorem, we have
\begin{align*}
	\liminf_{n\to\infty}\int f\,\d \mu_n \geq \liminf_{n\to\infty}\int f_k\,\d \mu_n = \int f_k\,\d \mu,\ \forall k\in\mathbb{N}\ \ \Rightarrow\ \ \liminf_{n\to\infty}\int f\,\d \mu_n\geq \lim_{k\to\infty}\int f_k\,\d \mu = \int f\,\d \mu.
\end{align*}

\item (iii) + (iv) $\Rightarrow$ (i): Let $f$ be a bounded continuous function. By (iii) and (iv). Then
\begin{align*}
	\liminf_{n\to\infty}\int f\,\d \mu_n \geq\int f\,\d \mu \geq \limsup_{n\to\infty}\int f\,\d \mu_n\ \ \Rightarrow\ \ \lim_{n\to\infty}\int f\,\d \mu_n = \int f\,\d \mu.
\end{align*}

\item (iii) $\Rightarrow$ (v): If $G$ is an open set, $\chi_G$ is bounded and lower semi-continuous. Take $f=\chi_G$ in (iii). \vspace{0.1cm}

\item (v) $\Rightarrow$ (i): Pick $f\in C_n(\Omega)$, and without loss of generality assume $0<f<1$. Then the function $\chi_{\{(\omega,t):f(\omega)>t\}} = \chi_{(0,\infty)}(f(\omega)-t)$ is measurable. By Fubini's theorem,
\begin{align*}
	\int_0^1\mu(f>t)\,\d t = \int_0^1\left(\int_\Omega\chi_{\{(\omega,t):f(\omega)>t\}}\,\d \mu\right)\,\d t = \int_\Omega\left(\int_0^1\chi_{\{(\omega,t):t<f(\omega)\}}\,\d t\right)\,\d \mu = \int f\,\d \mu.
\end{align*}
Since $f$ is continuous, $\{\omega:f(\omega)>t\}=f^{-1}((t,\infty))$ is open. By (v), $\liminf_{n\to\infty}\mu_n(f>t)\geq\mu(f>t)$. Using Fatou's lemma, we have
\begin{align*}
	\liminf_{n\to\infty}\int f\,\d \mu_n = \liminf_{n\to\infty}\int_0^1\mu_n(f>t)\,\d t\int_0^1\liminf_{n\to\infty}\mu_n(f>t)\,\d t\geq\int\mu(f>t)\,\d t = \int f\,\d \mu.
\end{align*}
By repeating the same procedure on $-f$, we have $\limsup_{n\to\infty}\int f\,\d \mu_n\leq\int f\,\d \mu$. Then (i) follows. \vspace{0.1cm}

\item (v) + (vi) $\Rightarrow$ (vii): Let $B\in\mathscr{B}$. Then
\begin{align*}
	\mu(\mathring{B})\overset{\text{(v)}}{\leq}\liminf_{n\to\infty}\mu_n(\mathring{B})\leq\liminf_{n\to\infty}\mu_n(B)\leq\limsup_{n\to\infty}\mu_n(B)\leq\limsup_{n\to\infty}\mu_n(\overline{B})\overset{\text{(vi)}}{\leq}\mu(\overline{B}).
\end{align*}
If $\mu(\delta B)=0$, all above inequalities become equalities, and they equal $\mu(B)$. \vspace{0.1cm}

\item (vii) $\Rightarrow$ (vi): Fix a closed set $F\subset\Omega$, and define the collection of sets $\{B_F(r):r\geq 0\}$, where
\begin{align*}
	B_F(r):=\{\omega\in\Omega:d(\omega,F)\leq r\}.
\end{align*}
\textit{Claim.} There exists a countable subset $C$ of $[0,\infty)$ such that $B_F(r)$ is a $\mu$-continuity set for all $r\in[0,\infty)\backslash C$. \vspace{0.1cm}\newline
\textit{Proof of claim.} Given $r\geq 0$, let $D_F(r)=\left\{\omega\in\Omega:d(\omega,F)=r\right\}$. Then $\{D_F(r):r\geq 0\}$ is a partition of $\Omega$. 

By continuity of $d(\cdot, F)$, $B_F(r)$ is a closed set. Furthermore, if $\omega\in \partial B_F(r)=\overline{\Omega\backslash B_F(r)}\cap B_F(r)$, choose a sequence $\omega_n\in\Omega\backslash B_F(r)$ such that $\omega_n\to\omega$. Again by continuity of $d(\cdot, F)$, $d(\omega,F)=\lim_{n\to\infty} d(\omega_n,F)\geq r$. Hence $d(\omega,F)=r$, which implies $\delta B_F(r)\subset D_F(r)$.

Let $C = \{r\geq 0:\mu(D_F(r))>0\}$ and $C_n = \{r\geq 0:\mu(D_F(r))>1/n\}$. Then $\vert C_n\vert <n$, and $C=\bigcup_{n=1}^\infty C_n$ is at most countable. Furthermore, for each $r\in[0,\infty)\backslash C$, it holds $$0\leq\mu(\partial B_F(r))\leq\mu(D_F(r)) = 0.$$
Therefore $B_F(r)$ is a $\mu$-continuity set, and $C$ is the desired countable set.\qed

\item Now we choose a sequence $r_k\searrow 0$ in $[0,\infty)\backslash C$. Then $B_F(r_k)\searrow F$. By (vii),
\begin{align*}
	\mu(B_F(r_k)) = \lim_{n\to\infty}\mu_n(B_F(r_k))\geq\limsup_{n\to\infty}\mu_n(F),\ \forall k\in\mathbb{N}\ \Rightarrow\ \mu(F) = \lim_{k\to\infty}\mu(B_F(r_k))\geq\limsup_{n\to\infty}\mu_n(F).
\end{align*}
Hence (vi) follows.
\end{proof}

\newpage
\section{Random Variables}
In this section, if not specifically indicated, our discussion is based on a probability space $(\Omega,\mathscr{F},\mathbb{P})$.
\subsection{Random Variables and Independence}
\begin{definition}[Random variables and distribution]\label{def:2.1} Let $(\Omega,\mathscr{F},\mathbb{P})$ be a probability space. A \textit{(real-valued) random variable} is a measurable real-valued function $X$ on $(\Omega,\mathscr{F})$. In other words, a real-valued function $X:\Omega\to\mathbb{R}$ is a random variable if
\begin{align*}
	\{X\leq x\}:= \left\{\omega\in\Omega:X(\omega)\leq x\right\}\in\mathscr{F},\ \forall x\in\mathbb{R}\ \Leftrightarrow\ X^{-1}\left(\mathscr{B}(\mathbb{R})\right)\subset\mathscr{F}.
\end{align*}
The collection $X^{-1}\left(\mathscr{B}(\mathbb{R})\right)$ is said to be the \textit{$\sigma$-algebra generated by $X$}. The function $F:\mathbb{R}\to[0,1]$,
\begin{align*}
	F(x)= \mathbb{P}\left(X\leq x\right) := \mathbb{P}\left(\{\omega\in\Omega:X(\omega)<x\}\right),\ \forall x\in\mathbb{R} \tag{2.1}\label{eq:2.1}
\end{align*}
is said to be the \textit{cumulative distribution function (c.d.f.)} of $X$, written $X\sim F$.
\end{definition}
\begin{remark} Generally, a measurable extended real-valued function $X:\Omega\to\overline{\mathbb{R}}$ is also called a random variable, if we have $\mathbb{P}(\vert X\vert=\infty)=\P(\{\omega\in\Omega:X(\omega)\in\{-\infty,\infty\}\})=0$.
\end{remark}
\begin{proposition}\label{prop:2.2} Let $F$ be the c.d.f. of a random variable $X$. Then $F$ satisfies the following:
\begin{itemize}
	\item[(i)] $F$ is monotone increasing on $\mathbb{R}$;
	\item[(ii)] $F$ is \textit{right-continuous}, i.e. $F(x)=\lim_{\epsilon\to 0^+}F(x+\epsilon)$ for all $x\in\mathbb{R}$;
	\item[(iii)] $F(-\infty):=\lim_{x\to-\infty}F(x)=0$, and $F(\infty):=\lim_{x\to\infty} F(x)=1$.
\end{itemize}
In fact, any function $F:\mathbb{R}\to[0,1]$ satisfying the properties (i)-(iii) is called a \textit{c.d.f.}.
\end{proposition}
\begin{proof}
Clearly $F$ is monotone increasing, and its left and right-hand limits exist everywhere. Then
\begin{align*}
	\lim_{\epsilon\to 0^+}F(x+\epsilon) &= \lim_{n\to\infty}F\left(x+\frac{1}{n}\right) = \lim_{n\to\infty}\mathbb{P}\left(\left\{\omega:X(\omega)\leq x+\frac{1}{n}\right\}\right) = \mathbb{P}\left(\bigcup_{n=1}^\infty\left\{\omega:X(\omega)\leq x+\frac{1}{n}\right\}\right)\\
	&= \mathbb{P}\left(\left\{\omega:X(\omega)\leq x\right\}\right) = F(x). 
\end{align*}
Then (ii) holds, and (iii) follows from a similar procedure.
\end{proof}

\paragraph{Remark.} Inspired by this proof, we can also associated $\mathbb{P}(X<x)$ with $F$ by the following formula:
\begin{align*}
	\mathbb{P}(X<x) = \mathbb{P}\left(\bigcup_{n=1}^\infty\left\{\omega:X(\omega)\leq x-\frac{1}{n}\right\}\right) = \lim_{n\to\infty}\mathbb{P}\left(X\leq x-\frac{1}{n}\right) = \lim_{\epsilon\to 0^+}F(x-\epsilon).
\end{align*}
Since $\P(X=x)=F(x)-\lim_{\epsilon\to 0^+} F(x-\epsilon)$, $F$ is continuous at a point $x\in\mathbb{R}$ if and only if $\P(X=x)=0$.

\begin{definition}[Distribution measure]\label{def:2.3} A random variable $X\sim F$ on $(\Omega,\mathscr{F},\mathbb{P})$ determines a pushforward measure $\mu_F=\mathbb{P}\circ X^{-1}$ on $(\mathbb{R},\mathscr{B}(\mathbb{R}))$:
$$\mu_F(B)=\mathbb{P}\left(X^{-1}B\right) = \mathbb{P}\left(X\in B\right) = \mathbb{P}\left(\{\omega:X(\omega)\in B\}\right),\ \forall B\in\mathscr{B}(\mathbb{R}).$$
The pushforward $\mu_F$ is said to be the \textit{distribution measure} of $X$, written $X\sim\mu_F$. It is easy to check that
\begin{align*}
	\mu_F((-\infty,b]) = F(b),\ \mu_F((a,\infty)) = 1-F(a),\ \mu_F((a,b]) = F(b)-F(a),\ \forall\ a<b.\tag{2.2}\label{eq:2.2}
\end{align*}
\end{definition}
\begin{remark} Let $\mathscr{S}$ be the collection of all finite unions of intervals of the following forms:
\begin{align*}
	(-\infty,b],\ (a,\infty),\ (a,b].
\end{align*}

Then $\mathscr{S}$ is a semiring of subsets of $\mathbb{R}$, and $\sigma(\mathscr{S})=\mathscr{B}(\mathbb{R})$. Given a c.d.f. $F:\mathbb{R}\to[0,1]$, we define a pre-measure $\mu_F$ on $\mathscr{S}$ by equation \hyperref[eq:2.2]{(2.2)}. Using Carathéodory's extension theorem, $\mu_F$ can be uniquely extended to a probability measure on $\mathscr{B}(\mathbb{R})$. Thus we find a one-to-one correspondence between a c.d.f. $F:\mathbb{R}\to[0,1]$ and Borel probability measure $\mu_F$ on $(\mathbb{R},\mathscr{B}(\mathbb{R}))$. In later discussion, we may not distinguish them.
\end{remark}

Let's see what the c.d.f. of a random variable may looks like.

\begin{definition}\label{def:2.4} Let $\mu$ be a Borel measure on $\mathbb{R}$. A point $x\in\mathbb{R}$ is said to be an \textit{atom} of $\mu$ if $\mu(\{x\})>0$.
\begin{itemize}
\item[(i)] (Discrete measure). $\mu$ is said to be \textit{discrete}, if there exists a countable subset $C$ of $\mathbb{R}$ such that
\begin{align*}
	\mu(A) = \sum_{x\in C\cap A}\mu(\{x\}),\ \forall A\in\mathscr{B}(\mathbb{R}).
\end{align*}
The atoms of $\mu$ are $D:=\{x\in\mathbb{R}:\mu(\{x\})>0\}\subset C$.
\item[(ii)] (Continuous measure). $\mu$ is said to be \textit{(absolutely) continuous} if $\mu\ll m$, where $m$ is the Lebesgue measure on $\mathbb{R}$. The Radon-Nikodym derivative $\rho:=\frac{d\mu}{dm}$ is said to be the \textit{density function} of $\mu$. If $\mu$ is a probability measure, $\rho$ is said to be the \textit{probability density function (p.d.f.)} of $\mu$.
\item[(iii)] (Singular measure). $\mu$ is said to be \textit{singular (continuous)} if $\mu$ has no atom and $\mu\perp m$, where $m$ is the Lebesgue measure on $\mathbb{R}$. In other words, $\mu$ is concentrated on a Lebesgue-null set $E$, where $\mu$ takes zero at each point of $E$.
\end{itemize}
\end{definition}
\begin{remark} The discrete and continuous measures are common. For example, the Poisson distribution is a discrete measure on $\mathbb{N}$, and the Gaussian distribution is a continuous measure on $\mathbb{R}$. Here we give an example of singular measures.
\end{remark}

\paragraph{Ternary Cantor sets and devil's stair.} A ternary Cantor set $K$ is obtained by repeatedly removing the open middle third from a collection of line segments, starting from the unit interval $[0,1]$:
\begin{align*}
	K_1=[0,1]\ \to\  K_2=\left[0,\frac{1}{3}\right]\cup\left[\frac{2}{3},1\right]\ \to\  K_3=\left[0,\frac{1}{9}\right]\cup\left[\frac{2}{9},\frac{1}{3}\right]\cup\left[\frac{2}{3},\frac{7}{9}\right]\cup\left[\frac{8}{9},1\right]\ \to\ \cdots.
\end{align*}

Indeed, the ternary Cantor set $K=\bigcup_{n=1}^\infty K_n$ is the set of numbers
in $[0,1]$ with a ternary expansion omitting the digit 1:
\begin{align*}
	K = \left\{\sum_{n=1}^\infty \frac{c_n}{3^n}:\ c_n\in\{0,2\}\right\},
\end{align*}
which is an uncountable Lebesgue-null set in $\mathbb{R}$. We define a function $f$ on $K$ by the following formula:
\begin{align*}
	f\left(\sum_{n=1}^\infty \frac{c_n}{3^n}\right) = \sum_{n=1}^\infty \frac{c_n}{2^{n+1}}.\tag{Devil's stair}
\end{align*}

It is clear that function $f:K\to[0,1]$ is monotone increasing and onto. Furthermore, for each connected component $(a,b)$ of $[0,1]\backslash K$, one have $f(a)=f(b)$. Hence we can extend $f$ to a continuous monotone increasing function on $F:[0,1]\to[0,1]$ that is constant on each connected component $(a,b)$ of $[0,1]\backslash K$. Clearly, $F$ is a c.d.f., and the associated distribution measure $\mu_F$ is a singular probability measure.

\begin{theorem}[Decomposition of Borel measures]\label{thm:2.5} If $\mu$ is a $\sigma$-finite Borel measure on $\mathbb{R}$, there exist uniquely discrete, continuous and singular $\sigma$-finite measures $\mu_d$, $\mu_c$ and $\mu_s$ such that $\mu=\mu_d+\mu_c+\mu_s$.
\end{theorem}
\begin{proof}
Let $D=\{x\in\mathbb{R}:\mu(\{x\})>0\}$. Since $\mu$ is $\sigma$-finite, $D$ is at most countable. Then $\mu_d(A)=\mu(A\cap D)$ is a discrete measure. ($\mu_d\equiv 0$ if $D=\emptyset$.) Furthermore, $\mu_d$ is unique and supported on all atoms of $\mu$.

By \hyperref[thm:1.60]{Theorem 1.60}, the measure $\mu-\mu_d$ has a unique Lebesgue decomposition $\mu-\mu_d=\mu_c+\mu_s$, where $\mu_c\ll m$ and $\mu_s\perp m$. Since $\mu-\mu_d$ has no atom, the result follows.
\end{proof}

\begin{remark} Likewise, a c.d.f. $F$ admits a unique convex combination $F=\alpha F_d + \beta F_c + (1-\alpha-\beta)F_s$, where the associated distribution measures of $F_d$, $F_c$ and $F_s$ are discrete, continuous and singular, respectively.
\end{remark}

\begin{definition}[Random vectors]\label{def:2.6} When $X_1,X_2,\cdots,X_n$ are all random variables, the function
\begin{align*}
	X=(X_1,\cdots,X_n):\Omega\to\mathbb{R}^n
\end{align*}
is said to be a \textit{random vector}. The function
\begin{align*}
	F(x_1,\cdots,x_n)=\mathbb{P}(X_1\leq x_1,\cdots,X_n\leq x_n)
\end{align*}
is called the \textit{joint distribution (function)} of $(X_1,\cdots,X_n)$. For each $k$, the function
\begin{align*}
	F_k(x)=\mathbb{P}(X_k=x) = F(\infty,\cdots,\infty,\underset{k\textit{-th}}{x},\infty,\cdots,\infty)
\end{align*}
is called the \textit{marginal distribution (function)} of $X_k$.
\end{definition}
\begin{remark} By \hyperref[thm:1.35]{Theorem 1.35}, a random vector is also a measurable function $X:(\Omega,\mathscr{F})\to(\mathbb{R}^n,\mathscr{B}(\mathbb{R}^n))$. Furthermore, the c.d.f. $F:\mathbb{R}^n\to[0,1]$ of $X$ satisfies the following:
\begin{itemize}
	\item[(i)] $F$ is monotone increasing with respect to each variable on $\mathbb{R}$;
	\item[(ii)] $F$ is \textit{right-continuous} on each variable;
	\item[(iii)] For all $x_1,\cdots,x_n\in\mathbb{R}$, $\lim_{x_j\to-\infty}F(x_1,\cdots,x_n)=0$, and $\lim_{x_1,\cdots,x_n\to\infty} F(x_1,\cdots,x_n)=1$.
\end{itemize}
\end{remark}
\begin{lemma}\label{lemma:2.7} Assume that two random variables $X$ and $Y$ has the same distribution measure. We say they are \textit{identically distributed} and write $X\overset{\d}{=}Y$. For any measurable function $\varphi$ such that $\varphi(X)$ and $\varphi(Y)$ are well-defined, we have $\varphi(X)\overset{\d}{=}\varphi(Y)$.
\end{lemma}
\begin{proof}
Denote by $\mu$ the distribution measures of $X$ and $Y$, respectively. For all $b\in\mathbb{R}$, we have
\begin{align*}
	\mathbb{P}(\varphi(X)\leq b) = \mathbb{P}(X\in\varphi^{-1}((-\infty,b])) &= \mu\left(\varphi^{-1}((-\infty,b])\right) = \mathbb{P}(Y\in\varphi^{-1}((-\infty,b])) = \mathbb{P}(\varphi(Y)\leq b).
\end{align*}
Then $\varphi(X)$ and $\varphi(Y)$ has the same c.d.f., hence the same distribution measure.
\end{proof}

Now we introduce independence of random variables.

\begin{definition}[Independence]\label{def:2.8} Two events $A,B\in\mathscr{F}$ are said to be \textit{independent} if $\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)$. Also, $n$ events $A_1,\cdots,A_n\in\mathscr{F}$ are said to be \textit{mutually independent} if for all $I\subset\{1,\cdots,n\}$, it holds
\begin{align*}
	\mathbb{P}\left(\bigcap_{i\in I} A_{i}\right) = \prod_{i\in I}\mathbb{P}(A_{i}).
\end{align*}
\end{definition}
\begin{remark} A group of pairwise independent events are not always mutually independent. For example, consider the discrete measure on $\Omega=\{1,2,3,4\}$ with a probability mass of $1/4$ at each atom. Let $A=\{1,2\}$, $B=\{2,3\}$, $C=\{1,3\}$. Then $\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)=1/4$, and so do event pairs $(B,C)$ and $(C,A)$. Nevertheless, $A,B$ and $C$ are not mutually independent, since $\mathbb{P}(A\cap B\cap C)=0$.
\end{remark}
\begin{definition}[Independence of random variables]\label{def:2.9} Two random variables $X$ and $Y$ are said to be \textit{independent} if their joint distribution is the product of marginal distributions:
\begin{align*}
	\mathbb{P}(X\leq x,Y\leq y)=\mathbb{P}(X\leq x)\,\mathbb{P}(Y\leq y),\ \forall x,y\in\mathbb{R}.
\end{align*}
Similarly, given $n$ random variables $X_1,\cdots,X_n$, they are said to be \textit{(mutually) independent} if
\begin{align*}
	\mathbb{P}(X_1\leq x_1,\cdots,X_n\leq x_n)=\prod_{i=1}^n\mathbb{P}(X_i\leq x_i),\ \forall x_1,\cdots,x_n\in\mathbb{R}.\tag{2.3}\label{eq:2.3}
\end{align*}
Clearly, if \hyperref[eq:2.3]{(2.3)} is satisfied, then for all index sets $I\subset\{1,\cdots,n\}$, we have
\begin{align*}
	\mathbb{P}\left(\bigcup_{i\in I}\{X_i\leq x_i\}\right)=\prod_{i\in I}\mathbb{P}(X_i\leq x_i),\ \forall x_1,\cdots,x_n\in\mathbb{R}.
\end{align*}
\end{definition}

\begin{theorem}\label{thm:2.10} Two random variables $X$ and $Y$ are independent if and only if for all $A,B\in\mathscr{B}(\mathbb{R})$,
\begin{align*}
	\P(X\in A,Y\in B)=\P(X\in A)\,\P(Y\in B).\tag{2.4}\label{eq:2.4}
\end{align*}
In other words, if $X\sim\mu_X$ and $Y\sim\mu_Y$, then $X$ and $Y$ are independent if and only if $(X,Y)\sim\mu_X\otimes\mu_Y$.
\end{theorem}
\begin{proof}
We only prove that \hyperref[eq:2.4]{(2.4)} holds when $X$ and $Y$ are independent. Given $A=(-\infty,x]$, let 
\begin{align*}
	\mathscr{M}_A=\left\{B\in\mathscr{B}(\mathbb{R}):\P(X\in A,Y\in B)=\P(X\in A)\,\P(Y\in B)\right\}.
\end{align*}

Clearly, $\mathscr{M}_A$ is a $\lambda$-system. By independence of $X$ and $Y$, we have $\mathscr{M}_0:=\{(-\infty,y]:y\in\mathbb{R}\}\subset\mathscr{M}_A$. Since $\mathscr{M}_0$ is a $\pi$-system, and since $\mathscr{B}(\mathbb{R})=\sigma(\mathscr{M}_0)$, by Sierpiński-Dynkin $\pi$-$\lambda$ system, we have $\mathscr{M}_A=\mathscr{B}(\mathbb{R})$.

Given any $B\in\mathscr{B}(\mathbb{R})$, let 
\begin{align*}
	\mathscr{M}^B=\left\{A\in\mathscr{B}(\mathbb{R}):\P(X\in A,Y\in B)=\P(X\in A)\,\P(Y\in B)\right\}.
\end{align*}

We can also verify that $\mathscr{M}^B$ is a $\lambda$-system, and $\mathscr{M}_0\subset\mathscr{M}^B$. Again by Sierpiński-Dynkin $\pi$-$\lambda$ system, we have $\mathscr{M}^B=\mathscr{B}(\mathbb{R})$. Hence \hyperref[eq:2.4]{(2.4)} holds for all $A,B\in\mathscr{B}(\mathbb{R})$.
\end{proof}

\begin{remark} Let $\mathscr{F}_1$ and $\mathscr{F}_2$ be two sub $\sigma$-algebras of $\mathscr{F}$. Then $\mathscr{F}_1$ and $\mathscr{F}_2$ are said to be independent if $\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)$ for all $A\in\mathscr{F}_1$ and all $B\in\mathscr{F}_2$.  By \hyperref[thm:2.10]{Theorem 2.10}, two random variables $X$ and $Y$ are independent indeed implies that the $\sigma$-algebras $\sigma(X)$ and $\sigma(Y)$ generated by $X$ and $Y$ are independent.
\end{remark}
\begin{corollary}\label{cor:2.11} If $X$ and $Y$ are two independent random variables, and $\psi,\varphi:\mathbb{R}\to\mathbb{R}$ are Lebesgue-measurable functions, then $\psi(X)$ and $\varphi(Y)$ are independent.
\end{corollary}
\begin{proof}
Let $A$ and $B$ be two Borel sets on $\mathbb{R}$. Then
\begin{align*}
	\P(\psi(X)\in A,\varphi(X)\in B) &= \P(X\in\psi^{-1}(A),Y\in\varphi^{-1}(B))\\
	&= \P(X\in\psi^{-1}(A))\,\P(Y\in\varphi^{-1}(B)) = \P(\psi(X)\in A)\,\P(\varphi(Y)\in B).
\end{align*}
Then we finish the proof.
\end{proof}
\newpage
\subsection{Expectation}
\paragraph{Definition 2.12\label{def:2.12}} (Expectation). Let $X\sim F$ be a random variable on $(\Omega,\mathscr{F},\mathbb{P})$. If $X$ is integrable, the \textit{expectation} of $X$ is defined as the Lebesgue integral
\begin{align*}
	\E X = \int X\,\d \P.
\end{align*}
Similarly, for any Lebesgue-measurable function $\varphi:\mathbb{R}\to\mathbb{R}$, if $\varphi\circ X$ is integrable, define
\begin{align*}
	\E[\varphi(X)] = \int \varphi\circ X\,\d \P.
\end{align*}
\paragraph{Theorem 2.13\label{thm:2.13}} (Integral transform). Using the integral transform formula in \hyperref[thm:1.49]{Theorem 1.49}, we immediately know that the expectation of $\varphi(X)$ equals the Lebesgue-Stieltjes integral
\begin{align*}
	\E[\varphi(X)] = \int \varphi\circ X\,\d \P = \int \varphi\,\d \mu_F =: \int \varphi(x)\,\d F(x).
\end{align*}
Particularly,
\begin{align*}
	\E X = \int x\,\d \mu_F(x) = \int x\,\d F(x).
\end{align*}
If $X\sim\mu_F$ is a discrete random variable, let $\mathcal{A}$ be the set of all atoms of $\mu_F$. Then we have
\begin{align*}
	\E[\varphi(X)] = \sum_{x\in\mathcal{A}}\varphi(x)\mu_F(\{x\}).
\end{align*}
If $X\sim\mu_F$ is a continuous random variable with density $\rho$, i.e. $\mu_F$ is continuous and $\frac{d\mu_F}{dm}=\rho$, then
\begin{align*}
	\E[\mathds{1}_A(X)] = \int\mathds{1}_A\,\d \mu_F = \mu_F(A) = \int \mathds{1}_A\rho\,\d m.
\end{align*}
By simple function approximation, for all measurable $\varphi$ with $\varphi(X)$ integrable, we have
\begin{align*}
	\E[\varphi(X)] = \int \varphi\rho\,\d m = \int\varphi(x)\rho(x)\,\d x.
\end{align*}

Another useful formula for calculating expectation follows from Fubini's theorem.
\paragraph{Theorem 2.14.\label{thm:2.14}} Let $X$ be a nonnegative random variable. Then
\begin{align*}
	\E X = \int_0^\infty \P(X>x)\,\d x.
\end{align*}
\begin{proof}
By Fubini's theorem,
\begin{align*}
	\int_0^\infty \P(X>x)\,\d x = \int_0^\infty \E[\mathds{1}_{\{X>x\}}]\,\d x = \E\left[\int_0^\infty \mathds{1}_{\{X>x\}}\,\d x\right] = \E\left[\int_0^X\,\d x\right] = \E X.
\end{align*}
Note that the function
\begin{align*}
	\mathds{1}_{\{X>x\}} = \mathds{1}_{\{X(\omega)>x\}} = \mathds{1}_{(0,\infty)}(X(\omega)-x)
\end{align*}
is defined on $\Omega\times\mathbb{R}$. Since $X$ is measurable, so is $\mathds{1}_{\{X>x\}}$. 
\end{proof}

\paragraph{Proposition 2.15.\label{prop:2.15}} Let $X$ and $Y$ be two random variables. The following properties of expectation follows from Lebesgue integral:
\begin{itemize}
	\item[(i)] If $X\geq 0\ a.s.$, i.e. $\P(X\geq 0)=1$, then $\E X\geq 0$. Additionally, if $\E X=0$, then $X=0\ a.s.$.
	\item[(ii)] For all $\alpha,\beta\in\mathbb{R}$, $\E[\alpha X+\beta Y]=\alpha \E X + \beta \E Y$. 
	\item[(iii)] For $1\leq p <\infty$, denote $\Vert X\Vert_p = \left(\E[\vert X\vert^p]\right)^{1/p}$.
	\begin{itemize}
		\item[$\bullet$] (Hölder's inequality). If $p,q>1$, $r\geq 1$ and $\frac{1}{p}+\frac{1}{q}=\frac{1}{r}$, then $\Vert XY\Vert_r\leq\Vert X\Vert_p\left\Vert Y\right\Vert_q$;
		\item[$\bullet$] (Moment inequality). If $1\leq p<q<\infty$, then $\Vert X\Vert_p\leq\Vert X\Vert_q$.
		\item[$\bullet$] (Minkowski's inequality). If $1\leq p\leq\infty$, then $\Vert X+Y\Vert_p \leq\Vert X\Vert_p + \Vert Y\Vert_p$.
	\end{itemize}
    \item[(iv)] (Jensen's inequality). If $g:\mathbb{R}\to\mathbb{R}$ is a convex function, and both $\E X$ and $\E[g(X)]$ are well-defined, then $\E[g(X)]\geq g(\E X)$. In addition, if $g$ is strongly convex and $\E[g(X)]=g(\E X)$, then $X=\E X\ a.s.$.
\end{itemize}
\begin{proof}
(iii) We first assume $r=1$. The convexity of $x\mapsto -\ln x$ implies Young's inequality:
\begin{align*}
	\frac{1}{p}\ln a^p + \frac{1}{q}\ln b^q \geq \ln\left(\frac{a^p}{p}+\frac{b^q}{q}\right)\ \Rightarrow\ \frac{a^p}{p}+\frac{b^q}{q}\leq ab,\quad \forall a,b>0,\ \frac{1}{p}+\frac{1}{q}=1.
\end{align*}
Then we have
\begin{align*}
	\frac{1}{p}\frac{\vert X\vert^p}{\Vert X\Vert_p^p} + \frac{1}{q}\frac{\vert Y\vert^q}{\Vert Y\Vert_q^q}\leq \frac{\vert XY\vert}{\left\Vert X\right\Vert_p\left\Vert Y\right\Vert_q},\quad \frac{1}{p}+\frac{1}{q}=1.\tag{2.5}\label{eq:2.5}
\end{align*}
Taking expectation on both sides of \hyperref[eq:2.5]{(2.5)} concludes. For the case $r>1$, we have
\begin{align*}
	\Vert XY\Vert_r^r = \left\Vert\left\vert XY\right\vert^r\right\Vert_1\leq\left\Vert\left\vert X\right\vert^r\right\Vert_{p/r}\left\Vert\left\vert Y\right\vert^r\right\Vert_{q/r} = \left\Vert X\right\Vert_p^r\left\Vert Y\right\Vert_q^r,\quad \frac{1}{p}+\frac{1}{q}=\frac{1}{r}.
\end{align*}
Hence we have Hölder's inequality. By taking $Y=1$ in Hölder's inequality, we have the moment equality. To obtain Minkowski's inequality $(p>1)$, take $1/q=1-1/p$. Then
\begin{align*}
	\Vert X+Y\Vert_p^p &\leq \E\left[\vert X+Y\vert^{p-1}\vert X\vert\right] + \E\left[\vert X+Y\vert^{p-1}\vert Y\vert\right]\\
	&\leq \left\Vert X\right\Vert_p\left(\E\left[\vert X+Y\vert^{(p-1)q}\right]\right)^{1/q} + \left\Vert Y\right\Vert_p\left(\E\left[\vert X+Y\vert^{(p-1)q}\right]\right)^{1/q} \tag{By Hölder's inequality}\\
	&= \left(\Vert X\Vert_p + \Vert Y\Vert_p\right)\left\Vert X+Y\right\Vert^{p/q}_p = \left(\Vert X\Vert_p + \Vert Y\Vert_p\right)\left\Vert X+Y\right\Vert^{p-1}.
\end{align*}

(iv) Since $g$ is a convex function defined on an open set $\mathbb{R}$, its subgradient set $\partial_x g$ at $x$ is nonempty for every $x\in\mathbb{R}$. By taking $x_0=\E X$ and $\alpha\in\partial_{x_0}g$, we have
\begin{align*}
	g(x) \geq g(x_0) + \alpha(x-x_0),\ \forall x\in\mathbb{R}.\tag{2.6}\label{eq:2.6}
\end{align*}
Taking expectation on both sides of \hyperref[eq:2.6]{(2.6)} immediately yields Jensen's inequality.

Now assume $g$ is strongly convex, then the inequality \hyperref[eq:2.6]{(2.6)} becomes strict when $x\neq x_0$. Let $$\varphi(x):=g(x)-g(x_0)-\alpha(x-x_0),$$ 
then $\varphi(x)=0$ implies $x=x_0$, and $\varphi(X)$ is a nonnegative random variable. If $\E[g(X)]=g(x_0)$, we have $\E[\varphi(X)]=0$. By (i), we have $\varphi(X)=0\ a.s.$, and $X=x_0\ a.s.$.
\end{proof}
\newpage

\subsection{Conditional Expectation and Distribution}
Now we introduce conditional expectation. In contrast to expectation, which takes real number, the conditional expectation is a random variable.
\paragraph{Definition 2.16\label{def:2.16}} (Conditional expectation). Let $\mathscr{G}$ be a sub $\sigma$-algebra of $\mathscr{F}$, i.e. $\mathscr{G}\subset\mathscr{F}$ and $\mathscr{G}$ is a $\sigma$-algebra of subsets of $\Omega$. Let $Y$ be a random variable. If $\E\vert Y\vert<\infty$, the \textit{conditional expectation of $Y$ with respect to $\mathscr{G}$} is defined as any random variable $\xi$ satisfying the following:
\begin{itemize}
	\item[(i)] $\xi$ is $\mathscr{G}$-measurable, i.e. $\xi^{-1}\left(\mathscr{B}(\mathbb{R})\right)\subset\mathscr{G}$;
	\item[(ii)] $\E[Y\mathds{1}_A]=\E[\xi\mathds{1}_A]$ for all $A\in\mathscr{G}$.
\end{itemize}
\paragraph{Remark.} We define a finite signed measure $\mu:\mathscr{G}\to\overline{\mathbb{R}}$ by assigning $\mu(A) := \E[Y\mathds{1}_A]$ for all $A\in\mathscr{G}$. 
Then $\P(A)=0$ implies $\mu(A)=0$, and $\mu\ll\P|_\mathscr{G}$. By Radon-Nikodym theorem, we take 
\begin{align*}
	\rho=\frac{d\mu}{d\P|_\mathscr{G}}\ \Rightarrow\ \mu(A)=\int_A\rho\,\d \P|_\mathscr{G}=\E[\rho\mathds{1}_A],\ \forall A\in\mathscr{G}.
\end{align*}

Then $\rho$ is the desired $\mathscr{G}$-measurable function.

Furthermore, if $\xi$ is a conditional expectation of $Y$ with respect to $\mathscr{G}$, we define $A=\{\omega:\rho(\omega)\geq\xi(\omega)\}$. Then $A$ is $\mathscr{G}$-measurable, and
\begin{align*}
	\E[(\rho-\xi)\mathds{1}_A]=\E[Y\mathds{1}_A]-\E[Y\mathds{1}_A] = 0\ \Rightarrow\ \rho\leq\xi\ a.e..
\end{align*}
By symmetry, we also have $\xi\leq\rho\ a.e.$. Hence $\xi=\rho\ a.e.$, and we see that the conditional expectation is uniquely determined almost everywhere, and we write $\E[Y|\mathscr{G}]$ for it.

\paragraph{} In fact, the expectation $\E[Y]$ can be viewed as the conditional expectation $\E[Y|\mathscr{F}_0]$, where $\mathscr{F}_0=\{\Omega,\emptyset\}$ is the smallest $\sigma$-algebra on $\Omega$.

\paragraph{Proposition 2.17\label{prop:2.17}} (Properties of conditional expectation). Let $\mathscr{G}\subset\mathscr{F}$ be a $\sigma$-algebra of subsets of $\Omega$. Let $X$ and $Y$ be two integrable random variables, that is, $X,Y\in L^1(\Omega,\mathscr{F},\mathbb{P})$. 
\begin{itemize}
	\item[(i)] (Total expectation formula). $\E\left[\E\left[X|\mathscr{G}\right]\right]=\E X$. In addition, if $\mathscr{H}$ is another sub $\sigma$-algebra of $\mathscr{F}$ such that $\mathscr{H}\subset\mathscr{G}$, then $\E\left[\E\left[X|\mathscr{G}\right]|\mathscr{H}\right]=\E\left[\E\left[X|\mathscr{H}\right]|\mathscr{G}\right]=\E[X|\mathscr{H}]\ a.s.$.
	\item[(ii)] (Monotonicity). If $X\geq 0\ a.s.$, then $\E[X|\mathscr{G}]\geq 0\ a.s.$. Hence $X\leq Y\ a.s.$ implies $\E[X|\mathscr{G}]\leq\E[Y|\mathscr{G}]\ a.s.$. In particular, $\left\vert\E[X|\mathscr{G}]\right\vert\leq\E[\left\vert X\right\vert|\mathscr{G}]\ a.s.$.
	\item[(iii)] ($\mathscr{G}$-linearity). For any $\mathscr{G}$-measurable random variable $\xi$ and $\eta$, it holds $\E[\xi X+\eta Y]=\xi\E X + \eta\E Y\ a.s.$.
	\item[(iv)] (Independence law). $\sigma(X)$ is independent of $\mathscr{G}$ if and only if $\E[\varphi(X)|\mathscr{G}]=\E[\varphi(X)]\ a.s.$ $\forall$ measurable $\varphi$.
	\item[(v)] (Conditional Jensen's inequality). If $g:\mathbb{R}\to\mathbb{R}$ is a convex function such that $g(X)$ is integrable, then $\E[g(X)|\mathscr{G}]\geq g\left(\E[X|\mathscr{G}]\right)\ a.s.$.
	\item[(vi)] The following inequalities almost surely hold:
	\begin{itemize}
		\item[$\bullet$] (Conditional Hölder's inequality). If $p,q>1$, $r\geq 1$ and $\frac{1}{p}+\frac{1}{q}=\frac{1}{r}$, then $$\E\left[\vert XY\vert^r|\mathscr{G}\right]^{1/r}\leq\E\left[\vert X\vert^p|\mathscr{G}\right]^{1/p}\E\left[\vert Y\vert^q|\mathscr{G}\right]^{1/q}\ a.s..$$
		\item[$\bullet$] (Conditional moment inequality). If $1\leq p < q$, then $\E\left[\vert X\vert^p|\mathscr{G}\right]^{1/p}\leq\E\left[\vert X\vert^q|\mathscr{G}\right]^{1/q}\ a.s.$.
		\item[$\bullet$] (Conditional Minkowski inequality). If $p\geq 1$, then $$\E\left[\vert X+Y\vert^p|\mathscr{G}\right]^{1/p}\leq\E\left[\vert X\vert^p|\mathscr{G}\right]^{1/p}+\E\left[\vert Y\vert^p|\mathscr{G}\right]^{1/p}\ a.s..$$
	\end{itemize}
\end{itemize}
\begin{proof}
(i) Let $\xi=\E[X|\mathscr{G}]$, then $\E[X\mathds{1}_A]=\E[\xi\mathds{1}_A]$ for all $A\in\mathscr{G}$. Choose $A=\Omega$, so we have $\E\left[\E\left[X|\mathscr{G}\right]\right]=\E X$. 

If $\mathscr{H}$ is another sub $\sigma$-algebra of $\mathscr{F}$ such that $\mathscr{H}\subset\mathscr{G}$, then a $\mathscr{G}$-measurable function is also $\mathscr{H}$-measurable, and $\E\left[\E\left[X|\mathscr{H}\right]|\mathscr{G}\right]=\E[X|\mathscr{H}]\ a.s.$. Let $\xi=\E[X|\mathscr{G}]$, and $\eta=\E[\E[X|\mathscr{G}]|\mathscr{H}]$. Then for all $A\in\mathscr{H}\subset\mathscr{G}$, we have $\E[\eta\mathds{1}_A]=\E[\xi\mathds{1}_A]=\E[X\mathds{1}_A]$, which implies $\E[X|\mathscr{H}]=\eta\ a.s.$.

\paragraph{}  (ii) Let $\xi=\E[X|\mathscr{G}]$, and define $A_n=\{\omega:\xi(\omega)\leq -n^{-1}\}\subset\mathscr{G}$. Then $$-n^{-1}\P(A_n)\geq\E[\xi\mathds{1}_{A_n}]=\E[X\mathds{1}_{A_n}]\geq 0\ \Rightarrow\ \P(A_n)=0,\ \forall n\in\mathbb{N}.$$
Let $A=\{\omega:\xi(\omega)<0\}=\bigcup_{n=1}^\infty A_n$. Then $\P(A)=\lim_{n\to\infty}\P(A_n)=0$, which implies $\xi\geq 0\ a.s.$.  

Since $\vert X\vert - X^+$ and $\vert X\vert - X^-$ are $a.s.$ nonnegative, we have $\left\vert\E[X|\mathscr{G}]\right\vert\leq\E[\left\vert X\right\vert|\mathscr{G}]\ a.s.$.

\paragraph{}  (iii) The $\mathbb{R}$-linearity of $\E[\cdot|\mathscr{G}]$ follows from linear operator $\E:L^1(\Omega,\mathscr{F},\mathbb{P})\to\mathbb{R}$. Now we prove that $\E[\cdot|\mathscr{G}]$ is $\mathscr{G}$-linear. For all $A\in\mathscr{G}$, we have
\begin{align*}
	\E[X\mathds{1}_A|\mathscr{G}]=\mathds{1}_A\E[X|\mathscr{G}]\ \Rightarrow\ \E[X\mathds{1}_A\mathds{1}_B]=\mathds{1}_A\E[X\mathds{1}_B],\ \forall B\in\mathscr{G}.
\end{align*}

By simple function approximation, for a $\mathscr{G}$-measurable function $\xi$ such that $\xi X\in L^1(\Omega,\mathscr{F},\P)$, we have
\begin{align*}
\E[\xi X\mathds{1}_B]=\xi\E[X\mathds{1}_B],\ \forall B\in\mathscr{G}\ \Rightarrow\ \E[\xi X|\mathscr{G}]=\xi\E[X|\mathscr{G}].
\end{align*}
Hence $\E[\cdot|\mathscr{G}]$ is a $\mathscr{G}$-linear operator. 

\paragraph{}  (iv) If $\sigma(X)$ and $\mathscr{G}$ are independent, we have
\begin{align*}
	\E[\mathds{1}_A(X)\mathds{1}_B]=\P(\{X\in A\}\cap B) = \P(X\in A)\,\P(B) = \E[\mathds{1}_A(X)]\E[\mathds{1}_B],\ \forall A\in\mathscr{B}(\mathbb{R}),\ B\in\mathscr{G}.
\end{align*}
Since $A$ is arbitrary, by simple function approximation, for any measurable $\varphi$ such that $\E\left\vert\varphi(X)\right\vert<\infty$,
\begin{align*}
	\E[\varphi(X)\mathds{1}_B] = \E[\varphi(X)]\E[\mathds{1}_B]=\E[\E[\varphi(X)]\mathds{1}_B],\ \forall B\in\mathscr{G}\ \Rightarrow\ \E[\varphi(X)|\mathscr{G}]=\E[\varphi(X)]\ a.s..
\end{align*}

Conversely, if $\E[X|\mathscr{G}]$, then for all $A\in\mathscr{B}(\mathbb{R})$ and all $B\in\mathscr{G}$, we have
\begin{align*}
	\P(\{X\in A\}\cap B)=\E[\mathds{1}_A(X)\mathds{1}_B]=\E[\E[\mathds{1}_A(X)\mathds{1}_B|\mathscr{G}]] = \E[\mathds{1}_B]\E[\mathds{1}_A(X)] = \P(B)\P(X\in A).
\end{align*}

\paragraph{} (v) Since $g$ is a convex function, there exists a countable set $\mathcal{C}\subset\mathbb{R}^2$ such that $g(x)=\sup_{(a,b)\in\mathcal{S}}(a+bx)$. That is, $g$ is the supremum of a countable collection of affine functions. Then $a+bX\leq g(X)$ for all $(a,b)\in\mathcal{S}$. By monotonicity and linearity of conditional expectation, we have $a+b\,\E[X|\mathscr{G}]\leq\E[g(X)|\mathscr{G}]\ a.s.$ for all $a,b\in\mathcal{S}$. Since $\mathcal{S}$ is countable, we have
\begin{align*}
	\P\left(\sup_{(a_n,b_n)\in\mathcal{S}}\left(a_n+b_n\,\E[X|\mathscr{G}]\right)>\E[g(X)|\mathscr{G}]\right) &= \P\left(\bigcap_{(a_n,b_n)\in\mathcal{S}}\left\{a_n+b_n\,\E[X|\mathscr{G}]>\E[g(X)|\mathscr{G}]\right\}\right)\\
	&\leq \lim_{n\to\infty}\sum_{k=1}^n\P\left(a_k+b_k\,\E[X|\mathscr{G}]>\E[g(X)|\mathscr{G}]\right) = 0.
\end{align*}
Hence $g(\E[X|\mathscr{G}])\leq \E[g(X)|\mathscr{G}]\ a.s.$.

\paragraph{} (vi) The conditional Hölder's inequality follows from Young's inequality and monotonicity of conditional expectation. The remaining part of this proof is totally parallel to \hyperref[prop:2.15]{Proposition 2.15 (iii)}.
\end{proof}

\paragraph{Remark.} Given a random variable $X\in L^p(\Omega,\mathscr{F},\P)$, where $1\leq p < \infty$. By conditional Jensen's inequality,
\begin{align*}
	\left(\E[X|\mathscr{G}]\right)^p\leq\E\left[\vert X\vert^p|\mathscr{G}\right]\ a.s.\ \Rightarrow\ \left\Vert\E[X|\mathscr{G}]\right\Vert_p^p \leq \E\left[\E\left[\vert X\vert^p|\mathscr{G}\right]\right] = \left\Vert X\right\Vert_p^p\ \Rightarrow\ \frac{\left\Vert\E[X|\mathscr{G}]\right\Vert_p}{\left\Vert X\right\Vert_p}\leq 1.
\end{align*}
Hence $\E[\cdot|\mathscr{G}]:L^p(\Omega,\mathscr{F},\P)\to L^p(\Omega,\mathscr{F},\P)$ is a bounded linear operator, and $\Vert\E[\cdot|\mathscr{G}]\Vert\leq 1$. Particularly, it can be viewed as a projection operator on the Hilbert space $L^2(\Omega,\mathscr{F},\P)$ of square-integrable variables. \vspace{0.1cm}

The convergence theorems for expectation can be extended to conditional expectation.
\paragraph{Theorem 2.18.\label{thm:2.18}} Let $\mathscr{G}$ be a sub $\sigma$-algebra of $\mathscr{F}$. 
\begin{itemize}
	\item[(i)] (Conditional monotone convergence theorem). Let $(X_n)_{n=1}^\infty$ be a increasing sequence of $L^1$ nonnegative random variables such that $X_n\uparrow X\in L^1(\Omega,\scr{F},\P)$. Then
	\begin{align*}
		\lim_{n\to\infty}\E[X_n|\scr{G}]=\E\left[X|\scr{G}\right]\ a.s..
	\end{align*}
	\item[(ii)] (Conditional Fatou's lemma). Let $(X_n)_{n=1}^\infty$ be a sequence of nonnegative $L^1$ random variables. Then
	\begin{align*}
		\E\left[\liminf_{n\to\infty} X_n\big|\mathscr{G}\right]\leq\liminf_{n\to\infty}\E[X_n|\mathscr{G}]\ a.s..
	\end{align*}
	\item[(iii)] (Conditional dominated convergence theorem). If $(X_n)_{n=1}^\infty$ is a sequence of random variables such that $X_n\to X\ a.s.$, and there exists a integrable random variable $Y\in L^1(\Omega,\mathscr{F},\P)$ such that $\vert X_n\vert\leq Y\ a.s.$ for all $n\in\mathbb{N}$, then
	\begin{align*}
		\E[X|\mathscr{G}]=\lim_{n\to\infty}\E[X_n|\mathscr{G}]\ a.s.\ and\ in\ L^1.
	\end{align*}
\end{itemize}
\begin{proof}
(i) Define $Y_n=\E[X-X_n|\scr{G}]$. By monotonicity of conditional expectation, $Y_n$ is a decreasing sequence. We denote by $Y$ the limit of sequence $(Y_n)$. For each $A\in\scr{G}$, 
\begin{align*}
	\E[Y_n\mathds{1}_A]=\E[(X-X_n)\mathds{1}_A].
\end{align*}
Since $\vert X-X_n\vert\leq\vert X\vert\in L^1(\Omega,\scr{F},\P)$, by Lebesgue dominated convergence theorem,  $$\E[Y\mathds{1}_A]=\lim_{n\to\infty}\E[Y_n\mathds{1}_A]=\lim_{n\to\infty}\E[(X-X_n)\mathds{1}_A]=\E\left[\lim_{n\to\infty}(X-X_n)\mathds{1}_A\right]=0.$$
Since $Y\geq 0$ is $\scr{G}$-measurable, $Y=0\ a.s.$, and the desired limit follows.

(ii) Let $Y_n=\inf_{k\geq n}X_k$, which is a increasing sequence of nonnegative $L^1$ random variables. By monotonicity of conditional expectation,\vspace{-0.1cm}
\begin{align*}
	\E[Y_n|\scr{G}]\leq\E[X_k|\scr{G}],\quad\text{for all}\ k\geq n.
\end{align*}
Hence $\E[Y_n|\scr{G}]\leq\inf_{k\geq n}\E[X_k|\scr{G}]$, and by (i),
\begin{align*}
	\E\left[\lim_{n\to\infty} Y_n\big|\scr{G}\right]=\lim_{n\to\infty}\E\left[Y_n|\scr{G}\right]\leq\liminf_{n\to\infty}\E[X_n|\scr{G}]\ a.s..
\end{align*} 
	
(iii) The almost sure convergence follows by applying (ii) on sequences $(Y+X_n)$ and $(Y-X_n)$. For the $L^1$ convergence, note that $0\leq\vert X_n-X\vert\leq 2Y$. By Lebesgue dominated convergence theorem, 
\begin{align*}
	\E\left[\,\left\vert\E[X_n|\mathscr{G}]-\E[X|\mathscr{G}]\right\vert\,\right]\leq \E\left[\E[\left\vert X_n-X\right\vert|\mathscr{G}]\right] = \E\left\vert X_n-X\right\vert\to 0.
\end{align*}
Hence $\E[X_n|\mathscr{G}]$ converges to $\E[X|\mathscr{G}]$ in $L^1$-norm.
\end{proof}


\paragraph{Theorem 2.19.\label{thm:2.19}} Let $\mathcal{H}=L^2(\Omega,\mathscr{F},\P)$ be the space of square-integrable random variables. Define
\begin{align*}
	\langle X,Y\rangle = \E[XY],\ \forall X,Y\in\mathcal{H}.
\end{align*}
Then $(\mathcal{H},\langle\cdot,\cdot\rangle)$ is a Hilbert space. Given a sub $\sigma$-algebra $\mathscr{G}\subset\mathscr{F}$, we also define $\mathcal{H}_\mathscr{G}=L^2(\Omega,\mathscr{G},\P|_\mathscr{G})$, then $\mathcal{H}_\mathscr{G}$ is a closed subspace of $\mathcal{H}$, and the conditional expectation operator $\E[\cdot|\mathscr{G}]$ is the projection onto $\mathcal{H}_\mathscr{G}$, i.e.
\begin{align*}
	\E[Y|\mathscr{G}] = \underset{X\in\mathcal{H}_\mathscr{G}}{\mathrm{argmin}}\,\E\left[(Y-X)^2\right],\ \forall Y\in\mathcal{H}.\tag{2.8}\label{eq:2.8}
\end{align*}
\begin{proof}
	The construction of Hilbert space $\mathcal{H}$ and $\mathcal{H}_\mathscr{G}$ follows from completeness of $L^p$-spaces. To prove that $\E[\cdot|\mathscr{G}]$ is the projection onto $\mathcal{H}_\mathscr{G}$, it suffices to show $\xi:=\E[Y|\mathscr{G}]$ is orthogonal to $Y-\xi$:
	\begin{align*}
		\langle \xi, Y-\xi\rangle = \E[\xi(Y-\xi)]=\E\left[\E[\xi(Y-\xi)|\mathscr{G}]\right]=\E\left[\xi\,\E[Y-\xi|\mathscr{G}]\right] = 0.
	\end{align*}
	The equation \hyperref[eq:2.8]{(2.8)} follows from the definition of projection.
\end{proof}

Similar to \hyperref[thm:2.13]{Theorem 2.13}, we also have the integral transform formula for conditional expectation.

\paragraph{Theorem 2.20\label{thm:2.20}} (Conditional integral transform). Let $(\Omega_1,\mathscr{F}_1,\P_1)$ and $(\Omega_2,\mathscr{F}_2,\P_2)$ be probability spaces, and let $T:(\Omega_1,\mathscr{F}_1)\to(\Omega_2,\mathscr{F}_2)$ be a measure-preserving transform, i.e. $\P_2=\P_1\circ T^{-1}$. If $\varphi\in L^1(\Omega_2,\mathscr{F}_2,\P_2)$, and $\mathscr{G}_2\subset\mathscr{F}_2$ is a sub $\sigma$-algebra, then
\begin{align*}
	\E_1[\varphi\circ T|T^{-1}\mathscr{G}_1]=\E_2[\varphi|\mathscr{G}_2]\circ T,\tag{2.7}\label{eq:2.7}
\end{align*}
where $\E_1$ and $\E_2$ are expectation operators on $(\Omega_1,\mathscr{F}_1,\P_1)$ and $(\Omega_2,\mathscr{F}_2,\P_2)$, respectively.
\begin{proof}
Let $\xi_2=\E_2[\varphi|\mathscr{G}_2]$. Then $\xi_1:=\xi_2\circ T$ is a $T^{-1}\mathscr{G}_2$-measurable function on $(\Omega_1,\mathscr{F}_1)$. By \hyperref[thm:1.49]{Theorem 1.49}, we have $\E_1[\psi\circ T]=\E_2[\psi]$ for all $\psi\in L^1(\Omega_2,\mathscr{F}_2,\P_2)$. For any $A_1\in T^{-1}\mathscr{G}_2$, we have $A_2:=TA_1\in\mathscr{G}_2$, and
\begin{align*}
	\E_1[\xi_1\cdot\mathds{1}_{A_1}] &= \E_1[(\xi_2\circ T)\cdot\mathds{1}_{A_1}] = \E_1[(\xi_2\circ T)\cdot(\mathds{1}_{A_2}\circ T)] = \E_1[(\xi_2\cdot\mathds{1}_{A_2})\circ T] =\E_2[\xi_2\cdot\mathds{1}_{A_2}]\\
	&= \E_2[\varphi\cdot\mathds{1}_{A_2}]
	=\E_1[(\varphi\cdot\mathds{1}_{A_2})\circ T] = \E_1[(\varphi\circ T)\cdot(\mathds{1}_{A_2}\circ T)] = \E_1[(\varphi\circ T)\cdot\mathds{1}_{A_1}]
\end{align*}
Since $A_1\in T^{-1}\mathscr{G}_2$ is arbitrary, we have $\E_1[\varphi\circ T|T^{-1}\mathscr{G}_1]=\xi_2\circ T\ a.s.$, which is \hyperref[eq:2.7]{(2.7)}.
\end{proof}

Now we introduce the conditional expectation with respect to a random variable.

\paragraph{Definition 2.21\label{def:2.21}} (Conditional expectation). Let $X$ and $Y$ be two random variables. If $\E\vert Y\vert <\infty$, the \textit{conditional expectation of $Y$ with respect to $X$} is defined as
\begin{align*}
	\E[Y|X]=\E[Y|\sigma(X)],
\end{align*}
where $\sigma(X)=X^{-1}(\mathscr{B}(\mathbb{R}))$ is the $\sigma$-algebra generated by $X$.

\paragraph{Remark.} In fact, if the conditional expectation $\E[Y|\sigma(X)]$ exists, it can be written as the composition of a measurable function $\varphi$ and the random variable $X$.

\paragraph{Theorem 2.22\label{thm:2.22}} (Doob-Dynkin). Let $(\Omega,\mathscr{F})$ and $(\Gamma,\mathscr{G})$ be two measurable spaces, Given a measurable function $T:\Omega\to\Gamma$, let $\sigma(T):=T^{-1}\mathscr{G}\subset\mathscr{F}$ be the $\sigma$-algebra generated by $T$. Then an extended real-valued function $g:\Omega\to\overline{\mathbb{R}}$ is $\sigma(T)$-measurable if and only if there exists a $\mathscr{G}$-measurable function $\varphi:\Gamma\to\overline{\mathbb{R}}$ such that $g=\varphi\circ T$. 

\begin{proof}
The sufficiency is clear, so we only prove the necessity. Assume $g$ is nonnegative and $\sigma(T)$-measurable. By simple function approximation, there exists $\{A_n\}_{n=1}^\infty\subset\sigma(T)$ and nonnegative numbers $\{\alpha_n\}_{n=1}^\infty$ such that
\begin{align*}
	g=\sum_{n=1}^\infty\alpha_n\mathds{1}_{A_n}.
\end{align*}
Since $A_n\in\sigma(T)$, there exists $B_n\in\mathscr{G}$ such that $A_n=T^{-1}B_n$, namely, $\mathds{1}_{A_n}=\mathds{1}_{B_n}\circ T$. Hence we define
\begin{align*}
	\varphi=\sum_{n=1}^\infty\alpha_n\mathds{1}_{B_n}\ \Rightarrow\ \varphi\circ T = g.
\end{align*}

For a general $\sigma(T)$-measurable function $g$, there exists $\mathscr{G}$-measurable function $\varphi^+,\varphi^-:\Gamma\to\overline{\mathbb{R}}$ such that $g^+=\varphi^+\circ T$ and $g^-=\varphi^-\circ T$. Let $E=\{\psi\in\Gamma:\varphi^+(\gamma)=\varphi^-(\gamma)=\infty\}\in\mathscr{G}$. Then $T^{-1}E=\emptyset$, since $\infty-\infty$ is undefined. Then $\varphi=\mathds{1}_{\Gamma\backslash E}(\varphi^+-\varphi^-)$ is the desired $\mathscr{G}$-measurable function.
\end{proof}

\paragraph{Remark.} By \hyperref[thm:2.22]{Theorem 2.22}, since $\E[Y|\sigma(X)]:(\Sigma,\mathscr{F})\to(\mathbb{R},\mathscr{B}(\mathbb{R}))$ is a $\sigma(X)$-measurable function, there exists a Borel-measurable function $\varphi:(\mathbb{R},\mathscr{B}(\mathbb{R}))\to(\mathbb{R},\mathscr{B}(\mathbb{R}))$ such that $\E[Y|\sigma(X)]=\varphi(X)$.

\paragraph{Theorem 2.23.\label{thm:2.23}} Let $X\sim\mu_X$ and $Y\sim\mu_Y$ be two independent random variables. If $\varphi:\mathbb{R}^2\to\mathbb{R}$ is measurable and $\varphi(X,Y)$ is integrable, then
\begin{align*}
	\E\left[\varphi(X,Y)|Y\right] = \E[\varphi(X,y)]\big|_{y=Y}:=\int\varphi(x,Y)\,\d \mu_X(x).\tag{2.9}\label{eq:2.9}
\end{align*}
\begin{proof}
Define $\psi(y)=\int\varphi(x,y)\,\d \mu_X(x),\ y\in\mathbb{R}$. For all $A\in\mathscr{B}(\mathbb{R})$, we have
\begin{align*}
	\E[\psi(Y)\mathds{1}_A(Y)] &= \int\psi(y)\mathds{1}_A(y)\,\d \mu_Y(y)\\
	&= \int\left(\int\varphi(x,y)\mathds{1}_A(y)\,\d \mu_X(x)\right)\,\d \mu_Y(y)\\
	&= \int\varphi(x,y)\mathds{1}_A(y)\,\d (\mu_X\otimes\mu_Y)(x,y) = \E\left[\varphi(X,Y)\mathds{1}_A(Y)\right].\tag{By Fubini's theorem}
\end{align*}
Hence we have $\E\left[\varphi(X,Y)|Y\right]=\psi(Y)$.
\end{proof}
\paragraph{Remark.} In fact, the equation \hyperref[eq:2.9]{(2.9)} has a more direct form:
\begin{align*}
	\E[\varphi(X,Y)|Y=y]=\E[\varphi(X,y)|Y=y] \overset{X\ind Y}{=} \E[\varphi(X,y)].
\end{align*}

Given a sub $\sigma$-algebra $\mathscr{G}$ of $\mathscr{F}$, we can define conditional probability $\mathbb{P}(\cdot|\mathscr{G})$ by conditional expectation: $\mathbb{P}(A|\mathscr{G})=\E[\mathds{1}_A|\mathscr{G}]$. This automatically induces a probability measure $\P(\cdot|\mathscr{G})(\omega)$ for each $\omega\in\Omega$.

\paragraph{Definition 2.24\label{def:2.24}} (Regular conditional probability). Let $\mathscr{G}$ be a sub $\sigma$-algebra of $\mathscr{F}$. A \textit{regular conditional probability} is a function $\P(\cdot|\mathscr{G})(\cdot):\mathscr{F}\times\Omega\to[0,1]$ satisfying the following:
\begin{itemize}
	\item[(i)] For $\P\textit{-a.e.}$ $\omega\in\Omega$,
	\begin{align*}
		\P(B|\mathscr{G})(\omega)=\E\left[\mathds{1}_B|\mathscr{G}\right](\omega),\ \forall B\in\mathscr{B}(\mathbb{R});
	\end{align*}
	\item[(ii)] For $\P\textit{-a.e.}$ $\omega\in\Omega$, $\P(\cdot|\mathscr{G})(\omega)$ is a probability measure on $(\Omega,\mathscr{F})$.
\end{itemize}
\paragraph{Definition 2.25\label{def:2.25}} (Regular conditional distribution). Let $\mathscr{G}$ be a sub $\sigma$-algebra of $\mathscr{F}$. Let $X\sim\mu_X$ be a random variable, where $\mu_X$ is a distribution measure. We define $\mu_{X|\mathscr{G}}(\cdot|\mathscr{G}):\mathscr{B}(\mathbb{R})\times\Omega\to[0,1]$ as follows:
\begin{align*}
	\mu_{X|\mathscr{G}}(B|\mathscr{G})(\omega) = \P(X^{-1}(B)|\mathscr{G})(\omega),\ \forall B\in\mathscr{B}(\mathbb{R}).
\end{align*}

The function $\mu_{X|\mathscr{G}}(\cdot|\mathscr{G}):\mathscr{B}(\mathbb{R})\times\Omega\to[0,1]$ is called a \textit{regular conditional distribution of $Y$ given $\mathscr{G}$} if (i) $\omega\mapsto\mu_{X|\mathscr{G}}(B|\mathscr{G})(\omega)$ is $\mathscr{F}$-measurable for all $B\in\mathscr{B}(\mathbb{R})$, and (ii) $\mu_{X|\mathscr{G}}(\cdot|\mathscr{G})(\omega)$ is a probability measure on $\mathscr{B}(\mathbb{R})$ for $\P\textit{-a.e.}$ $\omega\in\Omega$. Moreover, for all $A\in\mathscr{G}$ and $B\in\mathscr{B}(\mathbb{R})$, we have
\begin{align*}
	\P(\omega\in A,X\in B)=\int_A \mu_{X|\mathscr{G}}(B|\mathscr{G})\,\d \P.
\end{align*}

\paragraph{Theorem 2.26.\label{thm:2.26}} (Existence of regular conditional distribution). Let $\mathscr{G}$ be a sub $\sigma$-algebra of $\mathscr{F}$. Then every random variable $X$ has a regular conditional distribution $\mu_{X|\mathscr{G}}(\cdot|\mathscr{G})$ given $\mathscr{G}$.
\begin{proof}
For all rationals $r\in\mathbb{Q}$, define
\begin{align*}
	F(r|\cdot) = \P(X\in(-\infty,r]|\mathscr{G}):=\E[\mathds{1}_{\left\{\omega\in\Omega:X(\omega)\in(-\infty,r]\right\}}|\mathscr{G}].
\end{align*}

Clearly, for $r\leq s$, we have $\mathds{1}_{\left\{X\in(-\infty,r]\right\}}\leq\mathds{1}_{\left\{X\in(-\infty,s]\right\}}$. By monotonicity of conditional expectation, we have $F(r|\cdot)\leq F(s|\cdot)$ for $\P\textit{-a.e.}$ $\omega\in\Omega$. Denote $A_{r,s}=\left\{\omega\in\Omega:F(r|\omega)> F(s|\omega)\right\}$, so $\P(A_{r,s})=0$. Moreover, by dominated convergence theorem (\hyperref[thm:2.18]{Theorem 2.18}), there exist null sets $\{B_r\}_{r\in\mathbb{Q}}\subset\mathscr{F}$ and $C\in\mathscr{F}$ such that
\begin{align*}
	\lim_{n\to\infty} F\left(\left.r+\frac{1}{n}\right|\omega\right) = F(r|\omega),\ \forall\omega\in\Omega\backslash B_r
\end{align*}
as well as
\begin{align*}
	\inf_{r\in\mathbb{Q}}F(r|\omega) = 0\quad\textit{and}\quad\sup_{r\in\mathbb{Q}}F(r|\omega)=1,\ \forall\ \omega\in\Omega\backslash C.
\end{align*}

Let $E=\left(\bigcup_{r,s\in\mathbb{Q}:r<s}A_{r,s}\right)\cap\left(\bigcup_{r\in\mathbb{Q}}B_r\right)\cup C$. Then $\mu(N)=0$. For $\omega\in\Omega\backslash E$, define
\begin{align*}
	\widetilde{F}(x|\omega):=\inf_{r\in\mathbb{Q}:\,r>z} F(r|\omega),\ \forall x\in\mathbb{R}.
\end{align*}

Since $F(\cdot|\omega)$ is monotone increasing on $\mathbb{Q}$, $\widetilde{F}(\omega)|_\mathbb{Q}=F(\cdot|\omega)$. By construction, $F(\cdot|\omega):\mathbb{R}\to[0,1]$ is a c.d.f., and we can extend this to a unique probability measure $\mu_{X|\mathscr{G}}(\cdot|\mathscr{G})(\omega)$ on $\mathscr{B}(\mathbb{R})$ by Carathéodory's extension theorem. Hence $\mu_{X|\mathscr{G}}(\cdot|\mathscr{G})(\omega)$ is a Borel probability measure for $\P\textit{-a.e.}$ $\omega\in\Omega$.

For $\omega\in E$, define $\widetilde{F}(\cdot|\omega)=F_0$, where $F_0$ is an arbitrary but fixed c.d.f.. Then for $r\in\mathbb{Q}$ and $B=(-\infty,r]$,
\begin{align*}
	\omega\mapsto \mu_{X|\mathscr{G}}(B|\mathscr{G})(\omega) = \mathds{1}_E(\omega)F_0(r) + \mathds{1}_{\Omega\backslash E}(\omega)F(r|\omega)
\end{align*}
is $\mathscr{F}$-measurable, since $F(r|\omega)$ is $\mathscr{G}$-measurable by definition. We define
\begin{align*}
	\mathscr{D}=\left\{B\in\mathscr{B}(\mathbb{R}):\omega\mapsto\mu_{X|\mathscr{G}}(B|\mathscr{G})(\omega)\ \textit{is $\mathscr{F}$-measurable}\right\}.
\end{align*}
This is a $\lambda$-system, because
\begin{itemize}
	\item[(i)] $\mu_{X|\mathscr{G}}(\Omega|\mathscr{G})(\omega)\equiv 1$, which implies $\Omega\in\mathscr{D}$;
	\item[(ii)] For $E\subset F\in\mathscr{D}$, $\mu_{X|\mathscr{G}}(F\backslash E|\mathscr{G})(\omega) = \mu_{X|\mathscr{G}}(F|\mathscr{G})(\omega) - \mu_{X|\mathscr{G}}(E|\mathscr{G})(\omega)$, hence $F\backslash E\in\mathscr{D}$;
	\item[(iii)] For increasing sequence $B_n\in\mathscr{D}$, $\mu_{X|\mathscr{G}}(B|\mathscr{G})(\omega) = \lim_{n\to\infty}\mu_{X|\mathscr{G}}(B_n|\mathscr{G})(\omega)$, hence $B:=\bigcup_{n=1}^\infty B_n\in\mathscr{D}$.
\end{itemize}

Note that $\{(-\infty,r],r\in\mathbb{Q}\}$ is a $\pi$-system generating $\mathscr{B}(\mathbb{R})$. By Sierpinski-Dynkin $\pi$-$\lambda$ theorem, we have $\mathscr{D}=\mathscr{B}(\mathbb{R})$. Then $\mu_{X|\mathscr{G}}(B|\mathscr{G})(\cdot)$ is $\mathscr{F}$-measurable for all $B\in\mathscr{B}(\mathbb{R})$. Furthermore, for all $A\in\mathscr{G}$ and $B\in\mathscr{B}(\mathbb{R})$,
\begin{align*}
	\int_A\mu_{X|\mathscr{G}}(B|\mathscr{G})\,\d \P = \E\left[\mathds{1}_A\E[\mathds{1}_{\{X\in B\}}|\mathscr{G}]\right]=\E[\mathds{1}_{A\cap\{X\in B\}}] = \P(\omega\in A, X\in B).
\end{align*}
Thus we complete the full proof.
\end{proof}
\paragraph{Remark.} If $X$ is independent of $\mathscr{G}$, the conditional distribution $\mu_{X|\mathscr{G}}(\cdot|\mathscr{G})(\omega)$ is the same as the unconditional distribution $\mu_X$ for $\P\textit{-a.e.}$ $\omega\in\Omega$.

\paragraph{Definition 2.27\label{def:2.27}} (Conditional distribution). Let $X\sim\mu_X$ and $Y\sim\mu_Y$ be two random variables, where $\mu_X$ and $\mu_Y$ are distribution measures. If there exists a collection of probability measures $\{\mu_{Y|X}(\cdot|x)\}_{x\in\mathbb{R}}$ such that for $\mu_X\textit{-a.e.}$ $x$ and all Borel sets $A$ and $B$,
\begin{align*}
	\P(X\in A,Y\in B) = \int_A \mu_Y(B|x)\,\d \mu_X(x),
\end{align*}
then $\{\mu_Y(\cdot|x)\}_{x\in\mathbb{R}}$ is called the \textit{conditional distribution of $Y$ with respect to $X$}, and we write $Y|X\sim\mu_{Y|X}$. Furthermore, if there exists two-variable measurable function $(x,y)\mapsto F_{Y|X}(y|x)$ such that $F(\cdot|x)$ is a c.d.f. for any fixed $x$, and
\begin{align*}
	\P(X\leq x,Y\leq y)=\int_{(-\infty,x]} F_{Y|X}(y|t)\,\d \mu_X(t),
\end{align*}
then $\{F_{Y|X}(\cdot|x)\}_{x\in\mathbb{R}}$ is called the \textit{family of conditional distribution functions of $Y$ with respect to $X$}. 

\newpage
\subsection{Stochastic Convergence}
\paragraph{Review: Convergence of Random Variables.} Here are several categories of convergence of measurable functions we covered in \hyperref[sec:1.6]{\textit{Section 1.6}}. We summarized them in random variable version. Let $X_n:(\Omega,\mathscr{F})\to(\mathscr{B}(\mathbb{R}))$ be a sequence of random variables. Let $X$ be a random variable.
\begin{itemize}
\item[(i)] (Almost sure convergence). $X_n$ is said to converge almost surely to $X$ if
\begin{align*}
	\P\left(\lim_{n\to\infty} X_n=X\right)=1.
\end{align*}
\item[(ii)] (Convergence in Probability). $X_n$ is said to converge in probability to $X$ if for all $\eta>0$,
\begin{align*}
	\lim_{n\to\infty}\P\left(\vert X_n - X\vert\geq\eta\right)= 0.
\end{align*}
\item[(iii)] (Convergence in $L^p$-norm). Let $1\leq p<\infty$. $X_n$ is said to converge to $X$ in $L^p$-norm if
\begin{align*}
	\lim_{n\to\infty}\E\left[\vert X_n-X\vert^p\right] = 0.
\end{align*}
\end{itemize}

All these modes of convergence can be generalized to the case of random vectors by giving $\mathbb{R}^p$ a proper metric, e.g. the Euclidean distance and $L^p$-distance.

Since a probability space $(\Omega,\mathscr{F},\P)$ is a finite measure space, almost sure convergence implies convergence in probability. Also, by Chebyshev inequality, convergence in $L^p$-norm implies convergence in probability. Moreover, if $X_n$ is a uniformly integrable sequence that converges to $X$ in probability, it also converges to $X$ in $L^1$-norm. Now we introduce another convergence of random variables.

\paragraph{Definition 2.28\label{def:2.28}} (Convergence in distribution). A sequence of random variables $X_n\sim F_n$ is said to \textit{converges in distribution} to a random variable $X\sim F$ if
\begin{align*}
	\lim_{n\to\infty}F_n(x) = F(x)\ \ \Leftrightarrow\ \ \lim_{n\to\infty}\P(X_n\leq x) = \P(X\leq x)
\end{align*}
for each points $x$ of continuity of $F$, and we write $X\overset{d}{\to} X_n$. We also say that the sequence of cumulative distribution functions $F_n$ \textit{converges weakly to $F$}, and write $F_n\overset{w}{\to} F$.

\paragraph{Remark.} In fact, a sequence of random variables converges in distribution if and only if their distribution measures converges weakly. This is used an alternative definition of convergence in distribution.

\paragraph{Theorem 2.29\label{thm:2.29}} (Portmanteau lemma). Let $X_n\sim F_n$ be a sequence of random variables, and $X\sim F$. Then $X_n\overset{d}{\to} X$ if and only if the following equivalent conditions hold:
\begin{itemize}
	\item[(i)] $\E[f(X_n)]\to\E[f(X)]$ for all bounded continuous functions $f$;
	\item[(ii)] $\E[f(X_n)]\to\E[f(X)]$ for all bounded Lipschitz continuous functions $f$;
	\item[(iii)] $\liminf_{n\to\infty}\E[f(X_n)]\geq\E[f(X)]$ for all lower semi-continuous function $f$ bounded from below;
	\item[(iv)] $\limsup_{n\to\infty}\E[f(X_n)]\leq\E[f(X)]$ for all upper semi-continuous function $f$ bounded from above;
	\item[(v)] $\liminf_{n\to\infty}\P(X_n\in G)\geq\P(X\in G)$ for every open sets $G$;
	\item[(vi)] $\limsup_{n\to\infty}\P(X_n\in F)\leq\P(X\in F)$ for every closed sets $F$;
	\item[(vii)] $\lim_{n\to\infty}\P(X_n\in B)\to\P(X\in B)$ for all continuity sets $B$, i.e. $\P(X\in\partial B)=0$. 
\end{itemize}
\begin{proof}
$\Rightarrow$ (i): Without loss of generality, we let bounded continuous function $f$ take values in $[-1,1]$. Assume that $F$ is continuous. By $X_n\overset{d}{\to}X$, we have $\lim_{n\to\infty}\P(X_n\in I)=\P(X\in I)$ for all closed intervals $I$ on $\mathbb{R}$. Given $\epsilon>0$, choose a sufficiently large $I$ so that $\P(X\notin I)<\epsilon/5$. Since $f$ is uniformly continuous on the compact set $I$, we choose a partition $I=\bigcup_{j=1}^k I_j$ such that $f$ varies at most $\epsilon/5$ on each $I_j$. Take a point $x_j$ from each $I_j$, and define $\varphi=f(x_j)\mathds{1}_{I_j}$, then $\varphi$ is a simple function, and
\begin{align*}
	\left\vert\E[f(X_n)]-\E[\varphi(X_n)]\right\vert =\frac{\epsilon}{5}+\P(X_n\notin I),\quad \left\vert \E[f(X)]-\E[\varphi(X)]\right\vert =\frac{\epsilon}{5}+\P(X\notin I)\leq\frac{2\epsilon}{5}.\tag{2.10}\label{eq:2.10}
\end{align*}

Since $\lim_{n\to\infty}\P(X_n\in I)=\P(X\in I)$ and $\P(X\notin I)<\epsilon/5$, there exists $N_0$ such that $\P(X_n\notin I)<\epsilon/5$ for all $n\geq N_0$. Note that 
\begin{align*}
	\left\vert\E[\varphi(X_n)]-\E[\varphi(X)]\right\vert\leq\sum_{j=1}^k\left\vert\P(X_n\in I_j)-\P(X\in I_j)\right\vert\left\vert f(x_j)\right\vert.\tag{2.11}\label{eq:2.11}
\end{align*}
We also choose $N_1$ such that $\left\vert\P(X_n\in I_j)-\P(X\in I_j)\right\vert<\epsilon/(5k)$ for all $I_j$ and all $n\geq N_1$. Combine \hyperref[eq:2.10]{(2.10)} with \hyperref[eq:2.11]{(2.11)} and use triangle inequality, then $\left\vert\E[f(X_n)]-\E[f(X)]\right\vert < \epsilon$ for all $n\geq\max\{N_0,N_1\}$. Since $\epsilon>0$ is arbitrary, (ii) holds for all continuous c.d.f. $F$.

If $F:\mathbb{R}\to[0,1]$ is not continuous everywhere, we use rarity of discontinuity sets. The collection of sets $\{(-\infty,\alpha]:\alpha\in\mathbb{R}\}$ has disjoint boundaries, and at most countably many of them are discontinuity sets, say $\P(X=\alpha)>0$. As a result, there exists a dense subset $D\subset\mathbb{R}$ such that $F$ is continuous at each $\alpha\in D$. We choose closed intervals $I$ with boundaries on $D$.

(vii) $\Rightarrow$: For each point $x$ of continuity of $F$, choose $B=(-\infty,x]$.
\end{proof}

\paragraph{Remark.} This theorem can be easily extended to the case of random vectors.

\paragraph{} Following is an extremely useful theorem in analysis. It asserts that a continuous mapping preserves several modes of stochastic convergence of random variable sequences. The case of convergence in distribution applies Portmanteau lemma. 

\paragraph{Theorem 2.30\label{thm:2.30}} (Continuous mapping). Let $X$ be a random variable. If $g:\mathbb{R}\to\mathbb{R}$ is continuous everywhere on a set $C$ such that $\P(X\in C)=1$, then $g$ preserves the following modes of convergence:
\begin{itemize}
	\item[(i)] If $X_n\overset{a.s.}{\to} X$, then $g(X_n)\overset{a.s.}{\to} g(X)$; 
	\item[(ii)] If $X_n\overset{\P}{\to}X$, then $g(X_n)\overset{\P}{\to}g(X)$; 
	\item[(iii)] If $X_n\overset{d}{\to}X$, then $g(X_n)\overset{d}{\to}g(X)$.
\end{itemize}

\begin{proof}
(i) is trivial. (ii) Given $\eta>0$, define
\begin{align*}
	E_k=\left\{x\in C:\exists y\in\mathbb{R}\ \textit{such that}\ \vert y-x\vert<\frac{1}{k}\ \textit{and}\ \vert g(y)-g(x)\vert\geq\eta\right\},\ k\in\mathbb{N}.
\end{align*}
Since $g$ is continuous everywhere on $C$, the sequence $E_k\searrow\emptyset$. Then
\begin{align*}
	\P\left(\vert g(X_n)-g(X)\vert\geq\eta\right) \leq \P(X\in E_k)+\P\left(\vert X_n - X\vert\geq\frac{1}{k}\right)\tag{2.12}\label{eq:2.12}
\end{align*}
Given $\epsilon>0$, we first choose $K$ such that $\P(X\in E_K)\leq\epsilon/2$, then choose $N$ such that $\P(\vert X_n-X\vert\geq 1/K)<\epsilon/2$ for all $n\geq N$. Hence \hyperref[eq:2.12]{(2.12)} is controlled by arbitrarily small $\epsilon>0$.

(iii) Let $F\subset\mathbb{R}$ be a closed set. If $x\in\overline{g^{-1}(F)}$, there exists sequence $x_k\in g^{-1}(F)$ such that $x_k\to x$ and $g(x_k)\in F$. Since $F$ is closed and $g$ is continuous on $C$, if $x\in C$, we have $g(x)\in F$. Hence the following inclusions hold for all closed set $F$:
\begin{align*}
	g^{-1}(F)\subset\overline{g^{-1}(F)}\subset g^{-1}(F)\cup C^c.
\end{align*}

Using the Portmanteau lemma, we have
\begin{align*}
	\limsup_{n\to\infty}\P(g(X_n)\in F)\leq\limsup_{n\to\infty}\P\left(X_n\in \overline{g^{-1}(F)}\right)
	\leq\P\left(X\in\overline{g^{-1}(F)}\right)\leq P(g(X)\in F),
\end{align*}
where the last inequality holds because $\P(X\in C)=0$. Again by Portmanteau lemma, $g(X_n)\overset{d}{\to}g(X)$.
\end{proof}

Another important property associated with weak convergence is uniform tightness. Any random variable $X$ is \textit{tight} or $O_p(1)$, i.e. for each $\epsilon>0$, there exists $M>0$ such that $\P(\vert X\vert>M) < \epsilon$. This is a consequence of the properties of c.d.f. $F_X(x)=\P(X\leq x)$.
\paragraph{Definition 2.31.\label{def:2.31}} (Uniform tightness). A collection of random variables $\{X_\alpha,\alpha\in J\}$ is said to be \textit{uniformly tight}, if for every $\epsilon>0$, there exists a constant $M>0$ such that
\begin{align*}
	\sup_{\alpha\in J}\P\left(\vert X_\alpha\vert > M\right)<\epsilon.
\end{align*}
Clearly, any finite collection of random variable is uniformly tight.

\paragraph{} The following Prohorov's theorem is a generalization of Heine-Borel theorem.
\paragraph{Theorem 2.32\label{thm:2.32}} (Prohorov). Let $X_n$ be a sequence of random variables.
\begin{itemize}
	\item[(i)] If $X_n\overset{d}{\to}X$ for some random variable $X$, then $\{X_n:n\in\mathbb{N}\}$ is uniformly tight;
	\item[(ii)] If $\{X_n:n\in\mathbb{N}\}$ is uniformly tight, then there exists a subsequence $X_{n_k}$ that converges in distribution to some random variable $X$.
\end{itemize}
\renewcommand{\proofname}{Proof of \hyperref[thm:2.32]{Theorem 2.32 (i)}}
\begin{proof}
Given $\epsilon>0$, we choose $M_0$ such that $\P(\vert X\vert > M_0)<\epsilon/2$. By Portmanteau's theorem, we have $\limsup_{n\to\infty}\P(\vert X_n\vert >M_0)\leq\P(\vert X\vert>M_0)$. Hence we can choose $N$ such that
\begin{align*}
	\sup_{n\geq N}\P(\vert X_n\vert>M_0)<\P(\vert X\vert >M_0) + \frac{\epsilon}{2} < \epsilon.
\end{align*}

Note that any finite collection of random variables is uniformly tight. Then we choose $M_1$ such that $\P(\vert X_j\vert > M_1)<\epsilon$ for all $j=1,\cdots,N-1$. Let $M=\max\{M_0,M_1\}$, then $\sup_{n\in\mathbb{N}}\P(\vert X_n\vert> M) <\epsilon$.
\end{proof}
\renewcommand{\proofname}{Proof}

The proof of \hyperref[thm:2.32]{Theorem 2.32} (ii) uses Helly's selection theorem.

\paragraph{Theorem 2.33\label{thm:2.33}} (Helly's selection theorem). Let $f_n:\mathbb{R}\to[-M,M]$ be a uniformly bounded sequence of monotone increasing functions. Then there exists a subsequence $(f_{n_k})_{k=1}^\infty$ that converges pointwise to an monotone increasing function $f:\mathbb{R}\to[-M,M]$.
\begin{proof}
Choose a countable dense subset $\mathbb{Q}=\{r_k,k\in{\mathbb{N}}\}$ of $\mathbb{R}$. Then $f_n(r_1)$ is a bounded sequence. By Bolzano-Weierstrass theorem, choose a convergent subsequence $f_{1n}(r_1)\to f(r_1)$. Then $f_{1n}(r_2)$ is a bounded sequence, and again we choose one of its convergent subsequence $f_{2n}(r_2)\to f(r_2)$.

\textit{``diagonal trick'':} Repeat this procedure, so for each $k\in\mathbb{N}$, we choose a subsequence $f_{kn}$ such that $f_{kn}(r_j)\to f(r_j)$ for all indices $j\leq k$. Since $(f_{kn})_{n=1}^\infty$ is a subsequence of its predecessor $(f_{k-1,n})_{n=1}^\infty$, $(f_{nn}(r_k))_{n=1}^\infty$ is a subsequence of $(f_{kn}(r_k))_{n=1}^\infty$ from $n=k$ on, and we have $\lim_{n\to\infty}f_{nn}(r_k)= f(r_k)$ for all $k\in\mathbb{N}$. Hence we obtain a subsequence $f_{nn}$ that converges to $f$ pointwise on $\mathbb{Q}$. Clearly, $f:\mathbb{Q}\to[-M,M]$ is increasing.

For all irrationals $x\in\mathbb{R}\backslash\mathbb{Q}$, choose a increasing rational sequence $r_{k_j}\to x$, and let $f(x)=\lim_{j\to\infty}f(r_{k_j})$. Note this limit exists because $f(r_{k_j})$ is a bounded increasing sequence. Clearly, $f$ is increasing on $\mathbb{R}$ and bounded by $M$, and $r_{k_i} < x < r_{k_j}$ implies $f_{nn}(r_{k_i})-f(r_{k_j}) < f_{nn}(x)-f(x) < f_{nn}(r_{k_j})-f(r_{k_i})$ for all $n\in\mathbb{N}$.

Finally we prove $f_{nn}\to f$ pointwise on $\mathbb{R}$. Given $\epsilon>0$. If $x\in\mathbb{R}\backslash\mathbb{Q}$ is a point of continuity of $f$, we choose rationals $r< x < r^\prime$ with $\vert f(r_{k_i})-f(r_{k_j})\vert<\epsilon$. Then
\begin{align*}
	-\epsilon \leq \liminf_{n\to\infty} \left(f_{nn}(r)-f(r^\prime)\right) \leq f_{nn}(x)-f(x) \leq \limsup_{n\to\infty}\left(f_{nn}(r^\prime)-f(r)\right) < \epsilon.
\end{align*}

Hence $f_{nn}$ converges pointwise to $f$, except possibly at points of discontinuity of $f$. Being monotone increasing, $f$ has at most countably points of discontinuity. Since $f_{nn}$ is uniformly bounded by $M$, we repeat the ``diagonal'' trick to obtain a subsequence of $f_n$ that converges everywhere on $\mathbb{R}$.
\end{proof}

\paragraph{Corollary 2.34\label{cor:2.34}} (Helly's selection theorem). Let $F_n:\mathbb{R}\to[0,1]$ be a sequence of cumulative distribution functions. Then there exists a subsequence $F_{n_k}$ such that $F_{n_k}(x)\to F(x)$ at each point $x$ of continuity of a possibly defective distribution function $F$, i.e. $F$ only satisfies properties (i) and (ii) in \hyperref[prop:2.2]{Proposition 2.2}.
\begin{proof}
By \hyperref[thm:2.33]{Theorem 2.33}, we choose a subsequence $F_{n_k}$ of $F_n$ that converges pointwise to an increasing function $G:\mathbb{R}\to[0,1]$. Define $F(x)=\lim_{\epsilon\to 0^+}G(x+\epsilon)$ for all $x\in\mathbb{R}$. Then $F$ is right-continuous on $\mathbb{R}$, and $F_{n_k}$ converges to $F$ at all points of continuity of $F$.
\end{proof}
\renewcommand{\proofname}{Proof of \hyperref[thm:2.32]{Theorem 2.32 (ii)}}
\begin{proof}
Let $X_n\sim F_n$. By Helly's selection theorem, there exists a subsequence $F_{n_k}$ of the c.d.f. sequence $F_n$ that converges to a possibly defective distribution function $F$. It suffices to show that $F$ is proper. That is, $\lim_{x\to-\infty} F(x)=0$ and $\lim_{x\to\infty} F(x)=1$. 

Given $\epsilon>0$, by uniform tightness of $\{X_n,n\in\mathbb{N}\}$, we choose $M>0$ such that $F(M)>1-\epsilon$. Since the points of discontinuity of $F$ are rare, we slide $M$ slightly larger so that $M$ is a point of continuity of $F$. Then $F_{n_k}(M)\to F(M)>1-\epsilon$. Since $\epsilon>0$ is arbitrary, $F(x)\to 1$ as $x\to\infty$. The case $x\to-\infty$ is similar.
\end{proof}
\renewcommand{\proofname}{Proof}

Now we discuss the relationship between convergence in probability and convergence in distribution.

\paragraph{Theorem 2.35.\label{thm:2.35}} Let $X_n$, $X$, $Y_n$ and $Y$ be random variables. Let $c\in\mathbb{R}$ be a constant. Then
\begin{itemize}
	\item[(i)] If $X_n\overset{\P}{\to}X$ and $Y_n\overset{\P}{\to}Y$, then $(X_n,Y_n)\overset{\P}{\to}(X,Y)$;
	\item[(ii)] If $X_n\overset{d}{\to}X$ and $\vert X_n-Y_n\vert\overset{\P}{\to}0$, then $Y_n\overset{d}{\to}X$;
	\item[(iii)] If $X_n\overset{\P}{\to}X$, then $X_n\overset{d}{\to}X$;
	\item[(iv)] $X_n\overset{\P}{\to}c$ if and only if $X_n\overset{d}{\to}c$;
	\item[(v)] If $X_n\overset{d}{\to}X$ and $Y_n\overset{d}{\to}c$, then $(X_n,Y_n)\overset{d}{\to}(X,c)$;
\end{itemize}
\begin{proof}
(i) The result follows since $\rho((x_n,y_n),(x,y)):=\sqrt{\vert x-x_n\vert^2 + \vert y-y_n\vert^2}\leq\vert x-x_n\vert + \vert y-y_n\vert$.

(ii) For every bounded $1$-Lipschitz continuous function $f:\mathbb{R}\to[0,1]$, we have
\begin{align*} \left\vert\E[f(X_n)]-\E[f(Y_n)]\right\vert\leq\epsilon\E[\mathds{1}_{\{\vert X_n-Y_n\vert\leq\epsilon\}}] + \E[\mathds{1}_{\{\vert X_n-Y_n\vert>\epsilon\}}]\leq\epsilon+\P(\vert X_n-Y_n\vert>\epsilon),\ \forall\epsilon>0.
\end{align*}
Since $\epsilon>0$ is arbitrary, and $\P(\vert X_n-Y_n\vert>\epsilon)$ converges to zero, we have $\E[f(X_n)]-\E[f(Y_n)]\to 0$. By Portmanteau lemma, $\E[f(Y_n)]\to\E[f(X)]$, and $Y_n\overset{d}{\to} X$.

(iii) Since $X\overset{d}{\to}X$ trivially, this is a special case of (ii).

(iv) The ``only if'' case is a special case of (iii). For the converse, given any $\epsilon>0$, by Portmanteau lemma, $X_n\overset{d}{\to}c$ implies $X_n\overset{\P}{\to}c$:
\begin{align*}
	\limsup_{n\to\infty}\P(\vert X_n - c\vert\geq\epsilon)=\limsup_{n\to\infty}\P(X_n\in\mathbb{R}\backslash(c-\epsilon,c+\epsilon))\leq\P(c\in\mathbb{R}\backslash(c-\epsilon,c+\epsilon)) = 0.
\end{align*}

(v) Since $\rho((X_n,Y_n),(X_n,c))=\vert Y_n - c\vert\overset{\P}{\to} 0$, by (ii), it suffices to show that $(X_n,c)\overset{d}{\to}(X,c)$. For every $f\in C_b(\mathbb{R}^2)$, the mapping $x\mapsto f(x,c)$ is also bounded and continuous. By Portmanteau lemma, we have $\E[f(X_n,c)]\to\E[f(X,c)]$. Thus $(X_n,c)\overset{d}{\to}(X,c)$, and the result follows.
\end{proof}

We have the following useful corollary.
\paragraph{Lemma 2.36\label{lemma:2.36}} (Slutsky). Let $X_n$, $X$ and $Y_n$ be random variables. If $X_n\overset{d}{\to}X$ and $Y_n\overset{d}{\to}c\in\mathbb{R}$, then
\begin{itemize}
	\item[(i)] $X_n+Y_n\overset{d}{\to}X+c$;
	\item[(ii)] $X_nY_n\overset{d}{\to}cX$;
	\item[(iii)] If $c\neq 0$, then $Y_n^{-1}X_n\overset{d}{\to}c^{-1}X$.
\end{itemize}
\begin{proof}
By \hyperref[thm:2.35]{Theorem 2.35 (v)} and continuous mapping theorem [\hyperref[thm:2.30]{Theorem 2.30 (iii)}].
\end{proof}

Finally we introduce small $o$ and big $O$ symbols.
\paragraph{Definition 2.37\label{def:2.37}} (Stochastic $o$ and $O$ symbols). The notation $o_\P(1)$ denotes a sequence of random variables that converges to $0$ in probability. The notation $O_\P(1)$ denotes a sequence of random variables that is uniformly tight.  More generally, given a sequence of random variables $R_n$,
\begin{align*}
	X_n=o_\P(R_n)\ \ &\Leftrightarrow\ \ X_n=Y_nR_n\ \ \textit{and}\ \ Y_n\overset{\P}{\to} 0;\\
	X_n=O_\P(R_n)\ \ &\Leftrightarrow\ \ X_n=Y_nR_n\ \ \textit{and}\ \ Y_n=O_\P(1).
\end{align*}
\paragraph{Theorem 2.38\label{thm:2.38}} (Calculus with $o$ and $O$ symbols).
\begin{itemize}
	\item[(i)] $o_\P(1)+o_\P(1)=o_\P(1)$;
	\item[(ii)] $o_\P(1)+O_\P(1)=O_\P(1)$;
	\item[(iii)] $O_\P(1)o_\P(1)=o_\P(1)$;
	\item[(iv)] $(1+o_\P(1))^{-1}=O_\P(1)$;
	\item[(v)] $o_\P(R_n)=R_no_\P(1)$, $O_\P(R_n)=R_nO_\P(1)$;
	\item[(vi)] $o_\P(O_\P(1))=o_\P(1)$;
\end{itemize}
\begin{proof}
(i), (v) follows from definition. 

(ii) Let $X_n=o_\P(1)$ and $Y_n=O_\P(1)$. Given $\epsilon>0$, choose $M$ such that $\P(\vert Y_n\vert >M/2)<\epsilon/2$ for all $n\in\mathbb{N}$, and choose $N$ such that $\P(\vert X_n\vert > M/2)<\epsilon/2$ for all $n\geq N$. Then $\P(\vert X_n+Y_n\vert>M)<\epsilon$ for all $n\geq N$. Since $(X_n+Y_n)_{n=N}^\infty$ is uniformly tight, so is $(X_n+Y_n)_{n=1}^\infty$.

(iii) Let $X_n=o_\P(1)$ and $Y_n=O_\P(1)$. Given $\epsilon>0$, choose $M$ such that $\P(\vert Y_n\vert>M)<\epsilon/2$. Given $\eta>0$, choose $N$ such that $\P(\vert X_n\vert>\eta/M)<\epsilon/2$ for all $n\geq N$. Then $\P(\vert X_nY_n\vert>\eta)<\epsilon$ for all $n\geq N$.

(iv) Let $X_n=o_\P(1)$. For any $\epsilon>0$, there exists $0<\eta<1$ and $N>0$ such that $\P(\vert X_n\vert>\eta)<\epsilon$. Then $\P((1+X_n)^{-1}>\frac{1}{1-\eta})<\epsilon$. As a result, $((1+X_n)^{-1})_{n=N}^\infty$ is uniformly tight, and so is $((1+X_n)^{-1})_{n=1}^\infty$.

(vi) Follows from (iii) and (v).
\end{proof}

\paragraph{Theorem 2.39.\label{thm:2.39}} Let $R:\mathbb{R}\to\mathbb{R}$ be a function such that $R(0)=0$. Let $X_n=o_\P(1)$. Then for every $p>0$,
\begin{itemize}
	\item[(i)] If $R(h)=o(\vert h\vert^p)$ as $h\to 0$, then $R(X_n)=o_\P(\vert X_n\vert^p)$;
	\item[(ii)] If $R(h)=O(\vert h\vert^p)$ as $h\to 0$, then $R(X_n)=O_\P(\vert X_n\vert^p)$.
\end{itemize}
\begin{proof}
Define $g$ as $g(h)=\vert h\vert^{-p}R(h)$ for $h\neq 0$ and $g(0)=0$. Then $R(X_n)=\vert X_n\vert^p g(X_n)$.

(i) By assumption, $g$ is continuous at $0$. Then $g(X_n)\overset{\P}{\to}g(0)=0$ by continuous mapping theorem.

(ii) By assumption, there exists $\delta>0$ and $M>0$ such that $\vert g(h)\vert\leq M$ for all $\vert h\vert\leq\delta$. Then we have $\P(\vert g(X_n)\vert>M)\leq\P(\vert X_n\vert>\delta)\to 0$, and the sequence $g(X_n)$ is uniformly tight.
\end{proof}

\newpage
\subsection{Characteristic Functions}
\paragraph{Definition 2.40.\label{def:2.40}} Let $X\sim\mu$ be a (real-valued) random variable, where $\mu$ is a distribution measure. The \textit{characteristic function of $X$} is defined as
\begin{align*}
	\phi_X:\mathbb{R}\to\mathbb{C},\ \phi_X(\lambda)=\E\left[\e^{\i\lambda X}\right] = \int_\mathbb{R}\e^{\i \lambda x}\,\d \mu(x),\ \i^2=-1.
\end{align*}

\paragraph{Proposition 2.41\label{def:2.41}} (Properties of characteristic functions). If $\phi_X$ is the characteristic function of a random variable $X\sim\mu$, the following are true:
\begin{itemize}
	\item[(i)] $\phi_X(0)=1$;
	\item[(ii)] $\phi_X:\mathbb{R}\to\mathbb{C}$ is bounded and uniformly continuous.
	\item[(iii)] If $\E\left[\vert X\vert^n\right]<\infty$ for some $n\in\mathbb{N}$, then $\phi_X$ is $n$-differentiable, and its $k$-th derivative is
	\begin{align*}
		\phi_X^{(k)}(\lambda)=\E\left[(\i X)^k\e^{\i\lambda X}\right],\ k=1,\cdots,n.\tag{2.13}\label{eq:2.13}
	\end{align*}
    Furthermore, all these derivatives are uniformly continuous. Particularly, we have $\phi^{(k)}(0)=\i^k\E[X^k]$.
	\item[(iv)] $\phi_X$ is twice differentiable if and only if $\E[X^2]<\infty$. Generally, for each $k\in\mathbb{N}$, $\phi_X$ is $2k$-differentiable if and only if $\E[X^{2k}]<\infty$.
	\item[(v)] If $X$ is a continuous variable, then $\lim_{\lambda\to\pm\infty}\phi_X(\lambda)=0$.
\end{itemize}
\begin{proof}
(i) is clear by definition. To prove (ii), note that $\vert\phi_X(\lambda)\vert<1$ for all $\lambda\in\mathbb{R}$. For uniform continuity, we use the following inequality:
\begin{align*}
	\e^{\i\theta} - 1 = 2\i\e^{\frac{\i\theta}{2}}\sin\frac{\theta}{2}\ \Rightarrow\ \left\vert\e^{\i\theta}-1\right\vert\leq 2\left\vert\sin\frac{\theta}{2}\right\vert\leq\vert\theta\vert.
\end{align*}
Then for all $\lambda_1,\lambda_2\in\mathbb{R}$, we have
\begin{align*}
	\left\vert\phi_X(\lambda_1)-\phi_X(\lambda_2)\right\vert=\left\vert\E\left[\left(\e^{\i(\lambda_1-\lambda_2)X}-1\right)\e^{\i\lambda_2X}\right]\right\vert\leq\E\left\vert\e^{\i(\lambda_1-\lambda_2)X}-1\right\vert\leq2\int_\mathbb{R}\left\vert\sin\frac{(\lambda_1-\lambda_2)x}{2}\right\vert\,\d \mu(x). \tag{2.14}\label{eq:2.14}
\end{align*}
Given $\epsilon>0$, we choose $[-R,R]$ such that $\mu([-R,R])>1-\epsilon/4$. Then whenever $\vert\lambda_1-\lambda_2\vert<\frac{\epsilon}{2R}$, we have
\begin{align*}
	2\int_\mathbb{R}\left\vert\sin\frac{(\lambda_1-\lambda_2)x}{2}\right\vert\,\d \mu(x)\leq 2\int_{-R}^R\left\vert\sin\frac{(\lambda_1-\lambda_2)x}{2}\right\vert\,\d \mu(x) + \frac{\epsilon}{2} < 2\int_{[-R,R]}\frac{\epsilon}{4}\,\d \mu + \frac{\epsilon}{2} = \epsilon.\tag{2.15}\label{eq:2.15}
\end{align*}
Hence $\phi_X$ is uniformly continuous. \vspace{0.1cm}

(iii) Assume $\E\vert X\vert < \infty$. By Lebesgue dominated convergence theorem, since $\frac{1}{\epsilon}\left\vert\exp(\i\epsilon X)-1\right\vert\leq\vert X\vert$,
\begin{align*}
	\lim_{\epsilon\to 0}\frac{\phi_X(\lambda+\epsilon)-\phi_X(\lambda)}{\epsilon} = \lim_{\epsilon\to 0}\E\left[\frac{\e^{\i\epsilon X}-1}{\epsilon}\e^{\i\lambda X}\right] = \E\left[\lim_{\epsilon\to 0}\frac{\e^{\i\epsilon X}-1}{\epsilon}\e^{\i\lambda X}\right] = \E\left[\i X\e^{\i\lambda X}\right].
\end{align*}

Hence $\phi_X$ is differentiable. Furthermore, by monotone convergence theorem, we choose $R_1>0$ such that $\E[\vert X\vert\mathds{1}_{\{\vert X\vert>R_1\}}] < \epsilon/2$. Alike \hyperref[eq:2.14]{(2.14)} and \hyperref[eq:2.15]{(2.15)}, whenever $\vert\lambda_1-\lambda_2\vert<\epsilon/(2R_1^2)$, we have
\begin{align*}
	\left\vert\phi_X^\prime(\lambda_1)-\phi_X^\prime(\lambda_2)\right\vert = \E\left[\i X(\e^{\i\lambda_1 X}-\e^{\i\lambda_2 X})\right] < 2\int_{-R_1}^{R_1}\left\vert x\sin\frac{(\lambda_1-\lambda_2)x}{2}\right\vert\,\d \mu(x) + \frac{\epsilon}{2} < \epsilon.
\end{align*}
Therefore the derivative $\phi_X^\prime$ is uniformly continuous. Now assume \hyperref[eq:2.13]{(2.13)} holds for $k-1$. If $\E[\vert X\vert^k]<\infty$,
\begin{align*}
	\lim_{\epsilon\to 0}\frac{\phi_X^{(k-1)}(\lambda+\epsilon)-\phi_X^{(k-1)}(\lambda)}{\epsilon} = \lim_{\epsilon\to 0}\E\left[\frac{\e^{\i\epsilon X}-1}{\epsilon}(\i X)^{k-1}\e^{\i\lambda X}\right] = \E\left[\lim_{\epsilon\to 0}\frac{\e^{\i\epsilon X}-1}{\epsilon}(\i X)^{k-1}\e^{\i\lambda X}\right] = \E\left[(\i X)^k\e^{\i\lambda X}\right].
\end{align*}

Therefore $\phi_X$ is $k$-differentiable. Again by monotone convergence theorem, we choose $R_k>0$ such that $\E[\vert X\vert^k\mathds{1}_{\{\vert X\vert>R_k\}}] < \epsilon/2$. Whenever $\vert\lambda_1-\lambda_2\vert<\epsilon/(2R_k^{k+1})$, we have
\begin{align*}
	\left\vert\phi_X^{(k)}(\lambda_1)-\phi_X^{(k)}(\lambda_2)\right\vert = \E\left[(\i X)^k(\e^{\i\lambda_1 X}-\e^{\i\lambda_2 X})\right] < 2\int_{-R_k}^{R_k}\left\vert x^k\sin\frac{(\lambda_1-\lambda_2)x}{2}\right\vert\,\d \mu(x) + \frac{\epsilon}{2} < \epsilon.
\end{align*}
Hence $\phi_X^{(k)}$ is uniformly continuous. By induction we finish the proof of (iii). \vspace{0.1cm}

(iv) We only proves necessity, since (iii) implies sufficiency. By definition, if $\phi_X^{\prime\prime}(0)$ exists, we have
\begin{align*}
	\phi_X^{\prime\prime}(0)=\lim_{h\to 0}\frac{\phi_X(h)+\phi_X(-h)-2\phi(0)}{h^2} = \lim_{h\to 0}\int_{\mathbb{R}}\frac{2\cos(hx)-2}{h^2}\,\d \mu(x) = -2\lim_{h\to 0}\int_\mathbb{R}\frac{1-\cos(hx)}{h^2}\,\d \mu(x).
\end{align*}
By Fatou's lemma, we have
\begin{align*}
	\E[X^2] = \int_\mathbb{R}x^2\,\d \mu(x) = 2\int_\mathbb{R}\lim_{h\to 0}\frac{1-\cos(hx)}{h^2}\,\d \mu(x) \leq 2\liminf_{h\to 0}\int_\mathbb{R}\frac{1-\cos(hx)}{h^2}\,\d \mu(x) = -\phi_X^{\prime\prime}(0) < \infty.
\end{align*}
Generally, if $\phi_X^{(2k-2)}(0)$ exists, then $\E[X^{2k-2}]<\infty$, and by (iii), $\phi_X^{(2k-2)}(\lambda)=\E[(\i X)^{2k-2}\e^{\i\lambda X}]$. Then
\begin{align*}
	\phi_X^{(2k)}(0)=\lim_{h\to 0}\frac{\phi_X^{(2k-2)}(h)+\phi_X^{(2k-2)}(-h)-2\phi(0)}{h^2} &= \lim_{h\to 0}\int_{\mathbb{R}}(\i x)^{2k-2}\frac{2\cos(hx)-2}{h^2}\,\d \mu(x)\\
	&= (-1)^{k-1} 2\lim_{h\to 0}\int_\mathbb{R} x^{2k-2}\frac{1-\cos(hx)}{h^2}\,\d \mu(x).
\end{align*}
By Fatou's lemma, we have
\begin{align*}
	\E[X^{2k}] = \int_\mathbb{R}x^{2k}\,\d \mu(x) &= 2\int_\mathbb{R}\lim_{h\to 0}x^{2k-2}\frac{1-\cos(hx)}{h^2}\,\d \mu(x) \\
	&\leq 2\liminf_{h\to 0}\int_\mathbb{R}x^{2k-2}\frac{1-\cos(hx)}{h^2}\,\d \mu(x) = (-1)^{k-1}\phi_X^{(2k-2)}(0) < \infty.
\end{align*}
Then (iv) follows from induction. \vspace{0.1cm}

(v) Since $X$ is continuous, there exists a density function $\rho\in L^1(\mathbb{R})$ of $X$, and $\phi_X(\lambda)=\int_\mathbb{R}\rho(x)\e^{\i\lambda x}\,\d x$. The conclusion immediately follows from Riemann-Lebesgue lemma. We give a complete proof here.

Firstly, suppose $\rho\in C_c(\mathbb{R})$. For $\lambda\neq 0$, the substitution $x\to x-\frac{\pi}{\lambda}$ implies
\begin{align*}
	\phi_X(\lambda)=\int_\mathbb{R}\rho(x)\e^{\i\lambda x}\,\d x = \int_\mathbb{R}\rho\left(x-\frac{\pi}{\lambda}\right)\e^{\i\lambda x}\e^{\i\pi x}\,\d x = -\int_\mathbb{R}\rho\left(x-\frac{\pi}{\lambda}\right)\e^{\i\lambda x}\,\d x
\end{align*}
Use the two formulae to compute $\phi_X(\lambda)$, we have
\begin{align*}
	\vert\phi_X(\lambda)\vert\leq\frac{1}{2}\int_\mathbb{R}\left\vert \rho(x)-\rho\left(x-\frac{\pi}{\lambda}\right)\right\vert\,\d x.
\end{align*}
Since $\rho$ is continuous, $\left\vert\rho(x)-\rho\left(x-\frac{\pi}{\lambda}\right)\right\vert\to 0$ as $\vert\lambda\vert\to\infty$ for all $x\in\mathbb{R}$. By Lebesgue dominate convergence theorem, $\vert\phi_X(\lambda)\vert\to 0$ as $\vert\lambda\vert\to\infty$.

For the general case $\rho\in L^1(\mathbb{R})$, we use function approximation. Since $C_c(\mathbb{R})$ is dense in $L^1(\mathbb{R})$, we can choose $f\in C_c(\mathbb{R})$ such that $\Vert f-g\Vert_1\leq\epsilon$ for any $\epsilon>0$. Then
\begin{align*}
	\limsup_{\lambda\to\pm\infty}\vert\phi_X(\lambda)\vert \leq\limsup_{\lambda\to\pm\infty}\left\vert\int(\rho(x)-f(x))\e^{\i\lambda x}\,\d x\right\vert + \limsup_{\lambda\to\pm\infty}\left\vert\int f(x)\e^{\i\lambda x}\,\d x\right\vert \leq\epsilon+0=\epsilon.
\end{align*}
Since $\epsilon>0$ is arbitrary, we have $\vert\phi_X(\lambda)\vert\to 0$ as $\lambda\to\pm\infty$.
\end{proof}

In fact, we can determine a measure distribution uniquely by its characteristic function.
\paragraph{Theorem 2.42\label{thm:2.42}} (Inversion formula). Let $F$ be a c.d.f., and $\phi_F$ is the associated characteristic function.
\begin{itemize}
\item[(i)] For any two points $a<b$ of continuity of $F$,
\begin{align*}
	F(b)-F(a) = \lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^T\frac{\e^{-\i at}-\e^{-\i bt}}{\i t}\phi_F(t)\,\d t.
\end{align*}
Distinct c.d.f. $F$ have distinct characteristic functions.
\item[(ii)] For any $\varphi\in C_c(\mathbb{R})$, 
\begin{align*}
	\int\varphi\,\d F = \lim_{\delta\to 0^+}\frac{1}{2\pi} \int\varphi(x)\left(\int \e^{-\i tx}\phi_F(t)\e^{-\frac{\delta}{2}t^2}\,\d t\right)\,\d x.
\end{align*}
\item[(iii)] If $\Vert\phi_F\Vert_1:= \int\vert\phi_F(t)\vert\,\d t<\infty$, then $F$ has density function $\rho$:
\begin{align*}
	\rho(x)=\frac{1}{2\pi}\int\e^{-\i tx}\phi_F(t)\,\d t,
\end{align*}
and $\sup_{x\in\mathbb{R}}\rho(x)\leq\frac{1}{2\pi}\Vert\phi_F\Vert_1 < \infty$.
\end{itemize}
\begin{proof}
(ii) Let $Z\sim N(0,1)$ be independent of $X\sim F$. Then for all $\varphi\in C_c(\mathbb{R})$ and $\delta>0$,
\begin{align*}
	\E\left[\varphi(X+\sqrt{\delta}Z)\right] &= \E\left[\frac{1}{\sqrt{2\pi}}\int\varphi(X+\sqrt{\delta}\lambda)\e^{-\frac{\lambda^2}{2}}\,\d \lambda\right]\\
	&= \E\left[\frac{1}{\sqrt{2\pi}}\int\varphi(X+\sqrt{\delta}\lambda)\E\left[\e^{-\i\lambda Z}\right]\,\d \lambda\right] =\E\left[\frac{1}{\sqrt{2\pi}}\int\varphi(X+\sqrt{\delta}\lambda)\e^{-\i\lambda Z}\,\d \lambda\right]\\
	&=\E\left[\frac{1}{\sqrt{2\pi\delta}}\int\varphi(\xi)\e^{-\i\frac{\xi - X}{\sqrt{\delta}}Z}\,\d \xi\right] = \frac{1}{\sqrt{2\pi\delta}}\int\varphi(\xi)\E\left[\e^{-\i\frac{\xi - X}{\sqrt{\delta}}Z}\right]\,\d \xi\\
	&=\frac{1}{\sqrt{2\pi\delta}}\int\varphi(\xi)\E\left[\e^{-\i\frac{\xi Z}{\sqrt{\delta}}}\phi_F\left(\frac{Z}{\sqrt{\delta}}\right)\right]\,\d \xi =\frac{1}{2\pi\sqrt{\delta}}\int\varphi(\xi)\left(\int\e^{-\i\frac{\xi z}{\sqrt{\delta}}}\phi_F\left(\frac{z}{\sqrt{\delta}}\right)\e^{-\frac{z^2}{2}}\,\d z\right)\d\xi
\end{align*}
Note that $\varphi$ is continuous and bounded. Let $\delta\searrow 0$, we obtain (ii) by dominated convergence theorem. \vspace{0.1cm}

(iii) If $a<b$ are points of continuity of $F$, then by (i),
\begin{align*}
	\vert F(b)-F(a)\vert \leq \lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^T\left\vert\frac{\e^{-\i at}-\e^{-\i bt}}{\i t}\right\vert\left\vert\phi_F(t)\right\vert\,\d t.
\end{align*}
Since $\left\vert\frac{\exp(-\i at)-\exp(-\i bt)}{\i t}\right\vert\leq\vert b-a\vert$, we have 
\begin{align*}
	\vert F(b)-F(a)\vert\leq\frac{\vert b-a\vert}{2\pi}\Vert\phi_F\Vert_1.\tag{2.16}\label{eq:2.16}
\end{align*} 
For general $a<b$, we can find two sequence $a_n\nearrow a$ and $b_n\searrow b$ of points of continuity of $F$. Hence the estimate \hyperref[eq:2.16]{(2.16)} holds for all $a<b$, and $F$ is continuous. As a result, $F$ has density $\rho$, and
\begin{align*}
	\frac{F(b)-F(a)}{b-a}\leq\frac{1}{2\pi}\int\frac{\e^{\i at}-\e^{\i bt}}{\i(b-a)t}\phi_F(t)\,\d t.
\end{align*}
Let $b\searrow a$, the equation in (iii) holds, and the estimate of upper bound follows from \hyperref[eq:2.16]{(2.16)}.
\end{proof}

The proof of (i) requires some technical lemma.
\paragraph{Lemma 2.43\label{lemma:2.43}} (Dirichlet). Let $\alpha\in\mathbb{R}$. Then
\begin{align*}
	\lim_{T\to\infty}\int_{-T}^T\frac{\sin(\alpha t)}{t}\,\d t = \pi\,\mathrm{sgn}(\alpha),
\end{align*}
where $\mathrm{sgn}(\alpha)=\mathds{1}_{(0,\infty)}(\alpha)-\mathds{1}_{(-\infty,0)}(\alpha)$.
\begin{proof}
The case $\alpha=0$ is clear. It suffices to prove the case $\alpha=1$. Since $(s,t)\mapsto \e^{-st}$ is absolutely integrable on $[0,\infty)\times[0,\infty)$, by Fubini's theorem,
\begin{align*}
	\int_0^\infty\frac{\sin t}{t}\,\d t = \int_0^\infty\left(\int_0^\infty \e^{-st}\sin t\,\d s\right)\,\d t = \int_0^\infty\left(\int_0^\infty \e^{-st}\sin t\,\d t\right)\,\d s = \int_0^\infty\frac{1}{1+s^2}\,\d s = \frac{\pi}{2}.
\end{align*}
The result follows by changing variables.
\end{proof}
\renewcommand{\proofname}{Proof of \hyperref[thm:2.42]{Theorem 2.42 (i)}}
\begin{proof}
Let $a<b$ be two points of continuity of $F$, and $X\sim F$. By Fubini's theorem,
\begin{align*}
	\frac{1}{2\pi}\int_{-T}^T\frac{\e^{-\i at}-\e^{-\i bt}}{\i t}\phi_F(t)\,\d t &= \frac{1}{2\pi}\int_{-T}^T\frac{\e^{-\i at}-\e^{-\i bt}}{\i t}\E\left[\e^{\i tX}\right]\,\d t\\
	&= \E\left[\frac{1}{2\pi}\int_{-T}^T\frac{\e^{-\i at}-\e^{-\i bt}}{\i t}\e^{\i tX}\,\d t\right]\\
	&= \E\left[\frac{1}{2\pi}\int_{-T}^T\frac{\sin (tX-at) - \sin (tX-bt)}{t}\e^{\i tX}\,\d t\right]\tag{2.17}\label{eq:2.17}
\end{align*}
Note that $\int_0^T\frac{\sin t}{t}\,\d t\leq\int_0^{\pi}\frac{\sin t}{t}\,\d t\leq\pi$ for all $T>0$, the integrand in \hyperref[eq:2.17]{(2.17)} is bounded by $2$. By Lebesgue dominated convergence theorem,
\begin{align*}
	\lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^T\frac{\e^{-\i at}-\e^{-\i bt}}{\i t}\phi_F(t)\,\d t &= \E\left[\lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^T\frac{\sin (tX-at) - \sin (tX-bt)}{t}\e^{\i tX}\,\d t\right]\\
	&= \frac{1}{2}\E\left[\mathrm{sgn}(X-a)-\mathrm{sgn}(X-b)\right] \tag{By \hyperref[lemma:2.43]{Lemma 2.43}}\\
	&= \frac{1}{2}\left(1-2F(a) - 1 + 2F(b)\right) = F(b)-F(a),
\end{align*}
where the last row follows from continuity of $F$ at $a$ and $b$.
\end{proof}
\renewcommand{\proofname}{Proof}
\paragraph{Remark.} In high-dimensional case, a similar conclusion follows: Let $\mu_F$ be a distribution measure on $\mathscr{B}(\mathbb{R}^p)$, and let $\phi_F(\lambda)=\int_{\mathbb{R}^p}\exp\left(\i\langle x,\lambda\rangle\right)\,\d \mu_F(x)$ be the characteristic function of $\mu_F$. Let $A\subset\mathbb{R}^d$ be a cell of the form
\begin{align*}
	A=\left\{(x_1,\cdots,x_p):a_j\leq x_j\leq b_j\ \textit{for all}\ j\right\},
\end{align*}
where $a_j<b_j$ for all $j$ and $\mu_F(\partial A)=0$. Then
\begin{align*}
	\mu_F(A)=\lim_{T\to\infty}\frac{1}{(2\pi)^p}\int_{[-T,T]^p}\prod_{j=1}^p\left\{\frac{\e^{\i a_jt_j}-\e^{\i b_jt_j}}{it_j}\right\}\phi_F(t)\,\d t,\ \textit{where}\ t=(t_1,\cdots,t_p)\in\mathbb{R}^p. \tag{2.18}\label{eq:2.18}
\end{align*}
Note that at most countably many hyperplanes perpendicular to the coordinate axes can have positive $\mu_F$ measure. As a result, the cells $A$ with $\mu(\delta A)=0$ form a $\pi$-system that generate $\mathscr{B}(\mathbb{R}^p)$. Thus a distribution $\mu_F$ is uniquely determined by its characteristic function $\phi_F$ by \hyperref[eq:2.18]{(2.18)} and \hyperref[lemma:1.22]{Lemma 1.22}.

\paragraph{Corollary 2.44\label{cor:2.44}} (Independence). Let $X$ and $Y$ be two random variables. Let $\phi_X$ and $\phi_Y$ be the characteristic functions of $X$ and $Y$, respectively, and let $\phi_{X,Y}$ be the characteristic function of $(X,Y)$. Then $X$ and $Y$ are independent if and only if $\phi_{X,Y}(u,v)=\phi_X(u)\phi_Y(v)$.
\begin{proof}
The necessity is clear. We prove sufficiency here. If $\phi_{X,Y}(u,v)=\phi_X(u)\phi_Y(v)$, by inversion formula, $\mu_{X,Y}([a_1,b_1]\times[a_2,b_2])=\mu_X([a_1,b_1])\times\mu_Y([a_2,b_2])$ for all continuity rectangles $[a_1,b_1]\times[a_2,b_2]$, which form a $\pi$-system that generates $\mathscr{B}(\mathbb{R}^2)$. Then the result follows from \hyperref[lemma:1.22]{Lemma 1.22}.
\end{proof}

We also have the following useful corollary, which allows us to simplify some future proofs by doing only the 1-dimension case.
\paragraph{Lemma 2.45.\label{lemma:2.45}} (Cramér-Wold device). Let $X$ and $Y$ be two $p$-dimensional random vectors. Then $X\overset{d}{=}Y$ if and only $\langle X,\alpha\rangle\overset{d}{=}\langle Y,\alpha\rangle$ for all $\alpha\in\mathbb{R}^p$.
\begin{proof}
The necessity is clear. For sufficiency, note that when $\langle X,\alpha\rangle\overset{d}{=}\langle Y,\alpha\rangle$ for all $\alpha\in\mathbb{R}^p$, the characteristic functions of $X$ and $Y$ are the same.
\end{proof}

We can use characteristic functions to investigate the tail properties of distribution functions.

\paragraph{Proposition 2.46.\label{prop:2.46}} Let $\phi_X$ be the characteristic function of a random variable $X$. For each $\epsilon,\delta>0$, there exists a constant $K>0$ depending only on $\delta$ such that
\begin{align*}
	\P(\vert X\vert\geq\epsilon)\leq K\int_0^1\left[1-\Re\left(\phi_X\left(\frac{t\delta}{\epsilon}\right)\right)\right]\,\d t,\\
	\E\left[X^2\mathds{1}_{\{\vert X\vert\leq\epsilon\}}\right]\leq K\epsilon^2\left[1-\Re\left(\phi_X\left(\frac{t\delta}{\epsilon}\right)\right)\right].
\end{align*}
\begin{proof}
	We redefine that $\frac{\sin 0}{0}=1$ and $\frac{1-\cos 0}{0^2}=\frac{1}{2}$, so both $\frac{\sin x}{x}$ and $\frac{1-\cos x}{x^2}$ become uniformly continuous functions on $\mathbb{R}$. Then for any $\delta>0$, there exists $K>0$ such that
	\begin{align*}
		1-\frac{\sin x}{x}\geq\frac{1}{K},\ \forall\vert x\vert\geq\delta,\ \ \textit{and}\ \ \frac{1-\cos x}{x^2}\geq\frac{1}{K},\ \forall \vert x\vert\leq\delta.
	\end{align*}
	Let $X\sim F$, and let $\eta=\delta/\epsilon$. Then
	\begin{align*}
		\P\left(\vert X\vert\geq\epsilon\right) &= \E\left[\mathds{1}_{\{\vert\eta X\vert\geq\delta\}}\right] \leq K\E\left[1-\frac{\sin(\eta X)}{\eta X}\right]\\
		&= K - \E\left[K\int_0^1\cos(\eta tX)\,\d t\right] = K\int_0^1\left[1-\Re\left(\phi_X(\eta t)\right)\right]\,\d t,\tag{By Fubini's theorem}
	\end{align*}
    and
    \begin{align*}
		\E\left[X^2\mathds{1}_{\{\vert X\vert\leq\epsilon\}}\right]&=\frac{1}{\eta^2}\E\left[\vert\eta X\vert^2\mathds{1}_{\{\vert \eta X\vert\leq\delta\}}\right]\leq \frac{K}{\eta^2}\E\left[1-\cos(\eta X)\right] = K\epsilon^2\left[1-\Re\left(\phi_X(\eta t)\right)\right].
	\end{align*}
	Thus we complete the proof.
\end{proof}

\newpage
\subsection{Continuity Theorem and Central Limit Theorems}
\paragraph{Definition 2.47\label{def:2.47}} (Quantile). Given a c.d.f. $F:\mathbb{R}\to[0,1]$, the \textit{quantile} of distribution $F$ is defined as
\begin{align*}
	F^{-1}(p)=\inf\left\{\alpha\in\mathbb{R}:F(\alpha)\geq p\right\},\ \forall p\in(0,1).
\end{align*}

\paragraph{Remark.} The quantile $F^{-1}:(0,1)\to\mathbb{R}$ satisfies the following properties:
\begin{itemize}
	\item[(i)] $F^{-1}:(0,1)\to\mathbb{R}$ is monotone increasing.
	\item[(ii)] $F^{-1}$ has at most countably many points of discontinuity. To see this, let $E$ be the set of these points. Then for each $x\in E$, since $F^{-1}$ is monotone, define
	\begin{align*}
		l_x:=\lim_{y\nearrow x} F^{-1}(y) < \lim_{y\searrow x} F^{-1}(y)=:r_x.
	\end{align*}
    Since $\mathbb{Q}$ is dense in $\mathbb{R}$, choose $q_x\in\mathbb{Q}\cap(l_x,r_x)$. Since $F^{-1}$ is monotone increasing, the intervals $(l_x,r_x)$ are pairwise disjoint. Thus we obtain a bijection $x\mapsto q_x$ from $E$ to a subset of $\mathbb{Q}$. Hence $E$ has at most countably many elements. In fact, this conclusion holds for all monotone functions on $\mathbb{R}$.
    
	\item[(iii)] $F^{-1}$ is left-continuous. This follows from the right-continuity of $F$:
	\begin{align*}
		\left\{\alpha\in\mathbb{R}:F(\alpha)\geq p\right\} = \bigcap_{n=1}^\infty\left\{\alpha\in\mathbb{R}:F(\alpha)\geq p-\frac{1}{n}\right\}\ \ \Rightarrow\ \ F^{-1}(p)=\lim_{n\to\infty} F^{-1}\left(p-\frac{1}{n}\right).
	\end{align*}
\end{itemize}

\paragraph{Lemma 2.48\label{lemma:2.48}} (Galois inequality). Let $\alpha\in\mathbb{R}$ and $p\in(0,1)$. Then $F(\alpha)\geq p$ if and only if $F^{-1}(p)\leq\alpha$. Particulièrement, we have $F(F^{-1}(p))\geq p$ and $F^{-1}(F(\alpha))\leq\alpha$.
\begin{proof}
The ``only if'' case follows from definition. Conversely, assume $\alpha\geq F^{-1}(p):=\inf\{z\in\mathbb{R}:F(z)\geq p\}$. Then we have $\alpha+n^{-1}\in\{z\in\mathbb{R}:F(z)\geq p\}$ for all $n\in\mathbb{N}$. By right-continuity of $F$,
\begin{align*}
	F(\alpha)=\lim_{n\to\infty}F\left(\alpha+\frac{1}{n}\right) \geq p.
\end{align*}
Thus we finish the proof.
\end{proof}

\paragraph{Corollary 2.49\label{cor:2.49}} (Quantile transformation). Let $U\sim\mathrm{Unif}(0,1)$. Then $F^{-1}(U)\sim F$.
\begin{proof}
By Galois inequality, for all $x\in\mathbb{R}$,	we have $\P(F^{-1}(U)\leq x) = \P(F(x)\geq U)=F(x).$
\end{proof}

\paragraph{Theorem 2.50\label{thm:2.50}} (Weak convergence of quantiles). Let $F_n$ be a c.d.f. sequence, and $F$ a c.d.f.. Then $F_n\overset{w}{\to}F$ if and only if $F_n^{-1}(p)\to F^{-1}(p)$ for each point $p$ of continuity of $F^{-1}$. 
\begin{proof}
Assume $F_n\overset{w}{\to} F$, and let $Z\sim N(0,1)$. Since $F$ is discontinuous at at most countably many points, we have $F_n(Z)\overset{a.s.}{\to}F(Z)$, and $F_n(Z)\overset{d}{\to}F(Z)$. By Portmanteau lemma [\hyperref[thm:2.29]{Theorem 2.29 (vii)}], if the function $p\mapsto\P(F(Z)<p)$ is continuous at $p\in(0,1)$, we have $\P(F(Z)=p)=0$, and $\P(F_n(Z)<p)\to\P(F(Z)<p)$.

Let $\Phi$ be the c.d.f. of standard Gaussian variables. By Galois inequality,
\begin{align*}
	\Phi(F_n^{-1}(p))=\P(Z<F_n^{-1}(p))=\P(F_n(Z)<p)\overset{n\to\infty}{\to}\P(F(Z)<p)=\Phi(F^{-1}(p))
\end{align*}  
for each point $p$ of continuity of $\Phi\circ F^{-1}$. By continuity of $\Phi$, these contain all points of continuity of $F^{-1}$. Again, by continuity of $\Phi^{-1}$, we have $F_n^{-1}(p)\to F^{-1}(p)$ for each point $p$ of continuity of $F^{-1}$.

Conversely, assume that $F_n^{-1}(p)\to F^{-1}(p)$ for each point $p$ of continuity of $F^{-1}$. Let $U\sim\mathrm{Unif}(0,1)$, then $F_n^{-1}(U)\overset{a.s.}{\to}F^{-1}(U)$, since $F^{-1}$ has at most countably many points of discontinuity. Since $F_n^{-1}(U)\sim F_n$ and $F^{-1}(U)\sim F$, we have $F_n\overset{w}{\to}F$.
\end{proof}

\paragraph{Theorem 2.51\label{thm:2.51}} (Skorokhod's almost sure representations). Let $F_n:\mathbb{R}\to[0,1]$ be a sequence of c.d.f.'s such that $F_n\overset{w}{\to} F$, where $F$ is also a c.d.f.. Then there exists a probability space $(\Omega,\mathscr{F},\P)$ and a sequence of random variables $X_n$ on it such that $X_n\sim F_n$ for all $n\in\mathbb{N}$, and $X_n\overset{a.s.}{\to} X$, where $X\sim F$.
\begin{proof}
We use the quantile transformation. Let $\Omega=[0,1]$, and let $\P$ be the Lebesgue measure on $[0,1]$. Define $X(\omega)= F^{-1}(\omega)$ and $X_n(\omega)=F_n^{-1}(\omega)$ for all $n\in\mathbb{N}$. Then $X\sim F$, and $X_n\sim F_n$. By \hyperref[thm:2.50]{Theorem 2.50}, $X_n\to X$ on $\Omega$ except possibly at countably many points of discontinuity of $F^{-1}$, which form a null set.
\end{proof}

\paragraph{Corollary 2.52\label{cor:2.52}} (Convergence of characteristic functions). Let $F_n$ be a c.d.f. sequence, and let $\phi_{F_n}$ be the sequence of associated characteristic functions. If $F_n\overset{w}{\to}F$, where $F$ is a c.d.f., then $\phi_{F_n}\to\phi_F$ pointwise.
\begin{proof}
By Skorokhod's representation theorem, we can choose $X_n\sim F_n$ and $X\sim F$ such that $X_n\overset{a.s.}{\to}X$. Then $\e^{\i\lambda X_n}\overset{a.s.}{\to}\e^{\i\lambda X}$ for all $\lambda\in\mathbb{R}$. By Lebesgue dominated convergence theorem, $\phi_{F_n}\to\phi_F$ pointwise.
\end{proof}

\paragraph{Theorem 2.53\label{thm:2.53}} (Lévy's continuity theorem). Let $X_n$ be a sequence of random variables, and let $\phi_{n}$ be the sequence of associated characteristic functions. If $\phi_{n}$ converges pointwise to a function $\phi:\mathbb{R}\to\mathbb{C}$, the following are equivalent:
\begin{itemize}
	\item[(i)] $\{X_n\}_{n=1}^\infty$ is uniformly tight, i.e. $\lim_{M\to\infty}\sup_{n\in\mathbb{N}}\P(\vert X_n\vert \geq M)=0$.
	\item[(ii)] $X_n\overset{d}{\to}X$ for some random variable $X$.
	\item[(iii)] $\phi$ is the characteristic of some random variable $X$, i.e. $\phi(\lambda)=\E[\e^{\i\lambda X}]$;
	\item[(iv)] $\phi$ is continuous everywhere on $\mathbb{R}$;
	\item[(v)] $\phi$ is continuous at $0$.
\end{itemize}
\begin{proof}
(i) $\Rightarrow$ (ii): Let $F_n$ be the c.d.f. of $X_n$. By \hyperref[thm:2.32]{Theorem 2.32 (ii)}, for every subsequence of $X_n$, we can extract a further subsequence which converges some random variable $X\sim F$. By \hyperref[cor:2.52]{Corollary 2.52}, $\phi_F=\phi$, hence $F$ is uniquely determined by $\phi$. We can fix $X$ and conclude that every subsequence $X_{n_k}$ of $X_n$ has a further subsequence that converges in distribution to $X$.

It remains to show $X_n\overset{d}{\to}X$. If not, choose $f\in C_b(\mathbb{R})$ such that $\E[f(X_n)]$ does not converge to $\E[f(X)]$. Then there exists $\epsilon>0$ such that for all $k\in\mathbb{N}$ we can find $n_k\geq k$ such that $\vert\E[f(X_{n_k})]-\E[f(X)]\vert>\epsilon$. As a result, $\E[f(X_{n_k})]$ has no subsequence converging to $\E[f(X)]$, and $X_{n_k}$ has no subsequence converging in distribution to $X$, a contradiction! Hence $X_n\overset{d}{\to} X$. (This is called the subsequence trick.)

(ii) $\Rightarrow$ (iii) follows from \hyperref[cor:2.52]{Corollary 2.52}. (iii) $\Rightarrow$ (iv) and (iv) $\Rightarrow$ (v) are trivial.

(v) $\Rightarrow$ (i): Following \hyperref[prop:2.46]{Proposition 2.46}, we set $\delta=2$ and $K=5$. The following estimate holds for all $n\in\mathbb{N}$:
\begin{align*}
	\P(\vert X_n\vert\geq T)\leq 5\int_0^1\left[1-\Re\left(\phi_{n}\left(\frac{2t}{T}\right)\right)\right]\,\d t.
\end{align*}

By Lebesgue dominated convergence theorem,
\begin{align*}
	\lim_{n\to\infty}\P(\vert X_n\vert\geq T)\leq 5\int_0^1\left[1-\Re\left(\phi\left(\frac{2t}{T}\right)\right)\right]\,\d t.
\end{align*}
Since $\phi(0)=1$, and $\phi$ is continuous at $0$, the right-hand side of the above estimate converges to $0$ as $T\to\infty$. Given $\epsilon>0$, we choose $T_0$ such that $\lim_{n\to\infty}\P(\vert X_n\vert\geq T_0)<\epsilon/2$, and choose $N$ such that $\P(\vert X_n\vert\geq T_0)<\epsilon$ for all $n\geq N$. Then $\{X_n\}_{n=N}^\infty$ is uniformly tight, and so is $\{X_n\}_{n=1}^\infty$.
\end{proof}

\paragraph{Remark.} We can summarize a commonly used conclusion from \hyperref[thm:2.53]{Theorem 2.53}, which can be viewed as a converse of \hyperref[cor:2.52]{Corollary 2.52}: 

Let $F_n$ be a c.d.f. sequence, and let $\phi_{F_n}$ be the sequence of associated characteristic functions. If $\phi_{F_n}\to\phi$ pointwise, and $\phi$ is continuous at $0$, then $\phi$ is the characteristic function of some c.d.f. $F$, and $F_n\overset{w}{\to}F$.

\paragraph{Theorem 2.54\label{thm:2.54}} (Khintchine's weak law of large numbers). Let $(X_n)_{n=1}^\infty$ be a sequence of independent and identically distributed (i.i.d.) random variables such that $\E[\vert X_1\vert]<\infty$. Denote $\mu:=\E[X_1]<\infty$. Define
\begin{align*}
	\overline{X}_n = \frac{1}{n}\sum_{j=1}^n X_j.
\end{align*}
Then $\overline{X}_n\overset{\P}{\to}\mu$.
\begin{proof}
Without loss of generality, assume $\mu=0$. Let $\phi:\mathbb{R}\to\mathbb{C}$ be the characteristic function of $X_1$. Then the characteristic function of $\overline{X}_n$ is
\begin{align*}
	\phi_n(\lambda)=\E\left[\prod_{j=1}^n\e^{\i\frac{\lambda}{n}X_j}\right] = \phi\left(\frac{\lambda}{n}\right)^n.
\end{align*}

Since $\E[\vert X_1\vert]<\infty$, $\phi$ is differentiable, $\phi^\prime(0)=\i\,\E X_1=0$, and $\phi^\prime$ is uniformly continuous. Fix $\lambda\in\mathbb{R}$. Given any $\epsilon>0$, we can choose $N$ such that $\vert\phi^\prime(t)\vert\leq\epsilon$ for all $\vert t\vert \leq\vert\lambda\vert/N$. Hence
\begin{align*}
	\lim_{n\to\infty}\vert\phi_n(\lambda)-1\vert = \lim_{n\to\infty}\left\vert\phi\left(\frac{\lambda}{n}\right)^n - 1\right\vert\leq\lim_{n\to\infty}\left\vert\left(1+\int_0^{\lambda/n}\phi^\prime(t)\,\d t\right)^n - 1\right\vert\leq\max\left\{\e^{\vert\lambda\vert\epsilon} - 1, 1-\e^{-\vert\lambda\vert\epsilon}\right\}.
\end{align*}

Since $\epsilon>0$ is arbitrary, we have $\phi_n(\lambda)\to 1$ pointwise. By Lévy's continuity theorem, $\overline{X}_n\overset{d}{\to}0$. By \hyperref[thm:2.35]{Theorem 2.35 (iv)},we have  $\overline{X}_n\overset{\P}{\to}0$.
\end{proof}

\paragraph{Theorem 2.55\label{thm:2.55}} (Lindeberg-Lévy central limit theorem). Let $(X_n)_{n=1}^\infty$ be a sequence of i.i.d. random variables such that $\E\left[\vert X_1\vert^2\right]<\infty$. Denote $\mu:=\E X_1<\infty$, and $0<\sigma^2:=\mathrm{Var}(X_1)<\infty$. Define
\begin{align*}
	\overline{X}_n = \frac{1}{n}\sum_{j=1}^n X_j,\ Z_n=\frac{\sqrt{n}}{\sigma}(\overline{X}_n-\mu)
\end{align*}
Then $Z_n\overset{d}{\to}Z$, where $Z\sim N(0,1)$.
\begin{proof}
Without loss of generality, assume that $\mu=0$ and $\sigma^2=1$. Let $\phi$ be the characteristic function of $X_1$. Then the characteristic function of $Z_n$ is
\begin{align*}
	\phi_n(\lambda)=\E\left[\prod_{j=1}^n\e^{\i\frac{\lambda}{\sqrt{n}}X_j}\right] = \phi\left(\frac{\lambda}{\sqrt{n}}\right)^n.
\end{align*}
Since $\E[\vert X_1\vert^2]<\infty$, $\phi$ is twice-differentiable, $\phi^\prime(0)=\i\,\E X_1=0$, and $\phi^{\prime\prime}(0)=-\E[X_1^2]=-1$. 
\begin{align*}
	\phi\left(\frac{\lambda}{\sqrt{n}}\right) = 1 + \int_0^{\lambda/\sqrt{n}}\phi^\prime(t)\,\d t = 1 + \int_0^{\lambda/\sqrt{n}}\int_0^t\phi^{\prime\prime}(u)\,\d u\,\d t = 1 - \frac{\lambda^2}{2n} + \int_0^{\lambda/\sqrt{n}}\int_0^t\left(1+\phi^{\prime\prime}(u)\right)\,\d u\,\d t.
\end{align*}
Note that $\phi^{\prime\prime}$ is uniformly continuous. Given $\epsilon>0$, choose $N$ such that $\vert 1+\phi^{\prime\prime}(u)\vert<\epsilon$ for all $\vert u\vert\leq\vert\lambda\vert/\sqrt{N}$. Then for all $n\geq N$, we have
\begin{align*}
	1-\frac{\lambda^2(1+\epsilon)}{2n} \leq \phi\left(\frac{\lambda}{\sqrt{n}}\right) \leq 1-\frac{\lambda^2(1-\epsilon)}{2n}\ \Rightarrow\ \e^{-\frac{\lambda^2(1+\epsilon)}{2}}\leq\lim_{n\to\infty}\phi_n(\lambda)\leq\e^{-\frac{\lambda^2(1-\epsilon)}{2}}.
\end{align*}
Since $\epsilon>0$ is arbitrary, $\lim_{n\to\infty}\phi_n(\lambda)=\e^{-\lambda^2/2}$, which is the characteristic function of $Z\sim N(0,1)$.
\end{proof}

\paragraph{Theorem 2.56\label{thm:2.56}} (Lindeberg-Feller central limit theorem). Let $(X_n)_{n=1}^\infty$ be a sequence of independent random variables such that $\E X_n=0$ and $0<\sigma_n^2:=\E\left[X_n^2\right]<\infty$. Define
\begin{align*}
	s_n^2 = \sum_{k=1}^n \sigma_k^2,\ Z_n=\frac{1}{s_n}\sum_{k=1}^n X_k.
\end{align*}
Then $Z_n\overset{d}{\to}N(0,1)$ and
\begin{align*}
	\lim_{n\to\infty}\frac{\max_{1\leq k\leq n}\sigma_k^2}{s_n^2} = 0\tag{2.19}\label{eq:2.19}
\end{align*}
if and only if the following \textit{Lindeberg's condition} is satisfied:
\begin{align*}
	\lim_{n\to\infty}\frac{1}{s_n^2}\sum_{k=1}^n \E\left[X_k^2\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right] = 0,\ \forall\epsilon>0.\tag{2.20}\label{eq:2.20}
\end{align*}
\renewcommand{\proofname}{Proof of Suffieiency}
\begin{proof}
Suppose the Lindeberg's condition \hyperref[eq:2.20]{(2.20)} holds. Then for $1\leq k\leq n$ and all $\epsilon>0$,
\begin{align*}
	\frac{\max_{1\leq k\leq n}\sigma_k^2}{s_n^2} = \max_{1\leq k\leq n}\left\{\frac{1}{s_n^2}\E\left[X_k^2\mathds{1}_{\{\vert X_k\vert<\epsilon s_n\}}\right] + \frac{1}{s_n^2}\E\left[X_k^2\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right]\right\}\leq \epsilon^2 + \frac{1}{s_n^2}\sum_{k=1}^n \E\left[X_k^2\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right].
\end{align*}
Then \hyperref[eq:2.19]{(2.19)} is true follows by letting $n\to\infty$ and $\epsilon\searrow 0$. Let $\phi_n$ be the characteristic function of $X_n$. To prove $Z_n\to N(0,1)$, we need to show that the characteristic function of $Z_n$ satisfies
\begin{align*}
	\phi_{Z_n}(\lambda) = \prod_{k=1}^n\phi_k\left(\frac{\lambda}{s_n}\right)\ \rightarrow\ \e^{-\lambda^2/2}\ \textit{as}\ n\to\infty.\tag{2.21}\label{eq:2.21}
\end{align*}
The result then follows from Lévy's continuity theorem. We claim that \hyperref[eq:2.21]{(2.21)} holds if and only if
\begin{align*}
	\lim_{n\to\infty}\sum_{k=1}^n\left(\phi_k\left(\frac{\lambda}{s_n}\right)-1\right) + \frac{\lambda^2}{2} = 0,\tag{2.22}\label{eq:2.22}
\end{align*}
We first prove the following \hyperref[eq:2.23]{(2.23)}, which together with \hyperref[eq:2.22]{(2.22)} implies \hyperref[eq:2.21]{(2.21)}.
\begin{align*}
	\lim_{n\to\infty}\left\vert\exp\left\{\sum_{k=1}^n\left(\phi_k\left(\frac{\lambda}{s_n}\right)-1\right)\right\} - \prod_{k=1}^n\phi_k\left(\frac{\lambda}{s_n}\right)\right\vert = 0.\tag{2.23}\label{eq:2.23}
\end{align*}

\paragraph{} \textit{Claim I.}\ \ If $\phi:\mathbb{R}\to\mathbb{C}$ is a characteristic function, so is $\lambda\mapsto\e^{\phi(\lambda)-1}$.

Let $(Y_n)_{n=1}^\infty$ be a sequence of i.i.d. random variables, and let $N\sim\mathrm{Poisson}(1)$ be a random variable independent of $Y_n$'s. Define $W=\sum_{k=1}^N Y_k$. Then the characteristic function of $W$ is
\begin{align*}
	\E\left[\e^{\i\lambda W}\right] = \E\left[\E\left[\e^{\i\lambda W}|N\right]\right] = \E\left[\phi(\lambda)^N\right] = \sum_{n=0}^\infty\frac{\e^{-1}}{n!}\phi(\lambda)^n = \e^{\phi(\lambda)-1}.
\end{align*}

\textit{Claim II}\ \ (Product comparison). Given $\{a_1,\cdots,a_n\},\{b_1,\cdots,b_n\}\subset\{z\in\mathbb{C}:\vert z\vert\leq 1\}$, it holds
\begin{align*}
	\left\vert\prod_{j=1}^n a_j - \prod_{j=1}^n b_j\right\vert\leq \sum_{j=1}^n\vert a_j-b_j\vert.
\end{align*}
The case $n=1$ is clear. Then prove the case $n=2$:
\begin{align*}
	\left\vert a_1b_1 - a_2b_2\right\vert = \left\vert a_1(b_1-b_2) - (a_1-b_1)b_2\right\vert\leq\left\vert a_1 - b_1\right\vert+\left\vert a_2 - b_2\right\vert.
\end{align*}
We apply this formula on $\prod_{j=1}^{n-1}a_j,\prod_{j=1}^{n-1}b_j,a_n,b_n$, so the general case follows from induction.

\paragraph{}\textit{Proof of \hyperref[eq:2.23]{(2.23)}.} By Claims I and II,
\begin{align*}
	\left\vert\exp\left\{\sum_{k=1}^n\left(\phi_k\left(\frac{\lambda}{s_n}\right)-1\right)\right\} - \prod_{k=1}^n\phi_k\left(\frac{\lambda}{s_n}\right)\right\vert\leq\sum_{k=1}^n\left\vert\exp\left(\phi_k\left(\frac{\lambda}{s_n}\right)-1\right) - \phi_k\left(\frac{\lambda}{s_n}\right)\right\vert.\tag{2.24}\label{eq:2.24}
\end{align*}
By Taylor's theorem, we have
\begin{align*}
	\left\vert\phi_k\left(\frac{\lambda}{s_n}\right)-1\right\vert\leq 1+\frac{\lambda^2}{2s_n^2}\sup_{t\in\mathbb{R}}\phi^{\prime\prime}(t) - 1 = \frac{\lambda^2\sigma_k^2}{2s_n^2}\leq\frac{\lambda^2}{2}\max_{1\leq k\leq n}\frac{\sigma_k^2}{s_n^2}.
\end{align*}
Given any $0<\epsilon<1$, by \hyperref[eq:2.19]{(2.19)}, we can choose $N$ such that $\vert\phi_k(\lambda/s_n)-1\vert\leq\epsilon/2$ for all $n\geq N$ and all $1\leq k\leq n$. Since $\vert\e^z - z - 1\vert\leq\epsilon\vert z\vert$ for all $\vert z\vert\leq\epsilon/2$, following \hyperref[eq:2.24]{(2.24)}, we have
\begin{align*}
	\left\vert\exp\left\{\sum_{k=1}^n\left(\phi_k\left(\frac{\lambda}{s_n}\right)-1\right)\right\} - \prod_{k=1}^n\phi_k\left(\frac{\lambda}{s_n}\right)\right\vert\leq\sum_{k=1}^n\epsilon\left\vert\phi_k\left(\frac{\lambda}{s_n}\right) - 1\right\vert\leq\frac{\epsilon\lambda^2}{2s_n^2}\sum_{k=1}^n \sigma_k^2 = \frac{\epsilon\lambda^2}{2},\ \forall n\geq N.
\end{align*}
Since $\epsilon>0$ is arbitrary, the limit \hyperref[eq:2.23]{(2.23)} holds.

\paragraph{}\textit{Proof of \hyperref[eq:2.22]{(2.22)}.} Given $\epsilon>0$, we use the following expansion:
\begin{align*}
	\sum_{k=1}^n\left(\phi_k\left(\frac{\lambda}{s_n}\right)-1\right) + \frac{\lambda^2}{2} &= \sum_{k=1}^n\E\biggl[\underbrace{\e^{\i \lambda X_k/s_n} - 1 - \frac{\i\lambda}{s_n}X_k - \frac{(\i\lambda)^2}{2s_n^2}X_k^2}_{=:A_{n,k}}\biggr]\\
	&= \sum_{k=1}^n\E\left[A_{n,k}\mathds{1}_{\{\vert X_k\vert<\epsilon s_n\}}\right] + \sum_{k=1}^n\E\left[A_{n,k}\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right] =: S_{n,\epsilon}^{(1)}+S_{n,\epsilon}^{(2)}.
\end{align*}
Now we bound the two terms. For the first term,
\begin{align*}
	\left\vert S_{n,\epsilon}^{(1)}\right\vert\leq \sum_{k=1}^n\E\left\vert A_{n,k}\mathds{1}_{\{\vert X_k\vert<\epsilon s_n\}}\right\vert &\leq \sum_{k=1}^n\E\left[ \frac{1}{3!}\left\vert\frac{\lambda}{s_n}X_k\right\vert^3\mathds{1}_{\{\vert X_k\vert<\epsilon s_n\}}\right]\\
	&= \frac{\vert\lambda\vert^3}{6s_n^3}\sum_{k=1}^n\E\left[\left\vert X_k\right\vert^3\mathds{1}_{\{\vert X_k\vert<\epsilon s_n\}}\right] \leq \frac{\vert\lambda\vert^3\epsilon}{6s_n^2}\sum_{k=1}^n\E\left[\vert X_k\vert^2\right] = \frac{\vert\lambda\vert^3\epsilon}{6}.\tag{2.25}\label{eq:2.25}
\end{align*}
By Lindeberg's condition, we can bound the second term as $n\to\infty$:
\begin{align*}
	\left\vert S_{n,\epsilon}^{(2)}\right\vert \leq \sum_{k=1}^n\E\left\vert A_{n,k}\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right\vert\leq\sum_{k=1}^n\E\left[\left\vert\frac{\lambda}{s_n}X_k\right\vert^2\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right]=\frac{\vert\lambda\vert^2}{s_n^2}\sum_{k=1}^n\E\left[ X_k^2\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right]\to 0.\tag{2.26}\label{eq:2.26}
\end{align*}
Since $\epsilon>0$, we can bound \hyperref[eq:2.22]{(2.22)} by arbitrarily small numbers, and the result follows.
\end{proof}

\paragraph{Remark.} In estimates \hyperref[eq:2.25]{(2.25)} and \hyperref[eq:2.26]{(2.26)}, we used the following estimate in case $n=2$:
\begin{align*}
	\left\vert\e^{\i\theta}-\sum_{k=1}^n\frac{(\i\theta)^k}{k!}\right\vert\leq\min\left\{\frac{2\theta^n}{n!},\frac{\theta^{n+1}}{(n+1)!}\right\},\ \forall\theta\in\mathbb{R}.
\end{align*}
The proof of necessity is given by William Feller.
\renewcommand{\proofname}{Proof of Necessity}
\begin{proof}
Assume that \hyperref[eq:2.19]{(2.19)} holds and $Z_n\overset{d}{\to} N(0,1)$. Note the proof of \hyperref[eq:2.23]{(2.23)} only uses \hyperref[eq:2.19]{(2.19)}. Then both \hyperref[eq:2.21]{(2.21)} and \hyperref[eq:2.23]{(2.23)} hold, which together imply \hyperref[eq:2.22]{(2.22)}. Let $\epsilon>0$. If $\lambda\neq 0$, we have
\begin{align*}
\frac{1}{s_n^2}\sum_{k=1}^n\E\left[X_k^2\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right] &= 1 - \frac{2}{\lambda^2}\sum_{k=1}^n\E\left[ \frac{\lambda^2X_k^2}{2s_n^2}\mathds{1}_{\{\vert X_k\vert<\epsilon s_n\}}\right]\\
&\leq \frac{2}{\lambda^2}\left(\frac{\lambda^2}{2} + \sum_{k=1}^n\E\left[ \left(\cos\left(\frac{\lambda X_k}{s_n}\right) - 1\right)\mathds{1}_{\{\vert X_k\vert<\epsilon s_n\}}\right]\right) \tag{by $1-\cos x\leq\frac{x^2}{2}$}\\
&= \frac{2}{\lambda^2}\left(\frac{\lambda^2}{2} +\sum_{k=1}^n\Re\left(\phi_k\left(\frac{\lambda}{s_n}\right) - 1\right) + \sum_{k=1}^n\E\left[\left(1-\cos\left(\frac{\lambda X_k}{s_n}\right)\right)\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right]\right)\\
&\leq \frac{2}{\lambda^2}\left(\frac{\lambda^2}{2} +\sum_{k=1}^n\Re\left(\phi_k\left(\frac{\lambda}{s_n}\right) - 1\right)\right) + \frac{4}{\lambda^2}\sum_{k=1}^n\P(\vert X_k\vert\geq\epsilon s_n)\\
&\leq \frac{2}{\lambda^2}\left(\frac{\lambda^2}{2} +\sum_{k=1}^n\Re\left(\phi_k\left(\frac{\lambda}{s_n}\right) - 1\right)\right) + \frac{4}{\lambda^2}\sum_{k=1}^n\frac{\sigma_k^2}{\epsilon^2s_n^2}\tag{By Chebyshev's inequality}\\
&\leq \frac{2}{\lambda^2}\left(\frac{\lambda^2}{2} +\sum_{k=1}^n\Re\left(\phi_k\left(\frac{\lambda}{s_n}\right) - 1\right)\right) + \frac{4}{\lambda^2\epsilon^2}.
\end{align*}

By \hyperref[eq:2.22]{(2.22)}, the first term converges to $0$ as $n\to\infty$. Since $\lambda\neq 0$ is arbitrary, we obtain the Lindeberg's condition \hyperref[eq:2.20]{(2.20)} by letting $\lambda^2\to\infty$.
\end{proof}
\renewcommand{\proofname}{Proof}
In practice, the Lindeberg's condition is not convenient to verify. In many cases, we would rather use one of its sufficient condition proposed by Lyapunov.

\paragraph{Theorem 2.57\label{thm:2.57}} (Lyapunov Condition). Let $(X_n)_{n=1}^\infty$ be a sequence of independent random variables such that $\E X_n=0$ and $0<\sigma_n^2:=\E\left[X_n^2\right]<\infty$. Let $s_n^2=\sigma_1^2+\cdots+\sigma_n^2$. If there exists $\delta>0$ satisfying the \textit{Lyapunov condition}:
\begin{align*}
	\lim_{n\to\infty}\frac{1}{s_n^{2+\delta}}\sum_{k=1}^n\E\left[\vert X_k\vert^{2+\delta}\right]= 0,\label{eq:2.27}\tag{2.27}
\end{align*}
then the Lindeberg's condition \hyperref[eq:2.20]{(2.20)} holds, hence the central limit theorem [\hyperref[thm:2.56]{Theorem 2.56}].
\begin{proof}
If there exists $\delta>0$ that satisfies the Lyapunov condition \hyperref[eq:2.27]{(2.27)}, then
\begin{align*}
	\frac{1}{s_n^2}\sum_{k=1}^n\E\left[X_k^2\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right] &= \sum_{k=1}^n\E\left[\left\vert\frac{X_k}{s_n}\right\vert^2\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right]\\
	&\leq \sum_{k=1}^n\E\left[\left\vert\frac{X_k}{s_n}\right\vert^2\left\vert\frac{X_k}{\epsilon s_n}\right\vert^\delta\mathds{1}_{\{\vert X_k\vert\geq\epsilon s_n\}}\right] \leq \frac{1}{\epsilon^\delta s_n^{2+\delta}}\sum_{k=1}^n\E\left[\vert X_k\vert^{2+\delta}\right]\to 0\ \ \textit{as}\ n\to\infty.
\end{align*}
Hence the Lindeberg's condition \hyperref[eq:2.20]{(2.20)} is satisfied.
\end{proof}
\paragraph{Remark.} A useful special case of the Lyapunov condition is when $\delta=1$:
\begin{align*}
	\lim_{n\to\infty}\frac{1}{s_n^3}\sum_{k=1}^n\E\left[\vert X_k\vert^3\right]= 0.
\end{align*} 

\section{Martingales}
\subsection{Processes, Filtrations and Stopping Times}
\begin{definition}[Stochastic processes]\label{def:3.1} Given a probability space $(\Omega,\mathscr{F},\P)$, a metric space $(E,d)$ and a nonempty set $\mathcal{T}$, a \textit{stochastic process} is a function $(t,\omega)\mapsto X_t(\omega)$ defined on the set $\Omega\times\mathcal{T}$ and taking values in $E$ such that $X_t(\cdot)$ is measurable for each $t\in\mathcal{T}$. 
\end{definition}

\paragraph{Remark.} We can also view a stochastic process as a collection $X=\{X_t\}_{t\in\mathcal{T}}$ of $E$-valued random variables indexed by elements of $\mathcal{T}$. If $\mathcal{T}$ is a topological space given the discrete topology, we call $\{X_t\}_{t\in\mathcal{T}}$ a \textit{discrete} stochastic process. Furthermore, if $\mathcal{T}=\mathbb{N}_0$, we call the process $\{X_n\}_{n=0}^\infty$ a \textit{stochastic sequence}.

\begin{definition}[Filtrations]\label{def:3.2} Let $\mathcal{T}$ be $\mathbb{N}_0$ or $\mathbb{R}_+$. A \textit{filtration} on $(\Omega,\mathscr{F},\P)$ is a collection $\{\mathscr{F}_t\}_{t\in\mathcal{T}}$ indexed by elements $\mathcal{T}$ of increasing sub $\sigma$-algebras of $\mathscr{F}$, i.e. $\mathscr{F}_s\subset\mathscr{F}_t$ for all $s<t$.
\paragraph{Remark.} We can also define the limit of a filtration $\left\{\mathscr{F}_t\right\}_{t\in\mathcal{T}}$ by $\mathscr{F}_\infty=\sigma\left(\bigcup_{t\in\mathcal{T}}\mathscr{F}_t\right)$. If $\mathcal{T}=\mathbb{N}$, then a filtration $\{\mathscr{F}_n\}_{n=1}^\infty$ satisfies
\begin{align*}
	\mathscr{F}_0\subset\mathscr{F}_1\subset\mathscr{F}_2\subset\cdots\subset\mathscr{F}_n\subset\mathscr{F}_{n+1}\subset\cdots\subset\mathscr{F}_\infty\subset\mathscr{F}.
\end{align*}
If $\mathcal{T}=[0,\infty)$, then a filtration $\{\mathscr{F}_t\}_{t\geq 0}$ satisfies
\begin{align*}
	\mathscr{F}_0\subset\mathscr{F}_s\subset\mathscr{F}_t\subset\mathscr{F}_\infty\subset\mathscr{F},\ \forall t>s\geq 0.
\end{align*}
\end{definition}
\begin{definition}[Adaptation]\label{def:3.3} Let $\mathcal{T}$ be $\mathbb{N}_0$ or $\mathbb{R}_+$, and let $\{\mathscr{F}_t\}_{t\in\mathcal{T}}$ be a filtration on $(\Omega,\mathscr{F},\P)$. A stochastic process $\{X_t\}_{t\in\mathcal{T}}$ is said to be \textit{adapted to $\{\mathscr{F}_t\}_{t\in\mathcal{T}}$}, if $X_t$ is $\mathscr{F}_t$-measurable for each $t\in\mathcal{T}$.
\end{definition}

\paragraph{Remark.} A stochastic process $\{\mathscr{F}_t\}_{t\in\mathcal{T}}$ automatically induces a \textit{canonical filtration}
\begin{align*}
	\mathscr{F}_t = \sigma(\left\{X_s\right\}_{s\leq t}),\ t\in\mathcal{T}.
\end{align*}
It is the minimal sub $\sigma$-algebra where every $X_s$ with $s\leq t$ is measurable. We also call this the $\sigma$-algebra generated by $\{X_s\}_{s\leq t}$. Clearly, the process $\{\mathscr{F}_t\}_{t\in\mathcal{T}}$ is adapted to its canonical filtration.

\begin{definition}[Stopping time]\label{def:3.4} Let $\mathcal{T}$ be $\mathbb{N}_0$ or $\mathbb{R}_+$. A random variable $\tau:\Omega\to\overline{\mathcal{T}}:=\mathcal{T}\cup\{\infty\}$ is said to be a \textit{stopping time of the filtration $\{\mathscr{F}_t\}_{t\in\mathcal{T}}$} if $\{\tau\leq t\}\in\mathscr{F}_t$ for all $t\in\mathcal{T}$. Without ambiguity, if the filtration is fixed, we say that $\tau$ is a \textit{stopping time}.
\end{definition}

\paragraph{Remark.} If $\tau$ is a stopping time, then the set $\{\tau<t\}$ is also $\mathscr{F}_t$-measurable for all $t\in\mathcal{T}$, since
\begin{align*}
	\{\tau<t\}=\bigcup_{n=1}^\infty\left\{\tau\leq t-\frac{1}{n}\right\}.
\end{align*}
Furthermore,
\begin{align*}
	\{\tau=\infty\}=\left(\bigcup_{n=1}^\infty\{\tau\leq n\}\right)^c\in\mathscr{F}_\infty.
\end{align*}
We may modify the definition of stopping time in discrete case. If $\mathcal{T}=\mathbb{N}_0$, then $\tau:\Omega\to\overline{\mathbb{N}}_0$ is a stopping time of  the filtration $\{\mathscr{F}_n\}_{n=1}^{\infty}$ if and only if $\{\tau=n\}\subset\mathscr{F}_n$ for all $n\in\mathbb{N}_0$, since $\{\tau\leq n\} = \bigcup_{k=0}^n\{\tau=k\}.$

\begin{definition}[$\sigma$-algebra generated by a stopping time]\label{def:3.5} Let $\mathcal{T}$ be $\mathbb{N}_0$ or $\mathbb{R}_+$, and let $\tau$ be a stopping time of the filtration  $\{\mathscr{F}_t\}_{t\in\mathcal{T}}$. The \textit{$\sigma$-algebra generated by $\tau$} is defined as
\begin{align*}
	\mathscr{F}_\tau = \left\{A\in\mathscr{F}_\infty: A\cap\{\tau\leq t\}\in\mathscr{F}_t,\ \forall t\in\mathcal{T}\right\}.
\end{align*}
\end{definition}

\paragraph{Remark.} We need to check that $\mathscr{F}_\tau$ is a $\sigma$-algebra. Clearly, $\Omega\in\mathscr{F}_\tau$. If $A\in\mathscr{F}_\tau$, then for all $t\in\mathcal{T}$, we have $A^c\cap\{\tau\leq t\} = \{\tau\leq t\}\cap(A\cap\{\tau\leq t\})^c\in\mathscr{F}_t$, which implies $A^c\in\mathscr{F}_\tau$. Finally, given $\{A_n\}_{n=1}^\infty\subset\mathscr{F}_\tau$, we have
\begin{align*}
	\left(\bigcup_{n=1}^\infty A_n\right)\cap\{\tau\leq t\} = \bigcup_{n=1}^\infty \left(A_n\cap\{\tau\leq t\}\right)\in\mathscr{F}_t,\ \forall t\in\mathcal{T}.
\end{align*}
Then $\bigcup_{n=1}^\infty A_n\in\mathscr{F}_\tau$. Therefore $\mathscr{F}_\tau$ is a $\sigma$-algebra.

\paragraph{} Now we concentrate on the case $\mathcal{T}=\mathbb{R}_+$.

\begin{definition}[Right-continuity]\label{def:3.6}
Let $\{\mathscr{F}_t\}_{t\geq 0}$ be a filtration on $(\Omega,\mathscr{F},\P)$. For every $t\in\mathbb{R}_+$, define
\begin{align*}
	\mathscr{F}_{t+}=\bigcap_{s>t}\mathscr{F}_s,\quad\textit{and}\quad\mathscr{F}_{\infty+}=\mathscr{F}_\infty.
\end{align*}

Then $\mathscr{F}_{t+}$ is a $\sigma$-algebra, and the collection $\{\mathscr{F}_{t+}\}_{t\geq 0}$ is also a filtration on $(\Omega,\mathscr{F},\P)$. If $\mathscr{F}_t=\mathscr{F}_{t+}$ for all $t\in\mathbb{R}_+$, then the filtration $\{\mathscr{F}_t\}_{t\geq 0}$ is said to be \textit{right-continuous}. By construction, the filtration $\{\mathscr{F}_{t+}\}_{t\geq 0}$ is automatically right-continuous.
\end{definition}

\begin{definition}[Completeness]\label{def:3.7} Let $\{\mathscr{F}_t\}_{t\geq 0}$ be a filtration on $(\Omega,\mathscr{F},\P)$, and let $\mathscr{N}$ be the sets of all $(\mathscr{F}_\infty,\P)$-negligible sets, i.e. $A\in\mathscr{N}$ if there exists $A^\prime\in\mathscr{F}_\infty$ such that $A^\prime\supset A$ and $\P(A^\prime)=0$. The filtration $\{\mathscr{F}_t\}_{t\geq 0}$ is said to be \textit{complete} if $\mathscr{N}\subset\mathscr{F}_0$. 
\end{definition}

\paragraph{Remark.} If $\{\mathscr{F}_t\}_{t\geq 0}$ is not complete, we can complete it by letting $\mathscr{F}_t^\prime=\sigma\left(\mathscr{F}_t\cup\sigma(\mathscr{N})\right)$ for every $t\in\mathbb{R}_+$. Apply this completion procedure to the canonical filtration $\mathscr{F}_t=\sigma\left(\{X_s\}_{s\leq t}\right)$ of a stochastic process $\{X_t\}_{t\geq 0}$, we obtain the \textit{completed canonical filtration} of $\{X_t\}_{t\geq 0}$.

Let $\{\mathscr{F}_t\}_{t\geq 0}$ be a complete filtration on $(\Omega,\mathscr{F},\P)$. By definition, if two random variables $\xi\overset{a.s.}{=}\eta$, and $\xi$ is $\mathscr{F}_t$-measurable, then $\eta$ is also $\mathscr{F}_t$-measurable.

\begin{definition}[Measurability and progressiveness]\label{def:3.8} A stochastic process $\{X_t\}_{t\geq 0}$ over a metric space $(E,d)$ is said to be \textit{measurable} if the mapping
\begin{align*}
	(\omega,t)\mapsto X_t(\omega)
\end{align*}
defined on $\Omega\times\mathbb{R}_+$ equipped with the product $\sigma$-field $\mathscr{F}\otimes\mathscr{B}(\mathbb{R}_+)$ is measurable. In addition, we fix a filtration $\{\mathscr{F}_t\}_{t\geq 0}$ on $(\Omega,\mathscr{F},\P)$. If for each $t\geq 0$, the mapping
\begin{align*}
	(\omega,s)\mapsto X_s(\omega)
\end{align*}
defined on $\Omega\times[0,t]$ equipped with the product $\sigma$-field $\mathscr{F}_t\otimes\mathscr{B}([0,t])$ is measurable, then the process $\{X_t\}_{t\geq 0}$ is said to be \textit{progressive}.
\end{definition}

\paragraph{Remark.} By definition, a progressive process $\{X_t\}_{t\geq 0}$ is both adapted and measurable. In later discussion, we fix the filtration $\{\mathscr{F}_t\}_{t\geq 0}$ on a probability space $(\Omega,\mathscr{F},\P)$ as well as the state space $(E,d)$.

\begin{proposition}\label{prop:3.9} Let $\{X_t\}_{t\geq 0}$ be an adapted stochastic process. If $\{X_t\}_{t\geq 0}$ is \textbf{(sample) right-continuous}, i.e. for all $\omega\in\Omega$, the mapping $t\mapsto X_t(\omega)$ is right-continuous, then $\{X_t\}_{t\geq 0}$ is progressive. The same conclusion holds if one replaces right-continuous with left-continuous.
\end{proposition}
\begin{proof}
We only prove the case of sample-right-continuity. The case of sample-left-continuity is similar. Fix $t>0$. For each $n\in\mathbb{N}$, define
\begin{align*}
	X_s^{(n)} = X_{\frac{kt}{n}}\ \textit{if}\ s\in\left[\frac{(k-1)t}{n},\frac{kt}{n}\right),\ k\in\{1,\cdots,n\}\quad \textit{and}\quad X_t^{(n)}=X_t.
\end{align*}
The sample-right-continuity of $\{X_t\}_{t\geq 0}$ implies that for all $\omega\in\Omega$,
\begin{align*}
	\lim_{n\to\infty}X_s^{(n)}(\omega) = X_s(\omega),\ \forall s\in[0,t].
\end{align*}
Furthermore, for every Borel set $B\in\mathscr{B}(E)$,
\begin{align*}
	\left\{(\omega,s)\in\Omega\times[0,t]:X_s^{(n)}(\omega)\in B\right\} = \left(\bigcup_{k=1}^n\left\{X_{\frac{kt}{n}}(\omega)\in B\right\}\times\left[\frac{(k-1)t}{n},\frac{kt}{n}\right)\right)\cup\left(\{X_t\in B\}\times\{t\}\right).
\end{align*}
This belongs to the product $\sigma$-algebra $\mathscr{F}_t\otimes\mathscr{B}([0,t])$. Hence the mapping $(\omega,s)\mapsto X_s^{(n)}(\omega)$ is measurable on $(\Omega\times[0,t],\mathscr{F}_t\otimes\mathscr{B}([0,t]))$, and so is the pointwise limit $(\omega,s)\mapsto X_s(\omega)$.
\end{proof}

\paragraph{Remark.} If the filtration $\{\mathscr{F}_t\}_{t\geq 0}$ was complete, we would only require that the sample path $t\mapsto X_t(\omega)$ is left/right-continuous for $\P$-$a.e.$ $\omega\in\Omega$.

\begin{proposition}\label{prop:3.10} Write $\mathscr{G}_t=\mathscr{F}_{t+}$ for every $t\in[0,\infty]$.
\begin{itemize}
	\item[(i)] A random variable $\tau:\Omega\to[0,\infty]$ is a stopping time of the filtration $\{\mathscr{G}_t\}_{t\geq 0}$ if and only if $\{\tau <t\}\in\mathscr{F}_t$ for all $t>0$. This is equivalent to the condition that $\tau\wedge t$ is $\mathscr{F}_t$-measurable for all $t> 0$.
	\item[(ii)] Let $\tau$ be a stopping time of the filtration $\{\mathscr{G}_t\}_{t\geq 0}$. Then 
	\begin{align*}
		\mathscr{F}_{\tau+}:=\left\{A\in\mathscr{F}_\infty:A\cap\{\tau <t\}\in\mathscr{F}_t,\ \forall t> 0\right\}=\mathscr{G}_{\tau}.
	\end{align*}
\end{itemize}
\end{proposition}
\begin{proof}
(i) Assume $\{\tau<s\}\in\mathscr{F}_s$ for all $s>0$, and fix $t\geq 0$. Then for all $s>t$,
\begin{align*}
	\{\tau\leq t\}=\bigcap_{n\in\mathbb{N}:\,t+n^{-1}<s}\left\{\tau<t+\frac{1}{n}\right\}\in\mathscr{F}_s\ \Rightarrow\ \{\tau\leq t\}\in\bigcup_{s>t}\mathscr{F}_s=\mathscr{G}_t.
\end{align*}
Conversely, if $\tau$ is a stopping time of $\{\mathscr{G}_t\}_{t\geq 0}$, then for all $t>0$, we have
\begin{align*}
	\{\tau<t\}=\bigcup_{n\in\mathbb{N}:\,t-n^{-1}>0}^\infty\underbrace{\left\{\tau\leq t-\frac{1}{n}\right\}}_{\in\mathscr{G}_{t-n^{-1}}\subset\mathscr{F}_t}\in\mathscr{F}_t.
\end{align*}

If $\tau\wedge t$ is $\mathscr{F}_t$-measurable, we have $\{\tau\leq s\}\in\mathscr{F}_t$ for all $s<t$. Then we have $\{\tau<t\}\in\mathscr{F}_t$ by taking $s_n\nearrow t$ and $\{\tau<t\}=\bigcup_{n=1}^\infty\{\tau\leq s_n\}$. Conversely, if $\tau$ is a stopping time of $\{\mathscr{G}_n\}_{n\geq 0}$, we have $\{\tau\leq s\}\in\mathscr{G}_s\subset\mathscr{F}_t$ for all $s<t$, and $\tau\wedge t$ is thus $\mathscr{F}_t$-measurable.

(ii) By definition, $\mathscr{G}_\tau:=\left\{A\in\mathscr{F}_\infty: A\cap\{\tau\leq t\}\in\mathscr{G}_t,\ \forall t\geq 0\right\}$. If $A\cap\{\tau<t\}\in\mathscr{F}_t$ for all $t>0$, then
\begin{align*}
	A\cap\{\tau\leq t\} = \bigcap_{n\in\mathbb{N}:\,t+n^{-1}<s}\biggl(A\cap\left\{\tau <t+\frac{1}{n}\right\}\biggr)\in\mathscr{F}_s,\ \forall s>t\geq 0\ \Rightarrow\  A\cap\{\tau\leq t\}\in\mathscr{G}_t,\ \forall t\geq 0.
\end{align*}
Conversely, if $A\in\mathscr{G}_\tau$, we have
\begin{align*}
	A\cap\{\tau<t\} = \bigcup_{n\in\mathbb{N}:\,t-n^{-1}>0}^\infty\biggl(\underbrace{A\cap\left\{\tau\leq t-\frac{1}{n}\right\}}_{\in\mathscr{G}_{t-n^{-1}}\subset\mathscr{F}_t}\biggr)\in\mathscr{F}_t,\ \forall t>0.
\end{align*}
Hence the conclusion follows. 
\end{proof}

\begin{proposition}[Properties of stopping times]\label{prop:3.11} Let $\tau,\sigma$ be two stopping times of the filtration $\{\mathscr{F}_t\}_{t\geq 0}$.
\begin{itemize}
\item[(i)] $\tau$ is a stopping time of $\{\mathscr{F}_{t+}\}_{t\geq 0}$, and $\mathscr{F}_\tau\subset\mathscr{F}_{\tau+}$. If $\{\mathscr{F}_t\}_{t\geq 0}$ is right-continuous, we have $\mathscr{F}_\tau=\mathscr{F}_{\tau+}$.
\item[(ii)] If $\tau=t$ is a constant stopping time, then $\mathscr{F}_{\tau}=\mathscr{F}_t$, and $\mathscr{F}_{\tau+}=\mathscr{F}_{t+}$.
\item[(iii)] $\tau$ is $\mathscr{F}_\tau$-measurable.
\item[(iv)] Given $A\in\mathscr{F}_\infty$, define $$\tau^A(\omega)=\begin{cases}
	\tau(\omega),\ &\omega\in A,\\
	\infty,\ &\omega\notin A.
\end{cases}$$ 
Then $\tau^A\in\mathscr{F}_\tau$ if and only if $\tau^A$ is a stopping time.
\item[(v)] If $\sigma\leq\tau$, then $\mathscr{F}_\sigma\subset\mathscr{F}_{\tau}$, and $\mathscr{F}_{\sigma+}\subset\mathscr{F}_{\tau+}$.
\item[(vi)] All $\sigma\wedge\tau$, $\sigma\vee\tau$ and $\sigma+\tau$ are stopping times, and  $\{\sigma\leq\tau\},\{\sigma=\tau\}\in\mathscr{F}_{\sigma\wedge\tau}=\mathscr{F}_\sigma\cap\mathscr{F}_\tau$.
\item[(vii)] A function $\omega\mapsto Y(\omega)$ defined on $\{\tau<\infty\}$ is $\mathscr{F}_\tau$-measurable if and only if for each $t\geq 0$, the restriction of $Y$ to the set $\{\tau\leq t\}$ is $\mathscr{F}_t$-measurable.
\item[(viii)] If $(\tau_n)_{n=1}^\infty$ is a monotone sequence of increasing stopping times, then $\tau_\infty=\lim_{n\to\infty}\tau_n$ is a stopping time.
\item[(ix)] If $(\tau_n)_{n=1}^\infty$ is a monotone sequence of decreasing stopping times, then $\tau_\infty=\lim_{n\to\infty}\tau_n$ is a stopping time of the filtration $\{\mathscr{F}_{t+}\}_{t\geq 0}$, and
\begin{align*}
	\mathscr{F}_{\tau_\infty+}=\bigcap_{n=1}^\infty\mathscr{F}_{\tau_n+}.
\end{align*}
In addition, if $(\tau_n)_{n=1}^\infty$ is stationary, i.e. for each $\omega\in\Omega$, there exists $N_\omega\in\mathbb{N}$ such that $\tau(\omega)=\tau_n(\omega)$ for all $n\geq N_\omega$, then $\tau$ is a stopping time, and
\begin{align*}
	\mathscr{F}_{\tau_\infty}=\bigcap_{n=1}^\infty\mathscr{F}_{\tau_n}.
\end{align*}
\end{itemize}
\end{proposition}
\begin{proof}
(i) By Remark of \hyperref[def:3.4]{Definition 3.4} and \hyperref[prop:3.10]{Proposition 3.10}, $\tau$ is also a stopping time of $\{\mathscr{F}_{t+}\}_{t\geq 0}$. The statement $\mathscr{F}_\tau\subset\mathscr{F}_{\tau+}$ follows from $\mathscr{F}_t\subset\mathscr{G}_t$. (ii) immediately follows from definition.

\item (iii) For all $\alpha\in\mathbb{R}$, we have
\begin{align*}
	\{\tau\leq\alpha\}\cap\{\tau\leq t\} = \{\tau\leq\alpha\wedge t\}\in\mathscr{F}_t,\ \forall t\geq 0\ \Rightarrow\ \{\tau\leq\alpha\}\in\mathscr{F}_\tau.
\end{align*}

\item (iv) The result immediately follows from the definition of $\mathscr{F}_\tau$, since
\begin{align*}
	\{\tau^A\leq t\}=A\cap\{\tau\leq t\},\ \forall t\geq 0.
\end{align*}

\item (v) If $A\in\mathscr{F}_\sigma$, then $A\cap\{\sigma\leq t\}\in\mathscr{F}_t$ for all $t\geq 0$. Since $\sigma\leq\tau$, we have $\{\sigma\leq t\}\supset\{\tau\leq t\}$, and
\begin{align*}
	A\cap\{\tau\leq t\} = (A\cap\{\sigma\leq t\})\cap\{\tau\leq t\}\in\mathscr{F}_t.
\end{align*}

\item (vi) For all $t\geq 0$, we have
\begin{align*}
	\{\sigma\wedge\tau\leq t\}=\{\sigma\leq t\}\cup\{\tau\leq t\}\in\mathscr{F}_t,\quad \{\sigma\vee\tau\leq t\}=\{\sigma\leq t\}\cap\{\tau\leq t\}\in\mathscr{F}_t,\\
	\textit{and}\quad \{\sigma+\tau>t\} = \{\tau\geq t\}\cup\left(\bigcup_{q\in\mathbb{Q}\,\cap\,(0,t]}\{t-q\leq\tau <t\}\cap\{\sigma>q\}\right)\in\mathscr{F}_t.
\end{align*}
Hence $\sigma\wedge\tau$, $\sigma\vee\tau$ and $\sigma+\tau$ are stopping times. By (v), $\mathscr{F}_{\sigma\wedge\tau}\subset\mathscr{F}_\sigma\cap\mathscr{F}_\tau$. Conversely, if $A\in\mathscr{F}_\sigma\cap\mathscr{F}_\tau$, 
\begin{align*}
	A\cap\{\sigma\wedge\tau\leq t\} = (\underbrace{A\cap\{\sigma\leq t\}}_{\in\mathscr{F}_t})\cup(\underbrace{A\cap\{\tau\leq t\}}_{\in\mathscr{F}_t}) \in\mathscr{F}_t,\ \forall t\geq 0\ \Rightarrow\ A\in\mathscr{F}_{\sigma\wedge\tau}.
\end{align*}
Hence $\mathscr{F}_{\sigma\wedge\tau}=\mathscr{F}_\sigma\cap\mathscr{F}_\tau$. By Proposition \ref{prop:3.10} (i), for all $t\geq 0$, both $\sigma\wedge t$ and $\tau\wedge t$ are $\mathscr{F}_t$-measurable, and
\begin{align*}
	\{\sigma\leq\tau\}\cap\{\sigma\leq t\} = \left\{\sigma\leq t\right\}\cap\left\{\sigma\wedge t\leq\tau\wedge t\right\}\in\mathscr{F}_t\ &\Rightarrow\ \{\sigma\leq\tau\}\in\mathscr{F}_\sigma,\\
	\{\sigma\leq\tau\}\cap\{\tau\leq t\} = \left\{\sigma\leq t\right\}\cap\left\{\tau\leq t\right\}\cap\left\{\sigma\wedge t\leq\tau\wedge t\right\}\in\mathscr{F}_t\ &\Rightarrow\ \{\sigma\leq\tau\}\in\mathscr{F}_\tau.
\end{align*}
Then $\{\sigma\leq\tau\}\in\mathscr{F}_\sigma\cap\mathscr{F}_\tau=\mathscr{F}_{\sigma\wedge\tau}$, and $\{\sigma=\tau\}=\{\sigma\leq\tau\}\cap\{\sigma\geq\tau\}\in\mathscr{F}_{\sigma\wedge\tau}$.

\item (vii) We first assume that for each $t\geq 0$, the restriction $Y|_{\{\tau\leq t\}}$ is $\mathscr{F}_t$-measurable. Then for every Borel set $B\in\mathscr{B}(E)$, we have $\{Y\in B\}\cap\{\tau\leq t\}\in\mathscr{F}_t$. Since $Y$ is defined on $\{\tau<\infty\}$, we have $$\{Y\in B\}=\bigcup_{n=1}^\infty\left(\{Y\in B\}\cap\{\tau\leq n\}\right)\in\mathscr{F}_\infty.$$
Hence $\{Y\in B\}\in\mathscr{F}_\tau$. Conversely, if $Y$ is $\mathscr{F}_\tau$-measurable, then $\{Y\in B\}\cap\{\tau\leq t\}\in\mathscr{F}_t$ for all $t\geq 0$.

\item (viii) For every $t\geq 0$, $\{\tau_\infty\leq t\} = \bigcap_{n=1}^\infty\left\{\tau_n\leq t\right\}\in\mathscr{F}_t.$

\item (ix) For every $t\geq 0$, $\{\tau_\infty< t\} = \bigcup_{n=1}^\infty\left\{\tau_n< t\right\}\in\mathscr{F}_t$. Hence $\tau$ is a stopping time of $\{\mathscr{F}_{t+}\}_{t\geq0}$ by \hyperref[prop:3.10]{Proposition 3.10 (i)}. By (v), we have $\mathscr{F}_{\tau+}\subset\mathscr{F}_{\tau_n+}$ for each $n\in\mathbb{N}$. Conversely, if $A\in\mathscr{F}_{\tau_n+}$ for each $n\in\mathbb{N}$,
\begin{align*}
	A\cap\{\tau_\infty<t\} = \bigcup_{n=1}^\infty\left(A\cap\{\tau_n<t\}\right)\in\mathscr{F}_t,\ \forall t>0\ \Rightarrow\ A\in\mathscr{F}_{\tau_\infty +}.
\end{align*}
Hence $\mathscr{F}_{\tau_\infty+}=\bigcap_{n=1}^\infty\mathscr{F}_{\tau_n+}$. Furthermore, if $\tau_n$ is stationary, then $\{\tau_\infty\leq t\} = \bigcup_{n=1}^\infty\left\{\tau_n\leq t\right\}\in\mathscr{F}_t$.
Thus $\tau_\infty$ is a stopping time, and $\mathscr{F}_{\tau_\infty}\subset\mathscr{F}_{\tau_n}$ for each $n\in\mathbb{N}$ by (v). Conversely, if $A\in\mathscr{F}_{\tau_n}$ for each $n\in\mathbb{N}$,
\begin{align*}
	A\cap\{\tau_\infty\leq t\} = \bigcup_{n=1}^\infty\left(A\cap\{\tau_n\leq t\}\right)\in\mathscr{F}_t,\ \forall t>0\ \Rightarrow\ A\in\mathscr{F}_{\tau_\infty}.
\end{align*}
Hence $\bigcap_{n=1}^\infty\mathscr{F}_{\tau_n}=\mathscr{F}_{\tau_\infty}$.
\end{proof}

\begin{proposition}\label{prop:3.12} Let $X=\{X_t\}_{t\geq 0}$ be a progressive process of $\{\mathscr{F}_t\}_{t\geq 0}$. If $T$ is a stopping time of $\{\mathscr{F}_t\}_{t\geq 0}$, then the function $X_\tau:\omega\mapsto X_{\tau(\omega)}(\omega)$, defined on the set $\{\tau<\infty\}$, is $\mathscr{F}_\tau$-measurable.
\end{proposition}
\begin{proof}
By Proposition \ref{prop:3.11} (vii), it suffices to show that the restriction of $X_\tau$ to $\{\tau\leq t\}$ is $\mathscr{F}_t$-measurable for all $t\geq 0$. The restriction $X_\tau|_{\{\tau\leq t\}}$ is a composition of two measurable mappings:
\begin{align*}
	&\tau\wedge t\ \textit{is}\ \mathscr{F}_t\textit{-measurable}:\ (\{\tau\leq t\},\mathscr{F}_t)\to(\{F\leq t\}\times[0,t],\mathscr{F}_t\otimes\mathscr{B}([0,t])),\ \omega\mapsto \left(\omega,\tau(\omega)\wedge t\right),\\
	&X\ \textit{is progressive}:\ (\{\tau\leq t\}\times[0,t],\mathscr{F}_t\otimes\mathscr{B}([0,t]))\to(E,\mathscr{B}(E)),\ (\omega,s)\mapsto X_s(\omega).
\end{align*}
Hence $X_\tau|_{\{\tau\leq t\}}$ is $\mathscr{F}_t$-measurable, and the result follows.
\end{proof}
\paragraph{Remark.} We also consider the discrete case. Let $(X_n)_{n=0}^\infty$ be an adapted sequence. Then for all $B\in\mathscr{B}(E)$, we have $\{X_\tau\in B\}\cap\{\tau\leq t\} = \bigcup_{n=0}^t\{X_n\in B\}\cap\{\tau=n\}\in\mathscr{F}_t$ for every $t\in\mathbb{N}$. Then we have $\{X_\tau\in B\}\in\mathscr{F}_\tau$, and $X_\tau$ is always $\mathscr{F}_\tau$-measurable. 

\begin{proposition}\label{prop:3.13}. Let $\{X_t\}_{t\geq 0}$ be a adapted process taking values in $(E,d)$, and let $A\subset E$ be a measurable subset of $E$. The \textbf{hitting time (or début) of $A$} is defined as
\begin{align*}
	\tau_A=\inf\left\{t\geq 0:X_t\in A\right\}.
\end{align*}
Given a random time $\sigma:\Omega\to[0,\infty]$, the \textit{first hitting time of $A$ after $\sigma$} is defined as
\begin{align*}
	\tau_A^\sigma=\inf\left\{t>\sigma:X_t\in A\right\}.
\end{align*}
Note that we set $\inf\emptyset=\infty$. Then
\begin{itemize}
	\item[(i)] If $\{X_t\}_{t\geq 0}$ is sample-right-continuous and $G\subset E$ is an open set, then $\tau_G$ is a stopping time of the filtration $\{\mathscr{F}_{t+}\}_{t\geq 0}$. Furthermore, if $\sigma$ is a stopping time of the filtration $\{\mathscr{F}_{t+}\}_{t\geq 0}$, so is $\tau_G^\sigma$.
	\item[(ii)] If $\{X_t\}_{t\geq 0}$ is sample-continuous and $F\subset E$ is a closed set, then $\tau_F$ is a stopping time (of the filtration $\{\mathscr{F}_{t}\}_{t\geq 0}$). Furthermore, if $\sigma$ is a stopping time of the filtration $\{\mathscr{F}_{t+}\}_{t\geq 0}$, so is $\tau_F^\sigma$.
\end{itemize}
\end{proposition}
\begin{proof}
(i) Fix $t>0$. If $\tau_G(\omega)<t$, then there exists $\tau_G(\omega)<s<t$ such that $X_s(\omega)\in G$. Since $G$ is open, and $t\mapsto X_t(\omega)$ is right-continuous, we can choose a rational $q\in(s,t)$ such that $X_q(\omega)\in G$. Hence
\begin{align*}
	\{\tau_G < t\} = \bigcup_{q\in\mathbb{Q}\,\cap\,[0,t)}\{X_q\in G\}\in\mathscr{F}_t,\ \forall t>0.
\end{align*}

Then by Proposition \ref{prop:3.10} (i), $\tau_G$ is a stopping time of $\{\mathscr{F}_{t+}\}_{t\geq 0}$. Furthermore, if $\sigma$ is a stopping time of $\{\mathscr{F}_{t+}\}_{t\geq 0}$, we have
\begin{align*}
	\{\tau_G^\sigma<t\}&=\bigcup_{q\in(0,t)}\left(\{\sigma < q\}\cap\left\{\inf\{s\geq q:X_s\in G\}<t\right\}\right)\\
	&=\bigcup_{q\in(0,t)}\left(\{\sigma < q\}\cap\left(\bigcup_{r\in\mathbb{Q}\,\cap\,[q,t)}\{X_r\in G\}\right)\right)\in\mathscr{F}_t,\ \forall t>0.
\end{align*}

(ii) Fix $t\geq 0$. If $\tau_G(\omega)\leq t$, choose $s_n\searrow s:=\tau_G(\omega)$ such that $X_{s_n}\in F$. Since $t\mapsto X_t(\omega)$ is continuous and $F$ is closed, we have $X_{s_n}(\omega)\to X_s(\omega)\in F$. Hence
\begin{align*}
	\{\tau_F\leq t\}=\bigcup_{s\in[0,t]}\{X_s\in F\} = \left\{\inf_{q\in\mathbb{Q}\,\cap\,[0,t]} d(X_q,F)=0\right\}\in\mathscr{F}_t,\ \forall t\geq 0.
\end{align*}
where the second equality holds because $d(\cdot,F)$ is continuous, and the inclusion holds because $d(\cdot,F)$ is Borel-measurable and $X_q$ is $\mathscr{F}_t$-measurable for all $q\in\mathbb{Q}\cap[0,t]$, and countable infimum preserves measurability. Furthermore, if $\sigma$ is a stopping time of $\{\mathscr{F}_{t+}\}_{t\geq 0}$, we have
\begin{align*}
	\{\tau_F^\sigma<t\}&=\bigcup_{q\in(0,t)}\left(\{\sigma < q\}\cap\left\{\inf\{s\geq q:X_s\in F\}<t\right\}\right)\\
	&=\bigcup_{q\in(0,t)}\left(\{\sigma < q\}\cap\left\{\inf_{r\in\mathbb{Q}\,\cap\,[q,t]} d(X_r,F)=0\right\}\right)\in\mathscr{F}_t,\ \forall t>0.
\end{align*}
Therefore $\tau_F^\sigma$ is a stopping time of $\{\mathscr{F}_{t+}\}_{t\geq 0}$.
\end{proof}
\paragraph{Remark.} Similarly, given any Borel set $A\in\mathscr{B}(\mathbb{R})$, we can define the hitting time of $A$ associated to a discrete process $X=\{X_t\}_{t=0}^\infty$:
\begin{align*}
	\tau_A=\min\{n\in\mathbb{N}_0:X_n\in A\},\ \tau_A^\sigma=\min\{n\in\mathbb{N}_0:n>\sigma, X_n\in A\}.
\end{align*}

It is easy to show that $\tau_A$ is a stopping time for any measurable set $A$, since $\{\tau_A\leq n\}=\bigcup_{k=0}^n\{X_k\in A\}$. Furthermore, if $\sigma$ is a stopping time, then the first hitting time $\tau_A^\sigma$ after $\sigma$ is also a stopping time:
\begin{align*}
	\{\tau_A^\sigma\leq n\}=\bigcup_{k=0}^{n-1}\left(\{\sigma=k\}\cap\left\{k<\tau_A^\sigma\leq n\right\}\right) = \bigcup_{k=0}^{n-1}\left(\{\sigma=k\}\cap\left(\bigcup_{j=k+1}^n\{X_j\in A\}\right)\right)\in\mathscr{F}_n,\ \forall n\in\mathbb{N}_0.
\end{align*}

\paragraph{} Finally we introduce a technical lemma about stopping times which resembles the form of simple function approximation.
\begin{proposition}\label{prop:3.14} Let $\tau$ be a stopping time. If $\sigma:\Omega\to[0,\infty]$ is a $\mathscr{F}_\tau$-measurable random variable such that $\sigma\geq\tau$. Then $\sigma$ is also a stopping time. Furthermore, 
\begin{align*}
	\tau_n = \sum_{k=0}^\infty\frac{k+1}{2^n}\mathds{1}_{\{k2^{-n}<\tau\leq(k+1)2^{-n}\}}+\infty\mathds{1}_{\{\tau =\infty\}},\ n\in\mathbb{N}
\end{align*}
is a sequence of stopping times decreasing to $\tau$.
\end{proposition}
\begin{proof}
Since $\sigma$ is $\mathscr{F}_\tau$-measurable, we have $\{\sigma\leq t\}\in\mathscr{F}_\tau$, and $\{\sigma\leq t\}=\{\sigma\leq t\}\cap\{\tau\leq t\}\in\mathscr{F}_t$ for all $t\geq 0$. Hence $\sigma$ is also a stopping time. 

Note that $\tau_n(\omega)=\inf\{k2^{-n}:k2^{-n}\geq\tau(\omega),k\in\mathbb{Z}\}$. Then we have $\tau_n\searrow\tau$. Since $\tau_n$ is a measurable function of $\tau$, it is $\mathscr{F}_\tau$-measurable, hence a stopping time by the first assertion.
\end{proof}

\newpage
\subsection{Discrete-time Martingales}
\begin{definition}[Discrete-time martingales]\label{def:3.15} Let $(X_n)_{n=0}^\infty$ be a real-valued and $L^1$ process that is adapted to the filtration $\{\mathscr{F}_n\}_{n=0}^\infty$. Here $L^1$ means $\E\vert X_n\vert<\infty$ for all $n\geq 0$. Then
\begin{itemize} 
\item[(i)] $(X_n)_{n=0}^\infty$ is said to be a \textit{martingale} if $\E[X_n|\mathscr{F}_m]=X_m$ for all $n>m\geq 0$;
\item[(ii)] $(X_n)_{n=0}^\infty$ is said to be a \textit{supermartingale} if $\E[X_n|\mathscr{F}_m]\leq X_m$ for all $n>m\geq 0$;
\item[(iii)] $(X_n)_{n=0}^\infty$ is said to be a \textit{submartingale} if $\E[X_n|\mathscr{F}_m]\geq X_m$ for all $n>m\geq 0$;
\end{itemize}
All these notations depends on the choice of the filtration $\{\mathscr{F}_n\}_{n=0}^\infty$, which is fixed in later discussion.
\end{definition}
\paragraph{Remark.} (i) If $(X_n)_{n=0}^\infty$ is a submartingale, then $(-X_n)_{n=0}^\infty$ is a supermartingale. \vspace{0.1cm}

(ii) The set of all martingales in $\left(L^1(\Omega,\mathscr{F},\P)\right)^{\mathbb{N}_0}$ is a vector space.

\paragraph{Proposition 3.16.\label{prop:3.16}} Let $f:\mathbb{R}\to\mathbb{R}$ be a convex function such that $\E[f(X_n)]<\infty$ for all $n\in\mathbb{N}_0$.
\begin{itemize}
	\item[(i)] If $(X_n)_{n=0}^\infty$ is a martingale, then $(f(X_n))_{n=0}^\infty$ is a submartingale.
	\item[(ii)] If $f$ is monotone increasing and $(X_n)_{n=0}^\infty$ is a submartingale, then $(f(X_n))_{n=0}^\infty$ is a submartingale.
\end{itemize}
\paragraph{Remark.} This is an immediate corollary of conditional Jensen's inequality [Proposition \ref{prop:2.17} (v)]. Here are some useful corollaries:
\begin{itemize}
	\item[(i)] If $(X_n)_{n=0}^\infty$ is a martingale, then $(\vert X_n\vert)_{n=0}^\infty$ is a submartingale; 
	\item[(ii)] If $(X_n)_{n=0}^\infty$ is a submartingale, then $(X_n^+)_{n=0}^\infty$ is a submartingale.
\end{itemize}

\begin{definition}[Predictable processes]\label{def:3.17} A discrete process $(H_n)_{n=0}^\infty$ is said to be \textit{predictable} if $H_0$ is a constant and $H_n$ is $\mathscr{F}_{n-1}$-measurable for all $n\geq 1$.  We define the integral (or the \textit{martingale transform}) of $(H_n)_{n=0}^\infty$ with respect to an adapted process $(X_n)_{n=0}^\infty$ by
\begin{align*}
	(H\cdot X)_0 = H_0 X_0,\ \ (H\cdot X)_n = (H\cdot X)_{n-1}+H_n(X_n-X_{n-1}) = H_0X_0+\sum_{k=1}^n H_k(X_k-X_{k-1}),\ \forall n\in\mathbb{N}.
\end{align*}

Clearly, $(H\cdot X)_n$ is an adapted process. If $X_n$ is a martingale, so is $(H\cdot X)_n$. Moreover, if $X_n$ is a submartingale (or supermartingale) and $H_n$ is nonnegative, then $(H\cdot X)_n$ is a submartingale (or supermartingale).
\end{definition}

\begin{theorem}[Doob's decomposition theorem]
\label{thm:3.18} Let $(X_n)_{n=1}^\infty$ be a submartingale. Then there exists an increasing predictable $L^1$ process $(A_n)_{n=0}^\infty$ staring with $A_0=0$ and a martingale $(M_n)_{n=0}^\infty$ such that $X_n=M_n+A_n$ for each $n\geq 0$, and the decomposition is unique.
\end{theorem}
\begin{proof}
We first prove the existence. Define $M_0=X_0$, $A_0=0$ and 
\begin{align*}
	M_n=X_0+\sum_{k=1}^n\left(X_k - \E[X_k|\mathscr{F}_{k-1}]\right),\  A_n=\sum_{k=1}^n\left(\E[X_k|\mathscr{F}_{k-1}]-X_{k-1}\right),\ \forall n\geq 1.
\end{align*}
Then $(M_n)_{n=1}^\infty$ and $(A_n)_{n=1}^\infty$ are the desired processes. To prove uniqueness, let $X_n=M_n^\prime+A_n^\prime$ be another decomposition satisfying the conditions given. Then $Y_n=M_n-M_n^\prime=A_n - A_n^\prime$ is a martingale and a predictable $L^1$ sequence, which implies $Y_n=\E[Y_n|\mathscr{F}_{n-1}]=Y_{n-1}=\cdots = Y_0 = 0$. Hence $Y_n\equiv 0$, and the Doob's decomposition is unique.
\end{proof}
\paragraph{Remark.} We have a similar conclusion for supermartingales: If $(X_n)_{n=1}^\infty$ is a submartingale, then there exists an decreasing predictable $L^1$ process $(A_n)_{n=0}^\infty$ staring with $A_0=0$ and a martingale $(M_n)_{n=0}^\infty$ such that $X_n=M_n+A_n$ for each $n\geq 0$, and the decomposition is unique.

\paragraph{Theorem 3.19.\label{thm:3.19}} (Doob's optional stopping theorem for discrete-time submartingales). Let $(X_n)_{n=1}^\infty$ be a submartingale, and let $\tau$ be a bounded stopping time. Then \begin{itemize}
	\item[(i)] $\E[X_\tau]\geq \E[X_0]$;
	\item[(ii)] If $\tau$ is bounded by $N\in\mathbb{N}$, then $\E[X_N|\mathscr{F}_\tau]\geq X_\tau$;
	\item[(iii)] If $\sigma$ is another bounded stopping time and $\sigma\leq\tau$, then $\E[X_\tau|\mathscr{F}_\sigma]\geq X_\sigma$.
\end{itemize}
\begin{proof}
(i) By definition, we have $\{\tau\geq k\}=\{\tau\leq k-1\}^c\in\mathscr{F}_{k-1}$ for all $k\in\mathbb{N}$. Then
\begin{align*}
	\E[X_\tau]=\E\left[X_0+\sum_{k=1}^N(X_k-X_{k-1})\mathds{1}_{\{\tau\geq k\}}\right] &= \E[X_0]+\sum_{k=1}^N\E\left[(X_k-X_{k-1})\mathds{1}_{\{\tau\geq k\}}\right]\\
	&=\E[X_0]+\sum_{k=1}^N\E\bigl[\underbrace{\E[(X_k-X_{k-1})|\mathscr{F}_{k-1}]}_{\geq 0}\mathds{1}_{\{\tau\geq k\}}\bigr]\geq \E[X_0].
\end{align*}

(ii) If $A\in\mathscr{F}_\tau$, we have $A\cap\{\tau=n\}\in\mathscr{F}_n$ for all $N\in\mathbb{N}_0$, and
\begin{align*}
	\E[X_N\mathds{1}_A] &= \sum_{n=0}^N\E\left[X_N\mathds{1}_{A\,\cap\,\{\tau=n\}}\right]=\sum_{n=0}^N\E\left[\E\left[X_N|\mathscr{F}_n\right]\mathds{1}_{A\,\cap\,\{\tau=n\}}\right]\\
	&\geq \sum_{n=0}^N\E\left[X_n\mathds{1}_{A\,\cap\,\{\tau=n\}}\right] = \sum_{n=0}^N\E\left[X_\tau\mathds{1}_{A\,\cap\,\{\tau=n\}}\right] = \E[X_\tau\mathds{1}_A].
\end{align*}
Since $X_\tau$ is $\mathscr{F}_\tau$-measurable, we have $\E[X_N|\mathscr{F}_\tau]\geq X_\tau$.

\paragraph{Remark.} Similarly, if $(X_n)_{n=1}^\infty$ is a supermartingale, then we have $\E[X_\tau]\leq\E[X_0]$ and $X_\tau\geq\E[X_N|\mathscr{F}_\tau]$. Furthermore, if $(X_n)_{n=1}^\infty$ is a martingale, then we have $\E[X_\tau]=\E[X_0]$ and $X_\tau=\E[X_N|\mathscr{F}_\tau]$. 

\paragraph{} (iii) Since $\sigma\leq\tau\leq N$, we have $\mathscr{F}_\sigma\subset\mathscr{F}_\tau$. We use Doob's decomposition $X_t=M_t+A_t$ of submartingale, where $M_t$ is a martingale and $A_t$ is an increasing predictable sequence. By (ii),
\begin{align*}
	M_\sigma = \E[M_N|\mathscr{F}_\sigma] = \E[\E[M_N|\mathscr{F}_\tau]|\mathscr{F}_\sigma] = \E[M_\tau|\mathscr{F}_\sigma].
\end{align*}
Clearly, $A_\tau\geq A_\sigma$, and $\E[A_\tau|\mathscr{F}_\sigma]\geq\E[A_\sigma|\mathscr{F}_\sigma]=A_\sigma$. Hence $\E[X_\tau|\mathscr{F}_\sigma] = \E[M_\tau+A_\tau|\mathscr{F}_\sigma] \geq M_\sigma + A_\sigma = X_\sigma$.
\end{proof}

The following theorem gives a necessary and sufficient condition of an adapted integrable sequence to be a martingale.

\paragraph{Theorem 3.20.\label{thm:3.20}} Let $(X_n)_{n=0}^\infty$ be an adapted and $L^1$ sequence. Then $(X_n)_{n=0}^\infty$ is a martingale if and only if $\E[X_\tau]=E[X_0]$ for every bounded stopping time $\tau$.
\begin{proof}
``$\Rightarrow$'': By \hyperref[thm:3.19]{Theorem 3.19 (i)}.

``$\Leftarrow$'': Assume that $\E[X_\tau]=E[X_0]$ for every bounded stopping time $\tau$. To prove that $(X_n)_{n=0}^\infty$ is a martingale, we show that for each $n\in\mathbb{N}$,
\begin{align*}
	\E[X_n|\mathscr{F}_{n-1}]=X_{n-1} \Leftrightarrow\ \E[X_n\mathds{1}_A]=\E[X_{n-1}\mathds{1}_A],\ \forall A\in\mathscr{F}_{n-1}.
\end{align*}

Let $\tau=(n-1)\mathds{1}_{A}+n\mathds{1}_{A^c}$, so $\tau$ is a bounded stopping time, and $\E[X_0]=\E[X_\tau]=\E[X_{n-1}\mathds{1}_{A}]+\E[X_n\mathds{1}_{A^c}]$. Since $n$ is a constant stopping time, we have $\E[X_n]=\E[X_0]$. Then $\E[X_n\mathds{1}_{A}]=\E[X_{n-1}\mathds{1}_{A}]$.
\end{proof}

\paragraph{Proposition 3.21\label{prop:3.21}} (Maximal inequality). If $(X_n)_{n=0}^\infty$ is a submartingale, then for every $n\in\mathbb{N}$,
\begin{align*}
	\lambda\P\left(\max_{0\leq k\leq n}\vert X_k\vert>\lambda\right)\leq \E[\vert X_0\vert] + 2\E[\vert X_n\vert],\ \forall \lambda>0.\tag{3.1}\label{eq:3.1}
\end{align*}
In addition, if $(X_n)_{n=0}^\infty$ is nonnegative, then
\begin{align*}
	\lambda\P\left(X_n^*>\lambda\right)\leq \E[X_n\mathds{1}_{\{X_n^*>\lambda\}}]\leq\E[X_n],\ \forall \lambda>0,\tag{3.2}\label{eq:3.2}
\end{align*}
where $X_n^*=\max_{0\leq k\leq n} X_k$.
\begin{proof}
Let $\tau=n\wedge\min\{n\in\mathbb{N}_0:\vert X_n\vert>\lambda\}$. Then $\tau$ is a bounded stopping time. By Markov's inequality,
\begin{align*}
	\lambda\P\left(\max_{0\leq k\leq n}\vert X_k\vert>\lambda\right)=\lambda\P(\vert X_\tau\vert>\lambda)\leq\E\left[\vert X_\tau\vert\mathds{1}_{\{\vert X_\tau\vert>\lambda\}}\right]\leq\E[\vert X_\tau\vert].\tag{3.3}\label{eq:3.3}
\end{align*}
Note that both $(X_n)$ and $(X_n^+)$ are submartingales, and $\vert X_\tau\vert\leq 2X_\tau^+ - X_\tau$. By \hyperref[thm:3.19]{Theorem 3.19},
\begin{align*}
	\lambda\P\left(\max_{0\leq k\leq n}\vert X_k\vert>\lambda\right)\leq2\E[X_\tau^+]-\E
	[X_\tau]\leq 2\E[X_n^+]-\E[X_0]\leq \E
	[\vert X_0\vert] + 2\E\left[\vert X_n\vert\right].
\end{align*}
In addition, if $(X_n)_{n=0}^\infty$ is nonnegative, \hyperref[eq:3.2]{(3.2)} immediately follows from \hyperref[eq:3.3]{(3.3)} and \hyperref[thm:3.19]{Theorem 3.19}.
\end{proof}
\paragraph{Remark.} Note that the inequality \hyperref[eq:3.1]{(3.1)} holds for both submartingales and supermartingales. If $(X_n)_{n=0}^\infty$ is a martingale, then $(\vert X_n\vert)_{n=0}^\infty$ is a nonnegative submartingale, and we can use \hyperref[eq:3.2]{(3.2)}:
\begin{align*}
	\lambda\P\left(\max_{0\leq k\leq n} \vert X_k\vert>\lambda\right)\leq \E[\vert X_n\vert\mathds{1}_{\{\vert X_n\vert^*>\lambda\}}]\leq\E[\vert X_n\vert],\ \forall \lambda>0.
\end{align*}

\paragraph{Proposition 3.22\label{prop:3.22}} (Doob's $L^p$-inequality). If $(X_n)_{n=0}^\infty$ is a nonnegative submartingale or a martingale, and $1< p<\infty$, then
\begin{align*}
	\E\left[\max_{0\leq k\leq n}\vert X_n\vert^p\right]\leq\left(\frac{p}{p-1}\right)^p\E\left[\vert X_n\vert^p\right],\ \forall n\in\mathbb{N}.\tag{3.4}\label{eq:3.4}
\end{align*}
\begin{proof}
We use a corollary of Fubini's theorem: for $p>0$ and a nonnegative random variable $X\in L^p(\Omega,\mathscr{F},\P)$,
$$\E[X^p]=\int_0^\infty p\lambda^{p-1}\P(X>\lambda)\,\d \lambda.$$
Without loss of generality, let $(X_n)_{n=0}^\infty$ be a nonnegative submartingale, and $Y:=\max_{0\leq k\leq n}X_n$. Then
\begin{align*}
	\E[Y^p] &= \int_0^\infty p\lambda^{p-1}\P(Y>\lambda)\,\d \lambda\leq \int_0^\infty p\lambda^{p-2}\E[X_n\mathds{1}_{\{\lambda<Y\}}]\,\d \lambda\tag{By \hyperref[prop:3.21]{Proposition 3.21}}\\
	&=\E\left[X_n\int_0^\infty p\lambda^{p-2}\mathds{1}_{\{Y>\lambda\}}\,\d \lambda\right] = \left(\frac{p}{p-1}\right)\E\left[X_nY^{p-1}\right].\tag{3.5}\label{eq:3.5}
\end{align*}
Note that $q=p/(p-1)$ is the Hölder's conjugate of $p$, we have
\begin{align*}
	\E\left[X_nY^{p-1}\right]\leq\left(\E[X_n^p]\right)^{1/p}\left(\E[Y^{(p-1)q}]\right)^{1/q} = \Vert X_n\Vert_p\left(\E[Y^p]\right)^{1-1/p}.\tag{3.6}\label{eq:3.6}
\end{align*}
Combining \hyperref[eq:3.5]{(3.5)} and \hyperref[eq:3.6]{(3.6)}, we have $\Vert Y\Vert_p\leq\frac{p}{p-1}\Vert X_n\Vert_p$, which is \hyperref[eq:3.4]{(3.4)}.
\end{proof}

Now we discuss the convergence of martingales.
\paragraph{Definition 3.23\label{def:3.23}} (Upcrossing number). Given a real sequence $(x_n)_{n=0}^\infty$ and $a<b$, the \textit{upcrossing number} of this sequence along $[a,b]$ before time $n$, denoted by $U_{[a,b]}^x(n)$, is the largest integer $k$ such that there exists a strictly increasing sequence
\begin{align*}
	0\leq s_1<t_1<s_2<t_2<\cdots<s_k<t_k\leq n
\end{align*}
of integers such that $x_{s_j}\leq a$ and $x_{t_j}\geq b$ for all $j\in\{1,\cdots,k\}$. The \textit{total upcrossing number} $U_{[a,b]}^x(\infty)$ of this sequence along $[a,b]$ is defined as the limit of monotone increasing sequence $U_{[a,b]}^x(n)$, which possibly takes $\infty$.

\paragraph{Lemma 3.24.\label{lemma:3.24}} A real sequence $(x_n)_{n=0}^\infty$ converges if and only if $U_{[a,b]}^x(\infty)<\infty$ for all $a,b\in\mathbb{Q}$ with $a<b$.
\begin{proof}
Assume $x_\infty=\lim_{n\to\infty} x_n$ (possibly infinite) exists. Then for all $a,b\in\mathbb{Q}$ with $a<b$, we have either $a<x_\infty$ or $b>x_\infty$. For the first case, there exists $N_a\in\mathbb{N}$ such that $x_n>a$ for all $n\geq N_a$, which implies $U_{[a,b]}^x(\infty)\leq N_a/2<\infty$. The second case is similar.

Conversely, if the sequence has no limit, let $a:=\liminf_{n\to\infty}x_n<b:=\limsup_{n\to\infty}x_n$, and pick $p,q\in\mathbb{Q}$ such that $a<p<q<b$. Now we choose two sequences $\{s_j\}_{j=1}^\infty$ and $\{t_j\}_{j=1}^\infty$ recursively as follows. First we pick any $s_1\in\mathbb{N}_0$ such that $x_{s_1}<p$, then pick $t_1>s_1$ such that $x_{t_1}>q$. This is possible, because $\limsup_{n\to\infty} x_n>q$. Repeated this procedure, we choose $s_2>t_1$ such that $x_{s_2}<p$, and $t_2>s_2$ such that $x_{t_2}>q$, $\cdots$. By definition of upcrossing numbers, we have $U_{[p,q]}^x(\infty)=\infty$, a contradiction!
\end{proof}

\paragraph{Definition 3.25\label{def:3.25}} (Upcrossing number). Given an adapted sequence $(X_n)_{n=0}^\infty$, the associated \textit{upcrossing number} is define as $U_{[a,b]}^X(n):\omega\mapsto U_{[a,b]}^{X(\omega)}(n)$, where $n\in\overline{\mathbb{N}}$. It can be depicted by a sequence of stopping times. Let $\tau_0=-\infty$, and define
\begin{align*}
	\sigma_j=\min\left\{k\in\mathbb{N}_0:k>\tau_{j-1},\ X_k\leq a\right\},\ \tau_j=\min\left\{k\in\mathbb{N}_0:k>\sigma_j,\ X_k\geq b\right\},\ j\geq 1.
\end{align*}
Then the upcrossing number of $(x_n)_{n=0}^\infty$ along $[a,b]$ before time $n\in\mathbb{N}$ is $U_{[a,b]}^X(n)=\max\{j:\tau_j\leq n\}$, and $U_{[a,b]}^X(\infty)=\lim_{n\to\infty}U_{[a,b]}^X(n)=\max\{j:\tau_j<\infty\}=\min\{j:\tau_j=\infty\}$.

\paragraph{Remark.} The upcrossing number $U_{[a,b]}^X(n)$ is also a stopping time, since $\{U_{[a,b]}^X(n)\leq k\}=\{\tau_k\geq n\}\in\mathscr{F}_k$.

\paragraph{Proposition 3.26\label{prop:3.26}} (Doob's upcrossing inequality). If $(X_n)_{n=0}^\infty$ is a submartingale, then for all real numbers $a<b$ and all $n\in\mathbb{N}$,
\begin{align*}
	\E\left[U_{[a,b]}^X(n)\right]\leq\frac{\E[(X_n-a)^+ -(X_0-a)^+]}{b-a}.\label{eq:3.7}\tag{3.7}
\end{align*}
\begin{proof}
Define stopping times $\{\sigma_j\}_{j=1}^\infty$ and $\{\tau_j\}_{j=1}^\infty$ as in \hyperref[def:3.25]{Definition 3.25}. Then $\{U_{[a,b]}^X(n)\geq k\}=\{\tau_k\leq n\}$, and
\begin{align*}
	H_k = \begin{cases}
		1,\ &\textit{if}\ \sigma_j<k\leq \tau_j\ \textit{for some}\ j\in\mathbb{N},\\
		0,\ &\textit{otherwise}.\end{cases}
\end{align*}
defines a nonnegative predictable process, since $\{H_k=1\}=\left(\bigcup_{j=0}^{k-1}\{X_j\leq a\}\right)\cap\{X_{k-1}<b\}$.

Define $Y_n=(X_n-a)^+$. Then $Y_n$ is a nonnegative submartingale, and we have $U_{[a,b]}^X(n)=U_{[0,b-a]}^Y(n)$. By definition, $(H\cdot Y)_n$ satisfies
\begin{align*}
	(H\cdot Y)_n=0 + \sum_{k=1}^n H_n(Y_n-Y_{n-1}) \geq (b-a)U_{[0,b-a]}^Y(n)=(b-a)U_{[a,b]}^X(n)
\end{align*}
Note that $((1-H)\cdot Y)_n$ is also a submartingale. Then
\begin{align*}
	(b-a)\E\left[U_{[a,b]}^X(n)\right]\leq\E\left[(H\cdot Y)_n\right] = \E\left[Y_n - ((1-H)\cdot Y)_n\right]\leq\E[Y_n]-\E[(1-H_0)Y_0] = \E[Y_n]-\E[Y_0].
\end{align*}
This is indeed the inequality \hyperref[eq:3.7]{(3.7)}.
\end{proof}

Following is the first martingale convergence theorem of Doob's.

\paragraph{Theorem 3.27\label{thm:3.27}} (Doob's convergence theorem for discrete-time submartingales). If $(X_n)_{n=0}^\infty$ is a submartingale, and $\sup_{n\in\mathbb{N}}\E[X_n^+]<\infty$, then $X_\infty=\lim_{n\to\infty}X_n$ \textit{a.s.} exists, and $X_\infty\in L^1(\Omega,\mathscr{F},\P)$.
\begin{proof}
Let $M:=\sup_{n\in\mathbb{N}}\E[X_n^+]<\infty$. Then for all $a\in\mathbb{R}$,
\begin{align*}
	(x-a)^+\leq x^+ + a^-,\ \forall x\in\mathbb{R}\ \Rightarrow\ \E[(X_n-a)^+]\leq \E[X_n]+a^- \leq M+a^-,\ \forall n\in\mathbb{N}.
\end{align*}
By Fatou's lemma and \hyperref[prop:3.26]{Proposition 3.26}, the total upcrossing number $U_{[a,b]}^X(n)\nearrow U_{[a,b]}^X(\infty)$ satisfies
\begin{align*}
	\E\left[U_{[a,b]}^X(\infty)\right] \leq\liminf_{n\to\infty}\E\left[U_{[a,b]}^X(n)\right]\leq\liminf_{n\to\infty}\frac{\E[(X_n-a)^+]}{b-a}\leq \frac{M+\vert a\vert}{b-a}<\infty,\ \forall \mathbb{R}\ni b>a.
\end{align*}
Then for all real numbers $a<b$, we have
\begin{align*}
	\P\left(U_{[a,b]}^X(\infty)<\infty\right) = 1.
\end{align*}

Let $\Omega_0=\bigcap_{a,b\in\mathbb{Q}:a<b}\left\{U_{[a,b]}^X(\infty)<\infty\right\}\vspace{0.1cm}$, so $\P(C)=1$. By \hyperref[lemma:3.24]{Lemma 3.24}, $X_n(\omega)$ converges for each $\omega\in\Omega_0$, and we denote the converging point by $X_\infty(\omega)$. For $\omega\notin\Omega_0$, define $X_\infty(\omega)=0$. Then $X_n\to X_\infty\ a.s.$.

Now we prove that $X_\infty\in L^1(\Omega,\mathscr{F},\P)$. By Fatou's lemma, $\E[X_\infty^+]\leq\liminf_{n\to\infty}\E[X_n^+]\leq M$, and $\E[X_\infty^-]\leq\liminf_{n\to\infty}\E[X_n^-]=\liminf_{n\to\infty}\E[X_n^+ -X_n]\leq M-\E[X_0]<\infty$.
\end{proof}

\paragraph{Remark.} \hyperref[thm:3.27]{Theorem 3.27} does not imply $L^1$-convergence. As a counter example, consider the random walk $X_0=0,\ X_n=\sum_{k=1}^n\xi_k$, where $\{\xi_k\}_{k=1}^\infty$ are i.i.d. Rademacher variables. Fix $\mathscr{F}_n$ to be the canonical filtration of $(X_n)_{n=0}^\infty$, and define $\tau=\min\{n\in\mathbb{N}:X_n=1\}$, which is a stopping time. Then the stopped process $Y_n=X_{n\wedge\tau}$ is a submartingale. Since $\E[Y_n^+]\leq 1$, the sequence $Y_n$ converges $a.s.$. Furthermore, $Y_n\to Y_\infty=1\ a.s.$, because $Y_{n+1}=Y_n\pm 1$ once $Y_n<1$. On the other hand, $\E[Y_n]=\E[X_{n\wedge\tau}]=\E[X_0]=0$ for all $n\in\mathbb{N}_0$.

\paragraph{Theorem 3.28\label{thm:3.28}} (Doob's convergence theorem for uniformly integrable discrete-time martingales). Let $(X_n)_{n=0}^\infty$ be a martingale. The following are equivalent:
\begin{itemize}
	\item[(i)] The collection $\{X_n\}_{n\in\mathbb{N}_0}$ is uniformly integrable.
	\item[(ii)] $X_n$ converges $a.s.$ and in $L^1$-norm.
	\item[(iii)] $(X_n)_{n=0}^\infty$ is closed, i.e. there exists $Z\in L^1(\Omega,\mathscr{F},\P)$ such that $X_n=\E[Z|\mathscr{F}_n]$ for all $n\in\mathbb{N}_0$.
\end{itemize}
\begin{proof}
(i) $\Rightarrow$ (ii): By \hyperref[thm:1.72]{Theorem 1.72}, we have $\sup_{n\in\mathbb{N}_0}\E[\vert X_n\vert]<\infty$. By Doob's convergence theorem for discrete-time submartingales [\hyperref[thm:3.27]{Theorem 3.27}], there exists $X_\infty\in L^1(\Omega,\mathscr{F},\P)$ such that $X_n\to X_\infty\ a.s.$, which implies $X_n\overset{\P}{\to}X_\infty$. Since $X_n$ is uniformly integrable, by \hyperref[thm:1.73]{Theorem 1.73}, $\Vert X_n-X_\infty\Vert_1\to 0$. \vspace{0.1cm}

(ii) $\Rightarrow$ (iii): Fix $n\in\mathbb{N}_0$. Since $\E[\cdot|\mathscr{F}_s]$ is a bounded linear operator on $L^1(\Omega,\mathscr{F},\mathbb{R})$, we have
\begin{align*}
	\E[X_\infty|\mathscr{F}_n]=\lim_{m\to\infty}\E[X_m|\mathscr{F}_n]=X_n.
\end{align*}

(iii) $\Rightarrow$ (i) is an immediate corollary of the following STRONGER CLAIM. 
\end{proof}

\paragraph{Claim 3.28*.\label{claim:3.28}} Given $Z\in L^1(\Omega,\mathscr{F},\P)$, the following collection is uniformly integrable:
\begin{align*}
	\left\{\E[Z|\mathscr{G}]:\mathscr{G}\ \textit{is a sub-$\sigma$-algebra of}\ \mathscr{F}\right\}
\end{align*}
\begin{proof}
Since $Z$ is integrable, by \hyperref[thm:1.57]{Theorem 1.57}, for every $\epsilon>0$, there exists $\delta>0$ such that $\E[\vert Z\vert\mathds{1}_A]<\epsilon$ for all $A\in\mathscr{F}$ with $\P(A)<\delta$. Given $M>0$, and define $X_\mathscr{G}=\E[Z|\mathscr{G}],\ Y_\mathscr{G}=\E[\vert Z\vert |\mathscr{G}]$. Then $\vert X_\mathscr{G}\vert\leq Y_\mathscr{G}$, and
\begin{align*}
	\E[\vert X_\mathscr{G}\vert\mathds{1}_{\{\vert X_\mathscr{G}\vert>M\}}]\leq\E[Y_\mathscr{G}\mathds{1}_{\{Y_\mathscr{G}>M\}}] = \E[\vert Z\vert\mathds{1}_{\{Y_\mathscr{G}>M\}}].
\end{align*}
By Chebyshev inequality, $\mu(Y_\mathscr{G}>M)\leq \frac{1}{M}\E[Y_\mathscr{G}]=\frac{1}{M}\E[\vert Z\vert]$. Once $M>\E[\vert Z\vert]/\delta$, we have $\E[\vert Z\vert\mathds{1}_{\{Y_\mathscr{G}>M\}}]<\epsilon$. Note the choice of $M$ is independent of $\mathscr{G}$. Since $\epsilon>0$, we have
\begin{align*}
	0\leq\lim_{M\to\infty}\sup_{\mathscr{G}\subset\mathscr{F}}\E\left[\vert X_\mathscr{G}\vert\mathds{1}_{\{\vert X_\mathscr{G}\vert>M\}}\right]\leq \lim_{M\to\infty}\sup_{\mathscr{G}\subset\mathscr{F}}\E\left[\vert Z\vert\mathds{1}_{\{\vert Y_\mathscr{G}\vert>M\}}\right] = 0.
\end{align*}
Hence the given random variable collection is uniformly integrable.
\end{proof}

\paragraph{Remark.} Given $Z\in L^1(\Omega,\mathscr{F},\P)$, the martingale $X_n=\E[Z|\mathscr{F}_n]$ described in \hyperref[thm:3.28]{Theorem 3.28 (iii)} is also called a \textit{Lévy's martingale or Doob's process}. In this case, we also have $X_\infty=\E[Z|\mathscr{F}_\infty]\ a.s..$

This is easy to prove. By our proof of (ii) $\Rightarrow$ (iii), we have $\E[X_\infty|\mathscr{F}_n]= X_n = \E[Z|\mathscr{F}_n]$ for all $n\in\mathbb{N}_0$. Hence for all $A\in\bigcup_{n=0}^\infty\mathscr{F}_n$, we have $\E[X_\infty\mathds{1}_A]=\E[Z\mathds{1}_A]$. Note that $\bigcup_{n=0}^\infty\mathscr{F}_n$ is a $\pi$-system that generates $\mathscr{F}_\infty$. By \hyperref[lemma:1.22]{Lemma 1.22}, we have $\E[X_\infty\mathds{1}_A]=\E[Z\mathds{1}_A]$ for all $A\in\mathscr{F}_\infty$. Also, if $\{\mathscr{F}_n\}_{n=0}^\infty$ is a filtration on $(\Omega,\mathscr{F},\P)$, and $Z$ is an integrable random variable, then $$\E[Z|\mathscr{F}_n]\to\E[Z|\mathscr{F}_\infty]\ a.s.\ and\ in\ L^1.$$

\paragraph{} We have a stronger version of martingale convergence theorem in $L^p$ when $p>1$.
\paragraph{Theorem 3.29\label{thm:3.29}} (Convergence theorem for $L^p$-bounded discrete-time martingales). If $(X_n)_{n=0}^\infty$ is a martingale such that $\sup_{n\in\mathbb{N}_0}\E\left[\vert X_n\vert^p\right]<\infty$, where $p>1$, then $X_n$ converges $a.s.$ and in $L^p$-norm.
\begin{proof}
Let $Y=\sup_{n\in\mathbb{N}_0}\vert X_n\vert$ and $M=\sup_{n\in\mathbb{N}_0}\E\left[\vert X_n\vert^p\right]$. By \hyperref[thm:3.27]{Theorem 3.27}, there exists $X_\infty\in L^p(\Omega,\mathscr{F},\P)$ such that $X_n\to X_\infty\ a.s.$. By Doob's $L^p$-inequality [\hyperref[prop:3.22]{Proposition 3.22}] and monotone convergence theorem,
\begin{align*}
	\E\left[\max_{1\leq k\leq n}\vert X_k\vert^p\right]\leq\left(\frac{p}{p-1}\right)^p\E\left[\vert X_n\vert^p\right]\leq\left(\frac{p}{p-1}\right)^p M\ \Rightarrow\ \E[Y^p]\leq\left(\frac{p}{p-1}\right)^p M < \infty.
\end{align*}
Since $\vert X_n-X_\infty\vert\leq 2Y$, by Lebesgue dominated convergence theorem, $\Vert X_n-X_\infty\Vert_p\to 0$.
\end{proof}

\paragraph{Definition 3.30\label{def:3.30}} (Stopped processes). Let $(X_n)_{n=1}^\infty$ be an adapted process, and let $\tau$ be a stopping time. The \textit{stopped process} $(X_n^\tau)_{n=0}^\infty$ is defined by
\begin{align*}
	X_n^\tau(\omega)=X_{t\wedge\tau(\omega)}(\omega),\ \forall\omega\in\Omega.
\end{align*}
\paragraph{Remark.} Clearly, the stopped process $(X_n^\tau)_{n=0}^\infty$ is also adapted, and
\begin{align*}
	X_n^\tau = \sum_{k=0}^{n-1}X_k\mathds{1}_{\{\tau=k\}} + X_n\mathds{1}_{\{\tau\geq n\}}\ \Rightarrow\ \begin{cases}
		\E[\vert X_n^\tau\vert]\leq \sum_{k=0}^n\E[\vert X_n\vert],\\
		\E[X_n^\tau|\mathscr{F}_{n-1}] = \sum_{k=0}^{n-1}X_k\mathds{1}_{\{\tau=k\}} + \E[X_n|\mathscr{F}_{n-1}]\mathds{1}_{\{\tau\geq n\}}.
	\end{cases}
\end{align*}
It is seen that if $X_n$ is a martingale (or submartingale or supermartingale, respectively), so is $X_n^\tau$.

\paragraph{Theorem 3.31\label{thm:3.31}} (Optional stopping theorem). Let $(X_n)_{n=1}^\infty$ be a martingale, and let $\tau$ be a stopping time. If at least one of the following conditions holds:
\begin{itemize}
	\item[(i)] $\tau$ is a bounded stopping time;
	\item[(ii)] $\E[\tau]<\infty$, and there exists $c>0$ such that $\E[\left\vert X_{n+1}-X_n
	\right\vert|\mathscr{F}_n]\leq c$ for all $n\in\mathbb{N}_0$; or
	\item[(iii)] The stopped process $X_n^\tau$ is uniformly bounded, and $\tau<\infty\ a.s.$;
\end{itemize}
Then $X_\tau$ is $a.s.$ well-defined, and $\E[X_\tau]=\E[X_0]$.

\begin{proof}
Note that (i) is proved in \hyperref[thm:3.19]{Theorem 3.19 (i)}. To prove (ii) and (iii), we use the following fact:
\begin{align*}
	\E[X_n^\tau]=\E[X_{n\wedge\tau}]=\E[X_0],\ \forall n\in\mathbb{N}_0,
\end{align*}
which holds because $n\wedge\tau$ is a bounded stopping time. We write the stopped process as
\begin{align*}
	X_n^\tau = X_0 + \sum_{k=1}^{n\wedge\tau}(X_k - X_{k-1}),\ \forall n\in\mathbb{N}_0.
\end{align*}

If (ii) holds, we have $\vert X_n^\tau\vert \leq\vert X_0\vert + c\tau$  for all $n$, and $\E[\vert X_0\vert] + c\E[\tau]<\infty$.  If (iii) holds, choose $M>0$ such that $\vert X_n^\tau\vert\leq M$ for all $n$. Hence under either (ii) or (iii), the sequence $X_n^\tau$ is dominated by a integrable function, and $X_n^\tau\to X_n\ a.s.$. By Lebesgue dominated convergence theorem, we have $\E[X_n^\tau]\to \E[X^\tau] = \E[X_0]$.
\end{proof}

\paragraph{Theorem 3.32\label{thm:3.32}} (Doob's optional stopping theorem for uniformly integrable discrete-time martingales). Let $(X_n)_{n=1}^\infty$ be a uniformly integrable martingale, and $X_\infty=\lim_{n\to\infty}X_n\ a.s.$ and in $L^1$. If $\tau$ is a stopping time, then $X_\tau\in L^1(\Omega,\mathscr{F},\P)$ and $$X_\tau=\E[X_\infty|\mathscr{F}_\tau]$$
with the convention that $X_\tau=X_\infty$ on $\{\tau=\infty\}$. If $\sigma\leq\tau$ is another stopping time, then $\E[X_\tau|\mathscr{F}_\sigma]=X_\sigma$.

\begin{proof}
By \hyperref[thm:3.28]{Theorem 3.28}, we have $\E[X_\infty|\mathscr{F}_n]=X_n$. Then for all $A\in\mathscr{F}_\tau$, $A\cap\{\tau=n\}\in\mathscr{F}_n$, and
\begin{align*}
	\E[X_\infty\mathds{1}_A]&=\sum_{n=0}^\infty\E\left[X_\infty\mathds{1}_{A\cap\{\tau=n\}}\right] + \E[X_\infty\mathds{1}_{A\cap\{\tau=\infty\}}]\\
	&=\sum_{n=0}^\infty\E\left[X_n\mathds{1}_{A\cap\{\tau=n\}}\right] + \E[X_\infty\mathds{1}_{A\cap\{\tau=\infty\}}]\\
	&=\sum_{n=0}^\infty\E\left[X_\tau\mathds{1}_{A\cap\{\tau=n\}}\right] + \E[X_\tau\mathds{1}_{A\cap\{\tau=\infty\}}] = \E[X_\tau\mathds{1}_A].
\end{align*}

Since $X_\tau$ is $\mathscr{F}_\tau$-measurable, we have $\E[X_\infty|\mathscr{F}_\tau]=X_\tau$. Furthermore, if $\sigma\leq\tau$ is another stopping time, then $\mathscr{F}_\sigma\subset\mathscr{F}_\tau$, and $\E[X_\tau|\mathscr{F}_\sigma]=\E[\E[X_\infty|\mathscr{F}_\tau]|\mathscr{F}_\sigma]=\E[X_\infty|\mathscr{F}_\sigma]=X_\sigma$.
\end{proof}

\paragraph{Definition 3.33\label{def:3.33}} (Backward martingales). A \textit{backward filtration} is an increasing sequence of sub $\sigma$-algebras $\{\mathscr{F}_n\}_{n\in-\mathbb{N}_0}$ indexed by nonpositive integers, i.e. $\mathscr{F}_n\supset\mathscr{F}_{n-1}$ for all $n\in-\mathbb{N}_0$. Let $(Y_n)_{n\in-\mathbb{N}_0}$ be an adapted sequence of integrable variables indexed by nonpositive integers.
\begin{itemize}
	\item[(i)] $(Y_n)_{n\in-\mathbb{N}_0}$ is said to be a \textit{backward martingale}, if $\E[X_n|\mathscr{F}_m]= X_m$ for all $m<n\leq 0$.
	\item[(ii)] $(Y_n)_{n\in-\mathbb{N}_0}$ is said to be a \textit{backward submartingale}, if $\E[X_n|\mathscr{F}_m]\geq X_m$ for all $m<n\leq 0$.
	\item[(iii)] $(Y_n)_{n\in-\mathbb{N}_0}$ is said to be a \textit{backward supermartingale}, if $\E[X_n|\mathscr{F}_m]\leq X_m$ for all $m<n\leq 0$.
\end{itemize}
\paragraph{Remark.} Likewise, we define the limit of the backward filtration $\{\mathscr{F}_n\}_{n\in-\mathbb{N}_0}$ by $\mathscr{F}_{-\infty}=\bigcap_{n\in-\mathbb{N}_0}\mathscr{F}_n$.

\paragraph{Theorem 3.34\label{thm:3.34}} (Doob's convergence theorem for backward discrete-time submartingales). If $(X_n)_{n\in-\mathbb{N}_0}$ is a backward submartingale, and $\lim_{n\to-\infty}\E[X_n]>-\infty$, then $\{X_n\}_{n\in\mathbb{N}_0}$ is uniformly integrable. Furthermore, there exists $X_{-\infty}\in L^1\left(\Omega,\mathscr{F}_{-\infty},\P\right)$ such that $X_{-\infty}=\lim_{n\to-\infty}X_n\ a.s.$ and in $L^1$. Moreover, we have $\E[X_n|\mathscr{F}_{-\infty}]\geq X_{-\infty}$ for all $n\in-\mathbb{N}_0$.
\begin{proof}
Recall Doob's upcrossing inequality \hyperref[prop:3.26]{Proposition 3.26}. Since the sequence $\{X_n,X_{n+1},\cdots,X_0,X_0,\cdots\}$ is also a submartingale for all $n\in-\mathbb{N}_0$, we have
\begin{align*}
	\E\left[U_{[a,b]}^X(n)\right]\leq\frac{\E[(X_0-a)^+-(X_n-a)^+]}{b-a}\leq \frac{\E[\vert X_0\vert]+\vert a\vert}{b-a},\ \forall a<b.
\end{align*}
Since $U_{[a,b]}^X(n)\nearrow U_{[a,b]}^X(-\infty)$, by monotone convergence /theorem, we have $\P\left(U_{[a,b]}^X(-\infty)<\infty\right)=1$, and
\begin{align*}
	\P\left(\bigcup_{a,b\in\mathbb{Q}:a<b}\left\{U_{[a,b]}^X(-\infty)<\infty\right\}\right)=1
\end{align*}

A symmetric version of \hyperref[lemma:3.24]{Lemma 3.24} implies that $X_n$ converges $a.s.$ when $n\to-\infty$. Since every sequence $(X_k)_{k\leq n}$ is $\mathscr{F}_n$-measurable, the limit $X_{-\infty}$ is $\mathscr{F}_n$-measurable for each $n\in-\mathbb{N}_0$, hence $\mathscr{F}_{-\infty}$-measurable. Furthermore, since $\E[X_n]\searrow L:=\lim_{n\to-\infty}\E[X_n]>-\infty$, by Lebesgue dominated convergence theorem
\begin{align*}
	\E[\vert X_n\vert]=2\E[X_n^+] - \E[X_n] \leq 2\E[X_0^+] - L < \infty,\ \forall n\in-\mathbb{N}_0\ \Rightarrow\ \E[X_{-\infty}]=\lim_{n\to-\infty}\E[X_n]=L.
\end{align*}

Given $\epsilon>0$, we can choose $N\in-\mathbb{N}_0$ such that $\E[X_n]>\E[X_N]-\epsilon/4$ for all $n\leq N$. Then 
\begin{align*}
\E\left[\vert X_n\vert\mathds{1}_{\{\vert X_n\vert> M\}}\right] &= \E\left[X_n\mathds{1}_{\{X_n>M\}}\right] - \E[X_n] + \E\left[X_n\mathds{1}_{\{X_n\geq -M\}}\right]\\
&< \E\left[\E[X_N|\mathscr{F}_n]\mathds{1}_{\{X_n>M\}}\right] - \E[X_N] + \frac{\epsilon}{4} + \E\left[\E[X_N|\mathscr{F}_n]\mathds{1}_{\{X_n\geq -M\}}\right]\\
&\leq \E\left[X_N\mathds{1}_{\{\vert X_n\vert>M\}}\right] + \frac{\epsilon}{4}\\
&\leq\E\left[\vert X_N\vert\mathds{1}_{\{\vert X_n\vert >M,\vert X_N\vert>M/2\}}\right] + \E\left[\vert X_N\vert\mathds{1}_{\{\vert X_n\vert >M,\vert X_N\vert\leq M/2\}}\right] + \frac{\epsilon}{4}\\
&\leq \E\left[\vert X_N\vert\mathds{1}_{\{\vert X_N\vert>M/2\}}\right] + \frac{1}{2}\E\left[\vert X_n\vert\mathds{1}_{\{\vert X_n\vert >M\}}\right] + \frac{\epsilon}{4},\ \forall n\leq N.
\end{align*}
Hence $\E\left[\vert X_n\vert\mathds{1}_{\{\vert X_n\vert> M\}}\right]<\E\left[\vert X_N\vert\mathds{1}_{\{\vert X_N\vert>M/2\}}\right]+\epsilon/2$ for all $n\leq N$. Choose $M>0$ so that the first term is less than $\epsilon/2$. Hence $\{X_n\}_{n\leq N}$ is uniformly integrable, and so is $\{X_n\}_{n\in -\mathbb{N}_0}$. Therefore $\Vert X_n- X_{-\infty}\Vert_1\to 0$. Moreover, for all $A\in\mathscr{F}_{-\infty}\subset\bigcap_{n\in-\mathbb{N}_0}\mathscr{F}_n$,
\begin{align*}
	\E[X_n\mathds{1}_A]-\E[X_m\mathds{1}_A] = \E[X_n\mathds{1}_A] - \E[\E[X_n|\mathscr{F}_m]\mathds{1}_A] \geq \E[X_n\mathds{1}_A]-\E[X_n\mathds{1}_A]\geq 0,\ \forall m<n.
\end{align*}

Let $m\to-\infty$. By Lebesgue dominated convergence theorem, $\E[X_n\mathds{1}_A]\geq\E[X_{-\infty}\mathds{1}_A]$ for all $n\in-\mathbb{N}_0$. Hence we have $\E[X_n|\mathscr{F}_{-\infty}]\geq X_{-\infty}$. 
\end{proof}

\paragraph{Remark.} If $(X_n)_{n\in-\mathbb{N}_0}$ is a backward martingale, the requirement $\lim_{n\to-\infty}\E[X_n]=\E[X_0]>-\infty$ is automatically satisfied. Then we have $X_n\to X_{-\infty}\ a.s.$ and in $L^1$. Moreover, $\E[X_0|\mathscr{F}_{-\infty}]=X_{-\infty}$.

We also have an immediate corollary of this theorem: If $\mathscr{F}_n\searrow\mathscr{F}_{-\infty}$ is a backward filtration, and $Z$ is a integrable random variable, then $\E[Z|\mathscr{F}_n]\to \E[Z|\mathscr{F}_{-\infty}]\ a.s.$ and in $L^1$. 

\paragraph{} The convergence theorem of backward martingales also gives rise to the strong law of large numbers.

\paragraph{Lemma 3.35\label{lemma:3.35}} (Kolmogorov's $0$-$1$ law). Let $(X_n)_{n=1}^\infty$ be a sequence of independent random variables. Define the \textit{tail $\sigma$-algebra} $\mathscr{F}_{-\infty}$ as follows:
\begin{align*}
	\mathscr{F}_{-n}=\sigma(X_{n+1},X_{n+2},\cdots),\ \forall\ n\in\mathbb{N}_0\ \ \textit{and}\ \ \mathscr{F}_{-\infty}=\bigcap_{n\in\mathbb{N}_0}\mathscr{F}_{-n}.
\end{align*}
Then $\mathscr{F}_{-\infty}$ is $\P$-trivial, i.e. $\P(A)\in\{0,1\}$ for all $A\in\mathscr{F}_{-\infty}$.
\begin{proof}
Define $\mathscr{F}_0=\{\Omega,\emptyset\}$, and $\mathscr{F}_n=\sigma(X_1,\cdots,X_n)$ for all $n\in\mathbb{N}$. Then $\mathscr{F}_n\ind\mathscr{F}_{-n}$, and $\mathscr{F}_{-\infty}\subset\mathscr{F}_{-n}$ is independent of $\mathscr{F}_n$ for all $n\in\mathbb{N}_0$. Let $\mathscr{F}_\infty=\sigma\left(\bigcup_{n=0}^\infty\mathscr{F}_n\right)$, then $\mathscr{F}_{-\infty}$ is independent of $\mathscr{F}_\infty$. Meanwhile, $\mathscr{F}_{-\infty}\subset\mathscr{F}_\infty$ implies that $\mathscr{F}_{-\infty}$ is independent of itself, and the result follows. 
\end{proof}

\paragraph{Remark.} We give a brief interpretation of the independence between $$\mathscr{F}_n=\sigma\left(\bigcup_{k=0}^n\sigma(X_k)\right)\ \ and\ \  \mathscr{F}_{-n}=\sigma\left(\bigcup_{k=n+1}^\infty\sigma(X_k)\right).$$

Let $\mathscr{G}\subset\mathscr{F}$ be a $\sigma$-algebra independent of every member of collection $\{\mathscr{F_\alpha}\subset\mathscr{F}\}_{\alpha\in J}$ of $\sigma$-algebras. Then $\mathscr{G}$ is also independent of $\sigma\left(\bigcup_{\alpha\in J}\mathscr{F}_\alpha\right)$, which is the smallest $\sigma$-algebra containing every member of $\{\mathscr{F_\alpha}\}_{\alpha\in J}$. This is easy to prove: For every $A\in\mathscr{G}$, the class $\{B\in\mathscr{F}:\P(A\cap B)=\P(A)\P(B)\}$ is a $\lambda$-system that contains $\bigcup_{\alpha\in J}\mathscr{F}_\alpha$, which is a $\pi$-system. The result then follows from Sierpiński-Dynkin $\pi$-$\lambda$ theorem.\vspace{0.1cm}

Now let $\{\mathscr{F}_\alpha\}_{\alpha\in J}$ and $\{\mathscr{G}_\beta\}_{\beta\in I}$ be two collections of $\sigma$-algebras such that $\mathscr{F}_\alpha\ind\mathscr{G}_\beta$ for all $\alpha$ and $\beta$. By applying the above conclusion twice, we have $\sigma\left(\bigcup_{\alpha\in J}\mathscr{F}_\alpha\right)\ind\sigma\left(\bigcup_{\beta\in I}\mathscr{G}_\beta\right)$.

\paragraph{Theorem 3.36\label{thm:3.36}} (Kolmogorov's strong law of large numbers). Let $(X_n)_{n=1}^\infty$ be a sequence of i.i.d. random variables such that $\E[\vert X_1\vert]<\infty$. Then
\begin{align*}
	\overline{X}_n:=\frac{1}{n}\sum_{j=1}^n X_j \to \mu:=\E[X_1]\ a.s..
\end{align*}
\begin{proof}
Let $S_n=X_1+\cdots+X_n$, and $Y_{-n}=\overline{X}_n=S_n/n$. Let $\mathscr{F}_{-n}=\sigma(S_n,X_{n+1},X_{n+2},\cdots)\searrow\mathscr{F}_{-\infty}$. Then
\begin{align*}
	\E[Y_{-n}|\mathscr{F}_{-n-1}]&=\frac{1}{n}\E\left[S_{n+1}-X_{n+1}|\sigma(S_{n+1},X_{n+2},\cdots)\right]\\
	&=\frac{S_{n+1}}{n}-\frac{1}{n}\E\left[X_{n+1}|\sigma(S_{n+1},X_{n+2},\cdots)\right]=\frac{S_{n+1}}{n}-\frac{S_{n+1}}{n(n+1)}=Y_{-n-1},
\end{align*}
where the third equality holds because of the exchangeability of $X_1,\cdots,X_n$ conditioning on $S_{n+1}$. Then $(X_{-n})_{n\in\mathbb{N}}$ is a backward martingale. By \hyperref[thm:3.34]{Theorem 3.34}, $Y_{-n}\to Y_{-\infty}\ a.s.$ and in $L^1$, with $Y_{-\infty}=\E[X_1|\mathscr{F}_{-\infty}]$.\vspace{0.1cm}

Let $\mathscr{G}_{-k}=(X_{k+1},X_{k+2},\cdots)$. For all $k\in\mathbb{N}$, we have $S_k/n\to 0\ a.s.$ as $n\to\infty$. As a result, $(Y_{-n}-\frac{S_k}{n})_{n=k}^\infty$ is a $\mathscr{G}_{-k}$-measurable sequence that converges to $Y_{-\infty}\ a.s.$. Then we conclude that $Y_{-\infty}$ is a $\mathscr{G}_{-k}$-measurable function by redefining it on a negligible set if required, and $Y_{-\infty}$ is measurable on the tail $\sigma$-algebra $\mathscr{G}_{-\infty}$. By \hyperref[lemma:3.35]{Lemma 3.35}, we have $\P(Y_{-\infty}>r)\in\{0,1\}$ for all $r\in\mathbb{R}$. Let $\mu=\inf\{r:\P(Y_{-\infty}>r)=0\}$. Then
\begin{align*}
	\P(Y_{-\infty}>\mu)=\P\left(\bigcup_{n=1}^\infty\left\{Y_{-\infty}>\mu+\frac{1}{n}\right\}\right) = 0,\quad \textit{and}\quad
	\P(Y_{-\infty}\geq\mu)=\P\left(\bigcap_{n=1}^\infty\left\{Y_{-\infty}>\mu-\frac{1}{n}\right\}\right)=1.
\end{align*}
Hence $\P(Y_{-\infty}=\mu)=1$, and $\E[X_1]=\E[\E[X_1|\mathscr{F}_{-\infty}]]=\E[Y_{-\infty}]=\mu$.
\end{proof}

\newpage
\subsection{Continuous-time Martingales}
\paragraph{Definition 3.37\label{def:3.37}} (Continuous-time martingales). Let $(X_t)_{t\geq 0}$ be a real-valued and $L^1$ process that is adapted to the filtration $\{\mathscr{F}_t\}_{t\geq 0}$. Here $L^1$ means $\E[\vert X_t\vert]<\infty$ for all $t\in\mathbb{R}_+$. Then
\begin{itemize} 
	\item[(i)] $(X_t)_{t\geq 0}$ is said to be a \textit{martingale} if $\E[X_t|\mathscr{F}_s]=X_s$ for all $t>s\geq 0$;
	\item[(ii)] $(X_t)_{t\geq 0}$ is said to be a \textit{supermartingale} if $\E[X_t|\mathscr{F}_t]\leq X_s$ for all $t>s\geq 0$;
	\item[(iii)] $(X_t)_{t\geq 0}$ is said to be a \textit{submartingale} if $\E[X_t|\mathscr{F}_t]\geq X_s$ for all $t>s\geq 0$;
\end{itemize}
All these notations depends on the choice of the filtration $\{\mathscr{F}_t\}_{t\geq 0}$, which is fixed in later discussion.

\paragraph{Remark I.} Clearly, if $(X_t)_{t\geq 0}$ is a submartingale, then $(-X_t)_{t\geq 0}$ is a supermartingale.

\paragraph{Remark II.} Similar to \hyperref[prop:3.16]{Proposition 3.16}, we have an immediate corollary of conditional Jensen's inequality. Let $f:\mathbb{R}\to\mathbb{R}$ be a convex function such that $\E[f(X_t)]<\infty$ for all $t\in\mathbb{R}_+$.
\begin{itemize}
	\item[(i)] If $(X_t)_{t\geq 0}$ is a martingale, then $(f(X_t))_{t\geq 0}$ is a submartingale. Particularly, $(\vert X_t\vert)_{t\geq 0}$ is a submartingale.
	\item[(ii)] In addition, if $f$ is monotone increasing and $(X_t)_{t\geq 0}$ is a submartingale, $(f(X_t))_{t\geq 0}$ is a submartingale. Particularly, $(X_t^+)_{t\geq 0}$ is a submartingale.
\end{itemize}

\paragraph{Proposition 3.38.\label{prop:3.38}} Let $(X_t)_{t\geq 0}$ be a submartingale. Then for all $t\geq 0$,
\begin{align*}
	\sup_{0\leq s\leq t}\E[\vert X_s\vert] < \infty.
\end{align*}
\begin{proof}
Clearly, we have $\E[X_s]=\E[\E[X_s|\mathscr{F}_0]]\geq\E[X_0]$. On the other hand, since $(X_t^+)$ is also a submartingale, we have $\E[X_s^+]\leq\E[X_t^+]$ for all $0\leq s\leq t$. Note that $\vert x\vert=2x^+-x$. Hence we have
\begin{align*}
	\E[\vert X_s\vert] = 2\E[X_s^+]-\E[X_s] \leq 2\E[X_t^+] - \E[X_0]<\infty,\ \forall s\in[0,t].
\end{align*}
The result immediately follows.
\end{proof}

\paragraph{Proposition 3.39.\label{prop:3.39}} Let $(X_t)_{t\geq 0}$ be an $L^2$ martingale. Then for all reals $0\leq s < t$ and all finite partitions $s=t_0<t_1<\cdots<t_k=t$, we have
\begin{align*}
	\E\left[\left.\sum_{j=1}^k(X_{t_j}-X_{t_{j-1}})^2\right|\mathscr{F}_s\right] = \E\left[X_t^2-X_s^2|\mathscr{F}_s\right] = \E\left[(X_t-X_s)^2|\mathscr{F}_s\right].
\end{align*}
\begin{proof}
For each $j=1,\cdots,k$,
\begin{align*}
	\E\left[(X_{t_j}-X_{t_{j-1}})^2|\mathscr{F}_s\right]=\E\left[\left.\E\left[(X_{t_j}-X_{t_{j-1}})^2|\mathscr{F}_{t_{j-1}}\right]\right|\mathscr{F}_s\right]=\E\left[\left.\E\left[X_{t_j}^2-X_{t_{j-1}}^2|\mathscr{F}_{t_{j-1}}\right]\right|\mathscr{F}_s\right]
\end{align*}
Then the desired result follows by summing over $j$.
\end{proof}

Now we extend the inequalities in \hyperref[prop:3.21]{Proposition 3.21} and \hyperref[prop:3.22]{Proposition 3.22} to continuous-time martingales with continuous sample paths.

\paragraph{Proposition 3.40\label{prop:3.40}} (i) (Maximal inequality). If $(X_t)_{t\geq 0}$ is a sample-right-continuous submartingale, then for every $t>0$,
\begin{align*}
	\lambda\P\left(\sup_{0\leq s\leq t}\vert X_s\vert>\lambda\right)\leq \E[\vert X_0\vert] + 2\E[\vert X_t\vert],\ \forall \lambda>0.
\end{align*}
In addition, if $(X_t)_{t\geq 0}$ is nonnegative, then
\begin{align*}
	\lambda\P\left(X_t^*>\lambda\right)\leq\E[X_t],\ \forall \lambda>0,\ \textit{where}\ X_t^*=\sup_{0\leq s\leq t} X_s.
\end{align*}

(ii) (Doob's $L^p$-inequality). If $(X_t)_{t\geq 0}$ is a sample-right-continuous martingale, then for every $t>0$,
\begin{align*}
	\E\left[\sup_{0\leq s\leq t}\vert X_s\vert^p\right]\leq\left(\frac{p}{p-1}\right)^p\E\left[\vert X_t\vert^p\right],\ \forall p>1.
\end{align*}
\begin{proof}
We fix $t>0$ and take a countable dense subset $t\in D\subset\mathbb{R}$. If $f$ is a right-continuous function, we have 
\begin{align*}
	\sup_{s\in D\cap[0,t]} f(s) =\sup_{0\leq s\leq t}f(s)
\end{align*}

Here is a brief interpretation using diagonal trick. Let $M=\sup_{0\leq s\leq t}f(s)$, then we can find a sequence $s_n\in[0,t]$ such that $f(s_n)\nearrow M$. Since $f$ is right continuous, and $D\ni t$ is dense in $\mathbb{R}$, we can find a sequence $D\cap[0,t]\ni t_{nk}\searrow s_n$ such that $f(t_{nk})\to f(s_n)$ for every $n$. Then $f(t_{nn})\to M$, and $\sup_{s\in D\cap[0,t]}f(s)=M$.

Hence, by right-continuity of $s\mapsto X_s(\omega)$ and the fact that $t\in D$, we have $\sup_{s\in D\cap[0,t]}\vert X_s\vert=\sup_{0\leq s\leq t}\vert X_s\vert$. Furthermore, we can view $D\cap[0,t]$ as the union of an increasing sequence of partitions $D_k=\{t_0^k,t_1^k,\cdots,t_k^k\}$, where $0\leq t_0^k<t_1^k<\cdots<t_k^k=t$. \vspace{0.1cm}

(i) For each $k\in\mathbb{N}$, we can apply the maximal inequality [\hyperref[prop:3.21]{Proposition 3.21}] of discrete form on sequence $Y_n=X_{t_{n\wedge k}^k}$, which is a submartingale of the filtration $\mathscr{G}_n=\mathscr{F}_{t_{n\wedge k}^k}$:
\begin{align*}
	\lambda\P\left(\max_{s\in D_k}\vert X_s\vert>\lambda\right)\leq \E[\vert X_0\vert]+2\E[\vert X_t\vert],\ \forall k\in\mathbb{N},\ \lambda>0.
\end{align*}
Note that $\max_{s\in D_k}\vert X_s\vert\nearrow\sup_{s\in D\cap[0,t]}\vert X_s\vert=\sup_{0\leq s\leq t}\vert X_s\vert$ as $k\to\infty$. By monotone convergence theorem, 
\begin{align*}
	\lambda\P\left(\max_{s\in D_k}\vert X_s\vert>\lambda\right)\nearrow\lambda\P\left(\sup_{s\in[0,t]}\vert X_s\vert>\lambda\right)\leq \E[\vert X_0\vert]+2\E[\vert X_t\vert],\ \forall\lambda>0.
\end{align*}
The case of nonnegative submartingale is similar. \vspace{0.1cm}

(ii) Same as the proof of (i), we apply Doob's inequality [\hyperref[prop:3.22]{Proposition 3.22}] of discrete form:
\begin{align*}
	\E\left[\max_{s\in D_k}\vert X_k\vert^p\right]\leq\left(\frac{p}{p-1}\right)^p\E\left[\vert X_t\vert^p\right],\ \forall p >1.
\end{align*}
Since $\max_{s\in D_k}\vert X_k\vert\nearrow\sup_{0\leq s\leq t}\vert X_s\vert$, use monotone convergence theorem.
\end{proof}

\paragraph{Remark.} If $(X_t)_{t\geq 0}$ is a submartingale, then for any dense subset $D\subset\mathbb{R}$ and every $t>0$,
\begin{align*}
	\P\biggl(\sup_{s\in D\cap[0,t]}\vert X_s\vert>\lambda\biggr)\leq \frac{1}{\lambda}\left(\E[\vert X_0\vert] + 2\E[\vert X_t\vert]\right),\ \forall \lambda>0.
\end{align*}
Let $\lambda\to\infty$, we obtain that $\sup_{s\in D\cap[0,t]}\vert X_s\vert<\infty\ a.s.$ for all $t> 0$.

\paragraph{Definition 3.41\label{def:3.41}} (Upcrossing number). Given a function $f:E\to\mathbb{R}$ and $a<b$, where $E\subset\mathbb{R}$, the \textit{upcrossing number} of this sequence along $[a,b]$, denoted by $U_{[a,b]}^f(E)$, is the largest $k\in\mathbb{N}$ such that there exists a finite and strictly increasing sequence $s_1<t_1<s_2<t_2<\cdots<s_k<t_k$ of elements of $E$ such that $f(s_j)\leq a$ and $f(t_j)\geq b$ for all $j\in\{1,\cdots,k\}$. If there exists no such sequence, we take $U_{[a,b]}^f(E)=0$. If such sequence exists for all $k\in\mathbb{N}$, we take $U_{[a,b]}^f(E)=\infty$,

\paragraph{} Given a sequence $x_n\in\mathbb{R}$, we write $x_n\upuparrows x$ if $x_n\nearrow x$ and $x_n<x$ for all $n$. Also, $x_n\downdownarrows x$ if $x_n\searrow x$ and $x_n>x$ for all $n$. Following this notation, one can define the left and right limits of a function $f:E\to\mathbb{R}$:
\begin{align*}
	f(t-):=\lim_{s\upuparrows t}f(s),\ \ \textit{and}\ \ f(t+):=\lim_{s\downdownarrows t}f(s).
\end{align*}

The function $f:E\to\mathbb{R}$ is said to be \textit{càdlàg (French: continue à droite, limite à gauche)}, if for all $t\in\mathring{E}$, the left limit $f(t-)<\infty$ exists, and the right limit $f(t+)$ exists and equals $f(t)$.

\paragraph{Lemma 3.42.\label{lemma:3.42}} Let $D$ be a countable dense subset of $\mathbb{R}_+$, and $f:D\to\mathbb{R}$. Assume that for every $T\in D$, 
\begin{itemize}
	\item[(i)] the function $f$ is bounded on $D\cap[0,T]$, and 
	\item[(ii)] $U_{[a,b]}^f(D\cap[0,T])<\infty$ for all rationals $a<b$.\vspace{0.05cm}
\end{itemize} 
Then the right limit $f(t+)=\lim_{D\ni s\downdownarrows t}f(s)$ exists for all $t\in\mathbb{R}_+$, and the left limit $f(t-)=\lim_{D\ni s\upuparrows t}f(s)$ exists for all $t\in\mathbb{R}_{++}$. Furthermore, the function $g:\mathbb{R}^+\to\mathbb{R}$ defined by $g(t)=f(t+)$ is càdlàg.
\begin{proof}
We first fix $t\in\overline{\mathbb{R}}_+$, and prove that $f(t+)=\lim_{D\ni s\downdownarrows t}f(s)$ exists. Take $D\ni T>t$. By assumption (i), there exists $M>0$ such that $\vert f(t)\vert\leq M$ for all $t\in D\cap[0,T]$. Take a sequence in $s_n\in D\cap[0,T]$ such that $s_n\downdownarrows t$. By Heine-Borel theorem, every subsequence of $f(s_n)$ has a further subsequence that converges. We prove that all such subsequences converges to the same point, which implies that $f(s_n)$ converges.

Argue by contradiction. If there exists two subsequence $s_n$ and $t_n$ such that $f(s_n)\to a$ and $f(t_n)=b>a$, take two rationals $a<p<q<b$. Then for any $k\in\mathbb{N}$, we can find a $f(t_{n_1})\geq q$, and $s_{n_1}<t_{n_1}$ such that $f(s_{n_1})\leq p$, and $t_{n_2}<s_{n_1}$ such that $f(t_{n_2})\geq q$, $\cdots$, and $s_{n_k}<t_{n_k}$ such that $f(s_{n_k})\leq p$. Thus we obtain an upcrossing sequence $s_{n_k}<t_{n_k}<s_{n_{k-1}}<y_{n_{k-1}}<\cdots<s_{n_1}<t_{n_1}$
of elements of $D\cap[0,T]$. Therefore, $U_{[p,q]}(D\cap[0,T])>k$ for all $k\in\mathbb{N}$, a contradiction to (ii)!

As a result, all such sequences $D\cap[0,T]\ni s_n\downdownarrows t$ converges. They should converge to the same point. Otherwise, we can construct a sequence not converging by interlacing two sequences that converges to distinct points. Therefore, the right limit $f(t+)=\lim_{D\ni s\downdownarrows t}f(s)$ exists for all $t\in\mathbb{R}_+$. Similarly, we can prove that the left limit $f(t-)=\lim_{D\ni s\upuparrows t}f(s)$ exists for all $t\in\mathbb{R}_{++}$.\vspace{0.1cm}

Now we prove $g(t)=f(t+)$ is càdlàg. Given $\epsilon>0$, we take $\delta>0$ such that $\vert f(s)-f(t-)\vert <\epsilon$ for all $s\in(t-\delta,t)$, and $\vert f(r)-f(t+)\vert <\epsilon$ for all $r\in(t,t+\delta)$.  Take $r_n\downdownarrows r\in(t,t+\delta)$, and $s_n\downdownarrows s\in(t-\delta,t)$. Then
\begin{align*}
	\vert g(r)-f(t+)\vert = \lim_{n\to\infty}\vert f(r_n)-f(t+)\vert < \epsilon,\ \ \textit{and}\ \ \vert g(s)-f(t-)\vert = \lim_{n\to\infty}\vert f(s_n)-f(t-)\vert < \epsilon.
\end{align*}
Hence $\lim_{r\downdownarrows t}g(r)=f(t+)$, $\lim_{s\upuparrows t}g(s)=f(t-)$, and $g$ is càdlàg.
\end{proof}

\paragraph{Theorem 3.43.\label{thm:3.43}} Let $(X_t)_{t\geq 0}$ be a submartingale, and let $D$ be a countable dense
subset of $\mathbb{R}_+$.
\begin{itemize}
\item[(i)] For $\P$-$a.e.\ \omega\in\Omega$, the restriction of $t\mapsto X_t(\omega)$ to $D$ has a right limit
\begin{align*}
	X_{t+}(\omega)=\lim_{D\ni s\downdownarrows t} X_s(\omega),\ \forall t\in\mathbb{R}_+
\end{align*}
and a left limit
\begin{align*}
	X_{t-}(\omega)=\lim_{D\ni s\upuparrows t} X_s(\omega),\ \forall t\in\mathbb{R}_{++}.
\end{align*}
\item[(ii)] For every $t\in\mathbb{R}_+$, $X_{t+}\in L^1(\Omega,\mathscr{F}_{t+},\P)$, and $\E[X_{t+}|\mathscr{F}_t]\geq X_t$ with equality holds if the mean function $t\mapsto\E[X_t]$ is right-continuous (in particular, if $(X_t)_{t\geq 0}$ is a martingale). The process $(X_{t+})_{t\geq 0}$ is a submartingale with respect to the filtration $(\mathscr{F}_{t+})_{t\geq 0}$.
\end{itemize}
\paragraph{Remark.} In (ii), if $X_{t+}$ is undefined on a negligible set $N$, we can just take $X_{t+}(\omega)=0$ for $\omega\in N$.
\begin{proof}
(i) Fix $T\in D$. Then $\sup_{s\in D\cap[0,T]}\vert X_s\vert<\infty\ a.s.$. As in the proof of \hyperref[prop:3.40]{Proposition 3.40}, we take an sequence $D_k$ increasing to $D\cap[0,T]$. Using Doob's upcrossing inequality [\hyperref[prop:3.26]{Proposition 3.26}] and monotone convergence theorem, for all $a<b$, we have
\begin{align*}
	\E\left[U_{[a,b]}^X(D\cap[0,T])\right]\leq\frac{\E\left[(X_T-a)^+-(X_0-a)^+\right]}{b-a}<\infty\ \ \Rightarrow\ \ U_{[a,b]}^X(D\cap[0,T])<\infty\ a.s..
\end{align*}
Set the negligible set $N$ as
\begin{align*}
	N=\bigcup_{T\in D}\left(\left\{\sup_{s\in D\cap[0,T]}\vert X_s\vert=\infty\right\}\cap\left(\bigcup_{a,b\in\mathbb{Q}}\left\{U_{[a,b]}^X(D\cap[0,T])=\infty\right\}\right)\right)\ \ \Rightarrow\ \ \P(N)=0.\tag{3.8}\label{eq:3.8}
\end{align*}
Outside $N$, the assumptions in \hyperref[lemma:3.42]{Lemma 3.42} are satisfied, and the result follows.\vspace{0.1cm}

(ii) We supplement the definition $X_{t+}(\omega)=0$ if $\lim_{D\ni s\downdownarrows t} X_s(\omega)$ does not exist, which occurs negligibly. Then $X_{t+}$ is $\mathscr{F}_{t+}$-measurable. Fix $t\in\mathbb{R}_+$, and choose $t_n\downdownarrows t$. Then we have $X_{t_n}\to X_{t_+}\ a.s.$. Set $Y_n=X_{t_{-n}}$ for all $n\in-\mathbb{N}_0$. Then $Y_n$ is a backward submartingale with respect to the backward filtration $\mathscr{G}_n=\mathscr{F}_{t_{-n}}$:
\begin{align*}
	\E[Y_n|\mathscr{G}_{n-1}] = \E[X_{t_{-n}}|\mathscr{F}_{t_{-(n-1)}}] \geq X_{t_{-(n-1)}} = Y_{n-1},\ \forall n\in-\mathbb{N}_0.
\end{align*}

By \hyperref[prop:3.38]{Proposition 3.38}, we have $\sup_{s\in D\cap[0,T]}\E[\vert X_s\vert]<\infty$, which implies $\lim_{n\to-\infty}\E[Y_n]>-\infty$. Using Doob's convergence theorem for discrete-time backward submartingales [\hyperref[thm:3.34]{Theorem 3.34}], we have $X_{t_n}\to X_{t+}$ in $L^1$, and $X_{t+}\in L^1(\Omega,\mathscr{F}_{t+},\P)$. Due to convergence in $L^1$, we have
\begin{align*}
	\E[X_{t+}|\mathscr{F}_t]=\lim_{n\to\infty}\E[X_{t_n}|\mathscr{F}_t]\geq X_t,\quad \textit{and}\quad \E[X_{t+}]=\lim_{n\to\infty}\E[X_{t_n}].
\end{align*}

Note the first equality holds because the conditional expectation operator $\E[\cdot|\mathscr{F}_t]$ is a bounded linear operator on $L^1(\Omega,\mathscr{F},\P)$. In addition, if the mean function $s\mapsto \E[X_s]$ is right-continuous, the second equality implies $\E[X_{t+}]=\E[X_t]$, which requires $\E[X_{t+}|\mathscr{F}_t]=X_t$.

Let $s<t$, and take $s_n\downdownarrows s$ such that $s_n\leq t_n$. Then $X_{s_n}\to X_{s+}\ a.s.$ and in $L^1$. Moreover, if $A\in\mathscr{F}_{s+}$, 
\begin{align*}
	\E[X_{t+}\mathds{1}_A] = \lim_{n\to\infty}\E[X_{t_n}\mathds{1}_A]\geq \lim_{n\to\infty}\E[X_{s_n}\mathds{1}_A] = \E[X_{s+}\mathds{1}_A].
\end{align*}

Since $X_{s+}$ is $\mathscr{F}_{s+}$-measurable, we have $\E[X_{t+}|\mathscr{F}_{s+}]\geq X_{s+}$. Therefore, $(X_{t+})_{t\geq 0}$ is a submartingale with respect to the filtration $(\mathscr{F}_{t+})_{t\geq 0}$.
\end{proof}

\paragraph{Theorem 3.44.\label{thm:3.44}} Let $(\mathscr{F}_t)_{t\geq 0}$ be a right-continuous and complete filtration. Let $(X_t)_{t\geq 0}$ be a submartingale such that the mean function $t\mapsto \E[X_t]$ is right continuous. Then $(X_t)_{t\geq 0}$ has an $a.s.$ modification with càdlàg sample paths, which is also a submartingale with respect to $(\mathscr{F}_t)_{t\geq 0}$.
\begin{proof}
Let $D$ be a countable subset of $\mathbb{R}_+$, and let $N$ be the negligible set defined in \hyperref[eq:3.8]{(3.8)}. We take $Y_t:=X_{t+}$ with the refinement $Y_t(\omega)=0$ for $\omega\in N$. By \hyperref[lemma:3.42]{Lemma 3.42}, the sample paths of $(Y_t)_{t\geq 0}$ are càdlàg.

Since $X_{t+}$ is $\mathscr{F}_t$-measurable by right-continuity of $(\mathscr{F}_t)_{t\geq 0}$, and since the negligible set $N$ falls in all $\mathscr{F}_t$ by completeness of $(\mathscr{F}_t)_{t\geq 0}$, the function $Y_t$ is $\mathscr{F}_t$-measurable. Furthermore,
\begin{align*}
	X_t = \E[X_{t+}|\mathscr{F}_t] = X_{t+} \overset{a.s.}{=} Y_t,\ \forall t\in\mathbb{R}_+.
\end{align*}
Hence $(Y_t)_{t\geq 0}$ is an $a.s.$ modification of $(X_t)_{t\geq 0}$, which is adapted to the filtration $(\mathscr{F}_t)_{t\geq 0}$. Furthermore, we have $\E[Y_t|\mathscr{F}_s]=\E[X_t|\mathscr{F}_s]\geq X_s=Y_s$ for all $t>s\geq 0$. Hence $(Y_t)_{t\geq 0}$ is also a submartingale.
\end{proof} 

\paragraph{Theorem 3.45\label{thm:3.45}} (Doob's first martingale convergence theorem). If $(X_t)_{t\geq 0}$ is a sample-right-continuous submartingale, and $\sup_{t\geq 0}\E[X_t^+]<\infty$, then $X_\infty=\lim_{t\to\infty}X_n$ \textit{a.s.} exists, and $X_\infty\in L^1(\Omega,\mathscr{F},\P)$.
\begin{proof}
Let $D$ be a countable subset of $\mathbb{R}_+$, and let $M:=\sup_{t\geq 0}\E[X_t^+]<\infty$. For all $a<b$, we can follow the proof of \hyperref[thm:3.43]{Theorem 3.43 (i)} and use monotone convergence theorem:
\begin{align*}
	\E\left[U_{[a,b]}^X(D\cap[0,T])\right]\leq\frac{\E[(X_T-a)^+-(X_0-a)^+]}{b-a}\leq\frac{M+\vert a\vert}{b-a} \quad\Rightarrow\quad \E\left[U_{[a,b]}^X(D)\right]\leq\frac{M+\vert a\vert}{b-a} < \infty.
\end{align*}
Hence we have
\begin{align*}
	U_{[a,b]}^X(D)<\infty\ a.s.\ \forall a,b\in\mathbb{Q}\ with\ a<b\quad\Rightarrow\quad X_\infty=\lim_{D\ni t\to\infty} X_t\in[-\infty,\infty]\ a.s.\ exists.
\end{align*}
We can further exclude values $\infty$ and $-\infty$ $\P$-$a.e.$, because the Fatou's lemma gives
\begin{align*}
	\E[X_\infty^+]\leq\liminf_{D\ni t\to\infty}\E[X_t^+]\leq M,\quad \E[X_\infty^-]\leq\liminf_{D\ni t\to\infty}\E[X_t^-]=\liminf_{D\ni t\to\infty}\E[X_t^+-X_t]\leq M-\E[X_0].
\end{align*}
Hence $X_\infty\in L^1(\Omega,\mathscr{F},\P)$. Furthermore, since $(X_t)_{t\geq 0}$ has right-continuous sample paths, we can drop the restriction $D\ni t$ in our limit.
\end{proof}

Similarly, we need uniform integrability of martingale to obtain convergence in $L^1$.

\paragraph{Theorem 3.46\label{thm:3.46}} (Doob's second martingale convergence theorem). Let $(X_t)_{t\geq 0}$ be a sample-right-continuous martingale. The following are equivalent:
\begin{itemize}
	\item[(i)] The collection $\{X_t\}_{t\in\mathbb{R}_+}$ is uniformly integrable.
	\item[(ii)] $X_t$ converges $a.s.$ and in $L^1$-norm.
	\item[(iii)] $(X_t)_{t\geq 0}$ is closed, i.e. there exists $Z\in L^1(\Omega,\mathscr{F},\P)$ such that $X_t=\E[Z|\mathscr{F}_t]$ for all $t\in\mathbb{R}_+$.
\end{itemize}
\begin{proof}
(i) $\Rightarrow$ (ii): By \hyperref[thm:3.45]{Theorem 3.45}, the limit $X_\infty=\lim_{t\to\infty} X_t\ a.s.$ exists. Since $\{X_t\}_{t\in\mathbb{R}_+}$ is uniformly integrable, convergence in $L^1$ also holds by \hyperref[thm:1.73]{Theorem 1.73}.

(ii) $\Rightarrow$ (iii) follows from the continuity of conditional expectation operator on $L^1(\Omega,\mathscr{F},\P)$.

(iii) $\Rightarrow$ (i) follows from \hyperref[claim:3.28]{Claim 3.28*}.
\end{proof}
\paragraph{Remark.} If (i)-(iii) are satisfied, the limit $X_\infty=\lim_{t\to\infty} X_t$ satisfies $\E[X_\infty|\mathscr{F}_t]=\lim_{s\to\infty}\E[X_s|\mathscr{F}_t]=X_t$.

\paragraph{Theorem 3.47\label{thm:3.47}} (Convergence theorem for $L^p$-bounded continuous-time martingales). Let $(X_t)_{t\geq 0}$ is a martingale such that $\sup_{t\geq 0}\E[\vert X_t\vert^p]<\infty$, where $p>1$. Then $X_\infty=\lim_{t\to\infty} X_t\ a.s.$ and in $L^p$.
\begin{proof}
Let $Y=\sup_{t\geq 0}\vert X_t\vert$, and $M=\sup_{t\geq 0}\E[\vert X_t\vert^p]<\infty$. By \hyperref[thm:3.46]{Theorem 3.45}, there exists $X_\infty\in L^p(\Omega,\mathscr{F},\P)$ such that $X_t\to X_\infty\ a.s.$. By Doob's $L^p$-inequality [\hyperref[prop:3.38]{Proposition 3.38 (ii)}] and monotone convergence theorem,
\begin{align*}
	\E\left[\sup_{0\leq s\leq t}\vert X_s\vert^p\right]\leq\left(\frac{p}{p-1}\right)^p\E[\vert X_t\vert^p]\leq\left(\frac{p}{p-1}\right)^p M\quad\Rightarrow\quad \E[Y^p]\leq\left(\frac{p}{p-1}\right)^p M<\infty.
\end{align*}
Since $\vert X_t-X_\infty\vert \leq 2Y$, by Lebesgue dominated convergence theorem, $\Vert X_t-X_\infty\Vert_p\to\infty$.
\end{proof}

\paragraph{}  Given a sample-right-continuous submartingale $(X_t)_{t\geq 0}$ such that $\sup_{t\geq 0}\E[X_t^+]<\infty$, and a stopping time $\tau$, we define the random variable
\begin{align*}
	X_\tau(\omega)=X_{\tau(\omega)}(\omega)\mathds{1}_{\{\tau <\infty\}}(\omega) + X_\infty(\omega)\mathds{1}_{\{\tau=\infty\}}(\omega),\ \textit{where}\ X_\infty=\lim_{t\to\infty} X_t\ a.s..
\end{align*}

By \hyperref[prop:3.9]{Proposition 3.9} and \hyperref[prop:3.12]{Proposition 3.12}, $(X_t)_{t\geq 0}$ is progressive, and the restriction of $X_\tau$ to $\{\tau<\infty\}$ is $\mathscr{F}_\tau$-measurable. Meanwhile, $\{X_\infty\mathds{1}_{\{\tau=\infty\}}\leq\alpha\}\subset\{\tau\leq t\}$ if $\alpha\geq 0$, and $\{X_\infty\mathds{1}_{\{\tau=\infty\}}\leq\alpha\}\cap\{\tau\leq t\}=\emptyset$ if $\alpha<0$. Therefore, $\{X_\infty\mathds{1}_{\{\tau=\infty\}}\leq\alpha\}\cap\{\tau\leq t\}\in\mathscr{F}_t$ for all $t\geq 0$, and $X_\infty\mathds{1}_{\{\tau=\infty\}}$ is $\mathscr{F}_\tau$-measurable. As a result, the random variable $X_\tau$ is $\mathscr{F}_\tau$-measurable.

\paragraph{Theorem 3.48\label{thm:3.48}} (Optional stopping theorem for submartingales). Let $(X_t)_{t\geq 0}$ be a sample-right-continuous submartingale. Let $\tau$ and $\sigma$ be two stopping times such that $\sigma\leq\tau$. Then $X_\tau,X_\sigma\in L^1(\Omega,\mathscr{F},\P)$, and $\E[X_\tau|\mathscr{F}_\sigma]\geq X_\sigma$, if either of the following conditions holds:
\begin{itemize}
	\item[(i)] $\tau$ and $\sigma$ are bounded stopping times;
	\item[(ii)] $(X_t)_{t\geq 0}$ is uniformly bounded by some $U\in L^1(\Omega,\mathscr{F}_0,\P)$ from above, i.e. $X_t\leq U$ for all $t\geq 0$.
\end{itemize}
\begin{proof}
(i) Suppose $\tau\leq M$, where $M\in\mathbb{N}$. Akin to \hyperref[prop:3.14]{Proposition 3.14}, we define two sequences of stopping times $\sigma_n\leq\tau_n$ that decrease to $\sigma$ and $\tau$, respectively:
\begin{align*}
	\sigma_n = \sum_{k=0}^{M2^n-1}\frac{k+1}{2^n}\mathds{1}_{\{k2^{-n}<\sigma\leq(k+1)2^{-n}\}},\ \tau_n = \sum_{k=0}^{M2^n-1}\frac{k+1}{2^n}\mathds{1}_{\{k2^{-n}<\tau\leq(k+1)2^{-n}\}},\ n\in\mathbb{N}.
\end{align*}

We fix $n\geq 2$. Then the sequence $(X_{k2^{-n}})_{k=0}^\infty$ is a discrete-time submartingale with respect to the filtration $(\mathscr{F}_{k2^{-n}})_{k=0}^\infty$. Furthermore, both $2^n\tau_{n-1}$ and $2^n\tau_n$ are stopping times of $(\mathscr{F}_{k2^{-n}})_{k=0}^\infty$:
\begin{align*}
	2^n\tau_{n-1}=\min\left\{k\in\{2,4,\cdots,M2^n\}:\ k2^{-n}\geq\tau\right\}\ &\Rightarrow\ \{2^n\tau_{n-1}\leq p\}=\left\{\tau\leq\left\lfloor\frac{p}{2}\right\rfloor 2^{1-n}\right\}\in\mathscr{F}_{p2^{-n}},\ \forall p\in\mathbb{N};\\
	2^n\tau_n = \min\left\{k\in\{1,2,\cdots,M2^n\}:k2^{-n}\geq\tau\right\}&\Rightarrow\ \{2^n\tau_n\leq p\}=\{\tau\leq p2^{-n}\}\in\mathscr{F}_{p2^{-n}},\ \forall p\in\mathbb{N}.
\end{align*}

By the optional stopping theorem for discrete-time submartingales \hyperref[thm:3.19]{[Theorem 3.19 (iii)]}, we know that $\E[X_{\tau_{n-1}}|\mathscr{F}_{\tau_n}]\geq X_{\tau_n}$. Hence $Y_{n}:=X_{\tau_{-n}}$ is a backward submartingale. Furthermore, by \hyperref[thm:3.19]{Theorem 3.19 (i, ii)}, we have $\E[X_{\tau_n}]\geq\E[X_0]$ for all $n\in\mathbb{N}$. Apply \hyperref[thm:3.34]{Theorem 3.34}, the sequence $(X_{\tau_n})_{n=1}^\infty$ is uniformly integrable.

Since $\tau_n(\omega)\searrow\tau(\omega)$, and the sample path $t\mapsto X_t(\omega)$ is right-continuous, we have $X_{\tau_n}\to X_\tau\ a.s.$, and this convergence also holds in $L^1$. Also, $X_{\sigma_n}\to X_\sigma\ a.s.$ and in $L^1$. Then for all $A\in\mathscr{F}_\sigma$, we have
\begin{align*}
	\text{(By \hyperref[thm:3.19]{Theorem 3.19})}\quad \E[X_{\tau_n}\mathds{1}_A]\geq\E[X_{\sigma_n}\mathds{1}_A],\ \forall n\in\mathbb{N}\ \Rightarrow\ \E[X_\tau\mathds{1}_A]\geq\E[X_\sigma\mathds{1}_A],
\end{align*}
where the $\Rightarrow$ follows from $L^1$ convergence. Hence $\E[X_\tau|\mathscr{F}_\sigma]\geq X_\sigma$.\vspace{0.1cm}

(ii) Apply (i) to bounded stopping times $0$ and $\tau\wedge n$, we have $\E[X_{\tau\wedge n}]\geq\E[X_0]>-\infty$. Since $X_t$ is bounded from above by $U$, by Fatou's lemma, $\E[X_\tau]\geq\limsup_{n\to\infty}\E[X_{\tau\wedge n}]\geq \E[X_0]>-\infty$. Similarly we have $\E[X_\sigma]\geq \E[X_0] >-\infty$. Hence $X_\tau,X_\sigma\in L^1(\Omega,\mathscr{F},\P)$.

Fix $A\in\mathscr{F}_\sigma\subset\mathscr{F}_\tau$, and define $\tau^A(\omega)=\tau(\omega)\mathds{1}_A + \infty\mathds{1}_{A^c}$. According to \hyperref[prop:3.11]{Proposition 3.11 (d)}, both $\tau^A$ and $\sigma^A$ are also stopping times. By (i), we have
\begin{align*}
	0 &\leq \E\left[X_{\tau^A\wedge n}\right]-\E\left[X_{\sigma^A\wedge n}\right]\\
	&=\E\left[X_n\mathds{1}_{A^c}+X_{\tau\wedge n}\mathds{1}_{A\cap\{\sigma\leq n\}}+X_n\mathds{1}_{A\cap\{\sigma>n\}}\right]-\E\left[X_n\mathds{1}_{A^c}+X_\sigma\mathds{1}_{A\cap\{\sigma\leq n\}}+X_n\mathds{1}_{A\cap\{\sigma>n\}}\right]\\
	&= \E\left[X_{\tau\wedge n}\mathds{1}_{A\cap\{\sigma\leq n\}}\right] - \E\left[X_{\sigma}\mathds{1}_{A\cap\{\sigma\leq n\}}\right]
\end{align*}
Note that $\mathds{1}_{A\cap\{\sigma\leq n\}}\to\mathds{1}_{A\cap\{\sigma<\infty\}}$ and $X_{\tau\wedge n}\to X_\tau$ as $n\to\infty$. By Lebesgue dominated convergence theorem, we have $\E[X_\tau\mathds{1}_{A\cap\{\sigma<\infty\}}]\geq\E[X_\sigma\mathds{1}_{A\cap\{\sigma<\infty\}}]$. Clearly, $\E[X_\tau\mathds{1}_{A\cap\{\sigma=\infty\}}]=\E[X_\sigma\mathds{1}_{A\cap\{\sigma=\infty\}}]$. Hence
\begin{align*}
	\E[\E[X_\tau|\mathscr{F}_\sigma]\mathds{1}_A]=\E[X_\tau\mathds{1}_A]\geq\E[X_\sigma\mathds{1}_A],\ \forall A\in\mathscr{F}_\sigma.
\end{align*}
Since $X_\sigma$ is $\mathscr{F}_\sigma$-measurable, we have $\E[X_\tau|\mathscr{F}_\sigma]\geq X_\sigma$.
\end{proof}

\paragraph{Theorem 3.49\label{thm:3.49}} (Optional stopping theorem for uniformly integrable martingales). Let $(X_t)_{t\geq 0}$ be a uniformly integrable sample-right-continuous martingale. Let $\tau$ be a stopping time. Then we have
\begin{align*}
	\E[X_\infty|\mathscr{F}_\tau]=X_\tau\in L^1(\Omega,\mathscr{F},\P).
\end{align*}
Furthermore, if $\sigma$ is another stopping time such that $\sigma\leq\tau$, then $\E[X_\tau|\mathscr{F}_\sigma]=X_\sigma.$
\begin{proof}
Using \hyperref[prop:3.14]{Proposition 3.14}, we define a sequence of stopping times $\tau_n\searrow\tau$ as follows:
\begin{align*}
	\tau_n=\sum_{k=0}^\infty\frac{k+1}{2^n}\mathds{1}_{\{k2^{-n}<\tau\leq(k+1)2^{-n}\}}+\infty\mathds{1}_{\{\tau=\infty\}},\ n\in\mathbb{N}.
\end{align*}

Clearly, $2^n\tau_n$ is a stopping time of the filtration $\{\mathscr{F}_{(k+1)2^{-n}}\}_{k=0}^\infty$. Apply \hyperref[thm:3.32]{Theorem 3.32} to discrete-time martingale $\{X_{(k+1)2^{-n}}\}_{k=0}^\infty$ with respect to the filtration $\{\mathscr{F}_{(k+1)2^{-n}}\}_{k=0}^\infty$, which is uniformly integrable, we have $\E[X_\infty|\mathscr{F}_{\tau_n}]=X_{\tau_n}\in L^1(\Omega,\mathscr{F},\P)$. Since $\tau_n(\omega)\searrow\tau(\omega)$, and the sample path $t\mapsto X_t(\omega)$ is right-continuous, we have $X_{\tau_n}\to X_\tau\ a.s.$. Furthermore, $\{X_{\tau_n}\}_{n=1}^\infty$ is uniformly integrable, so the convergence also holds in $L^1$. For all $A\in\mathscr{F}_\tau\subset\mathscr{F}_{\tau_n}$, $\E[X_\infty|\mathscr{F}_{\tau_n}]=X_{\tau_n}$ implies $\E[X_\infty\mathds{1}_A]=\E[X_{\tau_n}\mathds{1}_A]$. Then
\begin{align*}
	\E[X_\tau\mathds{1}_A] = \lim_{n\to\infty}\E\left[X_{\tau_n}\mathds{1}_A\right] = \E[X_\infty\mathds{1}_A],\ \forall A\in\mathscr{F}_\tau\quad \Rightarrow\quad \E[X_\infty|\mathscr{F}_\tau]=X_\tau.
\end{align*}
Furthermore, if $\sigma\leq\tau$ is a stopping time, then $\E[X_\tau|\mathscr{F}_\sigma]=\E\left[\E[X_\infty|\mathscr{F}_\tau]|\mathscr{F}_\sigma\right]=\E[X_\infty|\mathscr{F}_\sigma]=X_\sigma$.
\end{proof}

\paragraph{} Like in \hyperref[def:3.30]{Definition 3.30}, if an adapted process $(X_t)_{t\geq 0}$ is stopped by a stopping time $\tau$, we define the stopped process as $X_t^\tau = X_{t\wedge\tau}$ for $t\geq 0$.

\paragraph{Corollary 3.50.\label{cor:3.50}} Let $(X_t)_{t\geq 0}$ be a sample-right-continuous submartingale. Let $\tau$ be a stopping time.
\begin{itemize}
	\item[(i)] The stopped process $(X_t^\tau)_{t\geq 0}$ is a submartingale.
	\item[(ii)] In addition, if $(X_t)_{t\geq 0}$ is a uniformly integrable martingale, so is the stopped process $(X_t^\tau)_{t\geq 0}$. Moreover,
	\begin{align*}
		X_t^\tau = \E[X_\tau|\mathscr{F}_t].
	\end{align*}
\end{itemize}
\begin{proof}
(i) Fix $t>s\geq 0$. If $A\in\mathscr{F}_s\subset\mathscr{F}_t$, we have $A\cap\{\tau>s\}\in\mathscr{F}_s$, and $A\cap\{\tau>s\}\in\mathscr{F}_\tau$ by the very definition of $\mathscr{F}_\tau$. Hence $A\cap\{\tau>t\}\in\mathscr{F}_s\cap\mathscr{F}_\tau=\mathscr{F}_{\tau\wedge s}$, and
\begin{align*}
	\E\left[X_t^\tau\mathds{1}_A\right] - \E\left[X_s^\tau\mathds{1}_A\right]  &= \E\left[X_{\tau\wedge t}\mathds{1}_{A\cap\{\tau\leq s\}}\right] + \E\left[X_{\tau\wedge t}\mathds{1}_{A\cap\{\tau>s\}}\right] - \E\left[X_{\tau\wedge s}\mathds{1}_A\right]\\
	&= \E\left[X_{\tau\wedge s}\mathds{1}_{A\cap\{\tau\leq s\}}\right] + \E\left[X_{\tau\wedge t}\mathds{1}_{A\cap\{\tau>s\}}\right] - \E\left[X_{\tau\wedge s}\mathds{1}_A\right]\\
	&=\E\left[(X_{\tau\wedge t}-X_{\tau\wedge s})\mathds{1}_{A\cap\{\tau>s\}}\right]=\E\left[\E\left[X_{\tau\wedge t}-X_{\tau\wedge s}|\mathscr{F}_{\tau\wedge s}\right]\mathds{1}_{A\cap\{\tau>s\}}\right] = 0,
\end{align*}
where the last inequality follows from \hyperref[thm:3.48]{Theorem 3.48}, because $\tau\wedge s\leq\tau\wedge t$ are two bounded stopping times.\vspace{0.1cm}

(ii) Fix $t\geq 0$. If $A\in\mathscr{F}_t$, we have $A\cap\{\tau>t\}\in\mathscr{F}_t$, and $A\cap\{\tau>t\}\in\mathscr{F}_\tau$ by the very definition of $\mathscr{F}_\tau$. Hence $A\cap\{\tau>t\}\in\mathscr{F}_t\cap\mathscr{F}_\tau=\mathscr{F}_{\tau\wedge t}$, and
\begin{align*}
	\E\left[X_\tau\mathds{1}_A\right] - \E\left[X_t^\tau\mathds{1}_A\right] &= \E\left[X_{\tau\wedge t}\mathds{1}_{A\cap\{\tau\leq t\}}\right] + \E\left[X_\tau\mathds{1}_{A\cap\{\tau>t\}}\right] - \E\left[X_{\tau\wedge t}\mathds{1}_A\right]\\
	&=\E\left[(X_\tau-X_{\tau\wedge t})\mathds{1}_{A\cap\{\tau>t\}}\right]=\E\left[\E\left[X_\tau-X_{\tau\wedge t}|\mathscr{F}_{\tau\wedge t}\right]\mathds{1}_{A\cap\{\tau>t\}}\right] = 0,
\end{align*}
where the last inequality follows from \hyperref[thm:3.49]{Theorem 3.49}, because $\tau\wedge t\leq\tau$ is a stopping time. Since $A\in\mathscr{F}_t$ is arbitrary, and $X_t^\tau=X_{\tau\wedge t}$ is $\mathscr{F}_t$-measurable, we have $\E[X_\tau|\mathscr{F}_t]=X_t^\tau$. Since $X_\tau\in L^1(\Omega,\mathscr{F},\P)$, the stopped process $(X_t^\tau)_{t\geq 0}$ is uniformly integrable.
\end{proof}

\subsection{Local Martingales, Quadratic Variation and Bracket}
\paragraph{Review: Functions of bounded variation.} Given a function $f:[0,T]\to\mathbb{R}$, we define the \textit{total variation} of $f$ on interval $[0,T]$ as
\begin{align*}
	V_0^T f = \sup\left\{\sum_{j=1}^n\vert f(x_j)-f(x_{j-1})\vert:\ n\in\mathbb{N},\ 0=x_0<x_1<\cdots<x_n=T\right\}
\end{align*}
If $V_0^T f < \infty$, we say that $f:[0,T]\to\mathbb{R}$ has \textit{bounded variation}.

We can show that every function of bounded variation is the difference of two monotone increasing functions. If $f:[0,T]\to\mathbb{R}$ is a function of bounded variation, we define $v_f(0)=0$ and $v_f(t)=V_0^t f$ for all $t\in(0,T]$, which is the total variation of $f|_{[0,t]}$. Then we have $v_f(t)-v_f(s)=V_s^t(f)\geq\vert f(t)-f(s)\vert$ for all $0\leq s<t\leq T$. Therefore, both $v_f+f$ and $v_f-f$ are monotone increasing functions on $[0,T]$, and $f=\frac{1}{2}(v_f+f)-\frac{1}{2}(v_f-f)$. In addition, if $f(0)=0$, we can require both $v_f+f$ and $v_f-f$ to be nonnegative.

Furthermore, if $f:[0,T]\to\mathbb{R}$ is a càdlàg function of bounded variation such that $f(0)=0$, there exists a finite signed measure $\mu$ such that $\mu([0,t])=f(t)$ for all $t\in[0,T]$.

Since $v_f$ is monotone on $[0,T]$, $v_f$ has a left limit $v_f(s-)$ at every $s\in(0,T]$, and a right limit $v_f(t+)$ at every $t\in[0,T)$. We prove that $v_f(t+)=v_f(t)$ for all $t\in[0,T)$. Fix $\epsilon>0$. Since $f$ is right-continuous, choose $\delta>0$ such that $\vert f(x)-f(t)\vert<\epsilon/2$ for all $x\in(t,t+\delta)$. We also choose a partition $t=x_0<x_1<\cdots<x_n=T$ such that $\sum_{j=1}^n\vert f(x_j)-f(x_{j-1})\vert > V_t^T f -\frac{\epsilon}{2}$. Then for all $x<\min\{x_1,\delta\}$, we have
\begin{align*}
	V_t^T f - \frac{\epsilon}{2} < \sum_{j=1}^n\vert f(x_j)-f(x_{j-1})\vert \leq \vert f(x) - f(t)\vert + \vert f(x_1) - f(x)\vert + \sum_{j=2}^n\vert f(x_j)-f(x_{j-1})\vert \leq \frac{\epsilon}{2} + V_x^T f.
\end{align*}

Hence $v_f(x)-v_f(t) = V_t^x f = V_t^T f - V_x^T f < \epsilon$ for all $x\in\min\{x_1,\delta\}$, and $v_f$ is right-continuous. As a result, both $v_f+f$ and $v_f-f$ are nonnegative, monotone increasing and càdlàg functions on $[0,T]$. Akin to the Carathéodory extension procedure of a c.d.f. in the Remark of \hyperref[def:2.3]{Definition 2.3}, there exists two Borel measures $\mu^+$ and $\mu^-$ such that $\mu^+([0,t])=\frac{1}{2}\left(v_f(t)+f(t)\right)$ and $\mu^-([0,t])=\frac{1}{2}\left(v_f(t)-f(t)\right)$ for all $t\in[0,T]$. Then $\mu=\mu^+ - \mu^-$ is a signed measure with $\mu([0,t])=f(t)$ for all $t\in[0,T]$. 

Moreover, the total variation measure $\vert\mu\vert$ of $\mu$ satisfies $\vert\mu\vert([0,T])=v_f(T)=V_0^T f$. For any partition $0=x_0<x_1<\cdots<x_n=T$, we have $\sum_{j=1}^n\vert f(x_j)-f(x_{j-1})\vert\leq\vert\mu\vert([0,T])$, hence $V_0^t f\leq\vert\mu\vert([0,T])$. To prove the opposite, we define a probability measure $\P(A)=\frac{\vert\mu\vert(A)}{\vert \mu\vert([0,T])}$ on $\mathscr{B}([0,T])$. Let $P\amalg N=[0,T]$ be the Hahn decomposition associated with $\mu$, and define $Y=\mathds{1}_P-\mathds{1}_N$. Let $0=t_0^n<t^n_1<\cdots<t^n_{k_n}=T$ be an increasing sequence of partitions of interval $[0,t]$ such that the mesh $\max_{1\leq j\leq k_n}(t_j^n-t_{j-1}^n)\to 0$, and let $\mathscr{B}_n$ be the sub $\sigma$-algebra generated by intervals $(t_j^n,t_{j-1}^n]$. Then $(\mathscr{B}_n)_{n=1}^\infty$ is a filtration with $\mathscr{B}_\infty=\mathscr{B}([0,T])$, and $X_n=\E[Y|\mathscr{B}_n]$ is a uniformly integrable martingale sequence. Furthermore, by properties of conditional expectation, $X_n$ is a constant on each subinterval $(t_{j-1}^n,t_j^n]$, and
\begin{align*}
	X_n|_{(t_{j-1}^n,t_j^n]} = \frac{\E\left[X_n\mathds{1}_{(t_{j-1}^n,t_j^n]}\right]}{\P((t_{j-1}^n,t_j^n])} = \frac{\E\left[Y\mathds{1}_{(t_{j-1}^n,t_j^n]}\right]}{\P((t_{j-1}^n,t_j^n])} = \frac{\mu((t_{j-1}^n,t_j^n])}{\vert\mu\vert([0,T])\P((t_{j-1}^n,t_j^n])}=\frac{f(t_j^n)-f(t_{j-1}^n)}{\vert\mu\vert((t_{j-1}^n,t_j^n])},\ \forall 1\leq j\leq k_n.
\end{align*}

Now it suffices to prove that $\sum_{j=1}^{k_n}\left\vert f(t_j^n)-f(t_{j-1}^n)\right\vert\to \mu([0,T])$. By Doob's convergence theorem for uniformly integrable martingales, we have $X_n\to Y\ a.s.$ and in $L^1$. As a result,
\begin{align*}
	\E[\vert X_n\vert] = \sum_{j=1}^{k_n}\frac{\left\vert f(t_j^n)-f(t_{j-1}^n)\right\vert}{\mu([0,T])} \to \E[\vert Y\vert]=1.
\end{align*}

Also, we can see that $\mu=\mu^+-\mu^-$ is indeed the Jordan decomposition of $\mu$.

\paragraph{Review: Functions of finite variation.} A function $f:\mathbb{R}_+\to\mathbb{R}$ is said to have \textit{finite variation}, if the restriction $f|_{[0,T]}$ has bounded variation on $[0,T]$ for all $T\in\mathbb{R}_{++}$. In addition, if $f(0)=0$ and $f$ is càdlàg, we can find a unique $\sigma$-finite signed measure $\mu$ on $\mathscr{B}(\mathbb{R}_+)$ such that $\mu([0,t])=f(t)$ for all $t\in\mathbb{R}_+$, and $\vert\mu\vert([0,t])$ is the total variation of $f|_{[0,t]}$.

\paragraph{Review: Lebesgue Stieltjes integral.} Let $f:[0,T]\to\mathbb{R}$ be a càdlàg function of bounded variation with $f(0)=0$, so we can find a finite signed measure on $[0,T]$ such that $\mu([0,t])=f(t)$ for all $t\in[0,T]$. If $\varphi:[0,T]\to\mathbb{R}$ is a measurable function such that $\int_{[0,T]}\vert\varphi\vert\,\d \vert\mu\vert < \infty$, define the Lebesgue-Stieltjes integral
\begin{align*}
	\int_0^T \varphi(s)\,\d f(s) = \int_{[0,T]}\varphi\,\d\mu,\quad \int_0^T \varphi(s)\,\vert\d f(s)\vert = \int_{[0,T]}\varphi\,\vert\d\mu\vert.
\end{align*}

It is seen that the function $t\mapsto\int_0^t\varphi(s)\,\d f(s)$ is also of bounded variation on $[0,T]$. To see this, note the associated signed measure is $\nu(A)=\int_A\varphi\,\d \mu$, and the total variation on $[0,T]$ is $\vert\nu\vert([0,T])=\int_{[0,T]}\vert\varphi\vert\,\vert\d\mu\vert < \infty$.

Also, if $f:\mathbb{R}_+\to\mathbb{R}$ is a càdlàg function of finite variation with $f(0)=0$, we can define the Lebesgue-Stieltjes integral
\begin{align*}
	\int_0^\infty \varphi(s)\,\d f(s) = \lim_{T\to\infty}\int_0^T\varphi(s)\,\d f(s)
\end{align*}
for all measurable functions $\varphi$ such that $\int_0^\infty\vert\varphi(s)\vert\,\vert\d f(s)\vert < \infty$.

\paragraph{Review: Approximation of Lebesgue Stieltjes integral.} We can approximate a Lebesgue-Stieltjes integral of a continuous function by differentiating on a mesh. Let $0=t_0^n<t_1^n<\cdots<t_{k_n}^n=T$ be a sequence of partitions whose mesh $\max_{1\leq j\leq k_n}(t_j^n-t_{j-1}^n)\to 0$. Then
\begin{align*}
	\int_0^T \varphi_n(s)\,\d f(s) = \sum_{j=1}^{k_n}\varphi(t_j^n)\left(f(t_j^n)-f(t_{j-1}^n)\right),\ \textit{where}\ \varphi_n(s)=\varphi(0)\mathds{1}_{\{0\}}(s)+\sum_{j=1}^{k_n}\varphi(t_j^n)\mathds{1}_{(t_{j-1}^n,t_j^n]}(s).
\end{align*}

By continuity of $\varphi$, we have $\varphi_n\to\varphi$, and all these functions are dominated by a constant $\max_{s\in[0,T]}\vert\varphi(s)\vert$. By Lebesgue dominated convergence theorem, we have
\begin{align*}
	\int_0^T \varphi(s)\,\d f(s) = \lim_{n\to\infty}\sum_{j=1}^{k_n}\varphi(t_j^n)\left(f(t_j^n)-f(t_{j-1}^n)\right).
\end{align*}

\paragraph{Definition 3.51\label{def:3.51}} (Finite variation processes). An adapted process $(X_t)_{t\geq 0}$ is said to be a \textit{finite variation process} if all its sample paths $t\mapsto X_t(\omega)$ are functions of finite variation on $\mathbb{R}_+$. In addition, if all sample paths $t\mapsto X_t(\omega)$ are monotone increasing, the process $(X_t)_{t\geq 0}$ is said to be an \textit{increasing process}.
\paragraph{Remark.} If $(A_t)_{t\geq 0}$ is a finite variation process, then
\begin{align*}
	V_t = \int_0^t\vert\d A_s\vert,\ \forall t\in\mathbb{R}_+
\end{align*}
is an increasing process. Writing $A_t=\frac{1}{2}(V_t + A_t) - \frac{1}{2}(V_t - A_t)$ shows that any finite variation process can be written as the difference of two increasing processes. Note that $V_t$ is $\mathscr{F}_t$-measurable, because
\begin{align*}
	V_t = \lim_{n\to\infty}\sum_{j=1}^{k_n}\left\vert A_{t_j^n} - A_{t_{j-1}^n}\right\vert,\ \textit{where}\ 0=t_0^n<t_1^n<\cdots<t_{k_n}^n=t\ \textit{is an increasing sequence of partitions of}\ [0,t].
\end{align*}


\paragraph{Proposition 3.52.\label{prop:3.52}} Let $A=(A_t)_{t\geq 0}$ be a finite variation process, and let $H=(H_t)_{t\geq 0}$ be a progressive process such that $\int_0^t\vert H_s(\omega)\vert\left\vert\d A_s(\omega)\right\vert < \infty$ for all $t\in\mathbb{R}_+$ and all $\omega\in\Omega$. Then the process $H\cdot A$ defined by
\begin{align*}
	(H\cdot A)_t = \int_0^t H_s\,\d A_s,\ \forall t\in\mathbb{R}_+.
\end{align*}
is also a finite variation process.
\begin{proof}
It suffices to show that $H\cdot A$ is an adapted process: If $h:\Omega\times[0,t]\to\mathbb{R}$ is measurable on $\mathscr{F}_t\times\mathscr{B}([0,t])$, and $\int_0^t\vert h(\omega,s)\vert\,\vert\d A_s(\omega)\vert < \infty$ for all $\omega\in\Omega$, then $\omega\mapsto\int_0^t h(\omega,s)\,\d A_s(\omega)$ is $\mathscr{F}_t$-measurable.

If $h(\omega,s)=\mathds{1}_F(\omega)\mathds{1}_{(p,q]}(s)$, where $F\in\mathscr{F}_t$ and $(p,q]\subset[0,t]$, we have
\begin{align*}
	\int_0^t h(\omega,s)\,\d A_s(\omega) = \mathds{1}_F(\omega)\left(A_q(\omega)-A_p(\omega)\right).
\end{align*}
Clearly, $\mathds{1}_F(A_q-A_p)$ is $\mathscr{F}_t$-measurable. Now we define
\begin{align*}
	\mathscr{L}_t=\left\{G\in\mathscr{F}_t\otimes\mathscr{B}([0,t]):\omega\mapsto\int_0^t\mathds{1}_G(\omega,s)\,\d A_s(\omega)\ \textit{is $\mathscr{F}_t$-measurable}\right\}.
\end{align*}

Note this is a $\lambda$-system containing $\{F\times(p,q]:F\in\mathscr{F}_t,\ (p,q]\subset[0,t]\}$, which is a $\pi$-system generating $\mathscr{F}_t\otimes\mathscr{B}([0,t])$. By Sierpiński-Dynkin $\pi$-$\lambda$ theorem, we have $\mathscr{L}_t=\mathscr{F}_t\otimes\mathscr{B}([0,t])$. Hence $\omega\mapsto\int_0^t h(\omega,s)\,\d A_s(\omega)$ is $\mathscr{F}_t$-measurable for all simple functions $h$. The remaining part follows from simple function approximation and the Lebesgue dominated convergence theorem.
\end{proof}
\paragraph{Remark.} If the filtration $(\mathscr{F}_t)_{t\geq 0}$ is complete, then \hyperref[prop:3.52]{Proposition 3.52} holds for all progressive process $(H_t)_{t\geq 0}$ such that $\int_0^t\vert H_s(\omega)\vert\left\vert\d A_s(\omega)\right\vert < \infty$ for all $t\in\mathbb{R}_+$ and $\P$-$a.e.$ $\omega\in\Omega$. To see this, define
\begin{align*}
	H^\prime_t(\omega)=\begin{cases}
		H_t(\omega)\ &\textit{if}\ \int_0^t\vert H_s(\omega)\vert\left\vert\d A_s(\omega)\right\vert < \infty,\\
		0\ &\textit{otherwise}.
	\end{cases}
\end{align*}
Then $(H^\prime_t)_{t\geq 0}$ is still progressive, and we define $H\cdot A = H^\prime\cdot A$.

\paragraph{Definition 3.53\label{def:3.53}} (Continuous local martingales). An adapted sample-continuous process $X=(X_t)_{t\geq 0}$ with $X_0=0\ a.s.$ is said to be a \textit{continuous local martingale} if there exists an increasing sequence $(\tau_n)_{n=1}^\infty$ of stopping times such that $\tau_n\nearrow\infty$ and the stopped process $X^{\tau_n}=(X_t^{\tau_n})_{t\geq 0}$ is a uniformly integrable martingale. In this case, the sequence $(\tau_n)_{n=1}^\infty$ of stopping times is said to \textit{reduce} process $X$.

More generally, an adapted sample-continuous process $X=(X_t)_{t\geq 0}$ is said to be a \textit{continuous local martingale} if the process $Y_t=X_t-X_0$ is a continuous local martingale. Here we do not assume $X_0$ is $L^1$.

\paragraph{Remark.} In the \hyperref[def:3.53]{Definition 3.53}, one can replace ``uniformly integrable martingale'' by ``martingale''. In the latter case, $\tau_n\wedge n$ is a sequence of stopping times such that $X^{\tau_n\wedge n}$ is uniformly integrable, and $\tau_n\wedge n\nearrow\infty$.

The following two basic facts about continuous local martingales immediately follow from \hyperref[cor:3.50]{Corollary 3.50}. Suppose $X=(X_t)_{t\geq 0}$ is a continuous local martingale. Then:
\begin{itemize}
	\item[(i)] For any stopping time $\tau$, the stopped process $(X_t^\tau)_{t\geq 0}$ is also a continuous local martingale.
	\item[(ii)] If $(\tau_n)_{n=1}^\infty$ is a sequence of stopping times reducing $X$, and $(\sigma_n)_{n=1}^\infty$ is a sequence of stopping times such that $\sigma_n\nearrow\infty$, then $(\sigma_n\wedge\tau_n)_{n=1}^\infty$ also reduces $X$.
\end{itemize}

We can show that all continuous local martingales form a vector space. To see this, let $X$ and $X^\prime$ be two continuous local martingales reduced by stopping time sequences $(\tau_n)_{n=1}^\infty$ and $(\tau_n^\prime)_{n=1}^\infty$, respectively. Using property (ii), we know that $(\tau_n\wedge\tau_n^\prime)_{n=1}^\infty$ is a stopping time sequence that reduces process $X+X^\prime$.

\paragraph{Proposition 3.54.\label{prop:3.54}} Let $X=(X_t)_{t\geq 0}$ be a continuous local martingale.
\begin{itemize}
	\item[(i)] If $(X_t)_{t\geq 0}$ is nonnegative and $X_0\in L^1(\Omega,\mathscr{F},\P)$, then $(X_t)_{t\geq 0}$ is a supermartingale.
	\item[(ii)] If there exists random variable $Z\in L^1(\Omega,\mathscr{F},\P)$ such that $\vert X_t\vert\leq Z$ for all $t\geq 0$, then $(X_t)_{t\geq 0}$ is a uniformly integrable martingale.
	\item[(iii)] If $X_0=0$, the sequence of stopping times $\tau_n=\inf\{t\geq 0:\vert X_t\vert\geq n\}$ reduces $X$.
	\item[(iv)] If $W$ is a $\mathscr{F}_0$-measurable (real) random variable, then $(WX_t)_{t\geq 0}$ is also a continuous local martingale.
\end{itemize}
\begin{proof}
(i) Write $X_t=X_0+Y_t$. By definition, there exists a sequence $(\tau_n)_{n=1}^\infty$ of stopping times reducing $Y$. Whenever $t>s\geq 0$, since $X_0\in L^1(\Omega,\mathscr{F},\P)$, we have
\begin{align*}
	Y_{s\wedge\tau_n} = \E[Y_{t\wedge\tau_n}|\mathscr{F}_s] \quad\Rightarrow\quad X_{s\wedge\tau_n} = \E[X_{t\wedge\tau_n}|\mathscr{F}_s]\tag{3.9}\label{eq:3.9}
\end{align*}
Since $X$ is nonnegative, by Fatou's lemma (conditional version), we have
\begin{align*}
	X_s = \liminf_{n\to\infty} X_{s\wedge\tau_n} = \liminf_{n\to\infty} \E[X_{t\wedge\tau_n}|\mathscr{F}_s]\geq \E\left[\lim_{n\to\infty}X_{t\wedge\tau_n}|\mathscr{F}_s\right] = \E[X_t|\mathscr{F}_s].
\end{align*}

(ii) Following \hyperref[eq:3.9]{(3.9)}, we use Lebesgue dominated convergence theorem, because $\vert X_{t\wedge\tau_n}\vert\leq Z\in L^1(\Omega,\mathscr{F},\P)$ for all $n\in\mathbb{N}$ and $t\geq 0$. Then $X_{t\wedge\tau_n}\to X_t$ in $L^1$, and $X_s=\E[X_t|\mathscr{F}_s]$ for $0\leq s<t$.

(iii) By \hyperref[prop:3.13]{Proposition 3.13 (ii)}, $\tau_n=\inf\{t\geq 0:X_t\geq n\}$ is indeed a stopping time. By (ii), the stopped process $X^{\tau_n}$, bounded by $n$, is a uniformly integrable martingale for each $n\in\mathbb{N}$, and the result follows.

(iv) It suffices to show the case $X_0=0$. Choose the stopping times $\tau_n$ defined in (iii). Clearly, the process $(WX_{t\wedge\tau_n})_{t\geq 0}$ is adapted, and $\vert WX_{t\wedge\tau_n}\vert\leq n\vert W\vert$ is $L^1$. Furthermore, since $(\tau_n)_{n=1}^\infty$ reduces $(X_t)_{t\geq 0}$, and $W$ is $\mathscr{F}_0$-measurable, we have $\E[WX_{t\wedge\tau_n}|\mathscr{F}_s] = W\E[X_{t\wedge\tau_n}|\mathscr{F}_s] = WX_{s\wedge\tau_n}$ for all $t>s\geq 0$. Hence $(\tau_n)_{n=1}^\infty$ also reduces $(WX_t)_{t\geq 0}$, and the conclusion follows.
\end{proof}

\paragraph{Proposition 3.55.\label{prop:3.55}} If $X=(X_t)_{t\geq 0}$ is both a continuous local martingale and a finite variation process with $X_0=0$, then there exists a negligible set $N$ such that $X_t(\omega)=0$ for all $t\in\mathbb{R}_+$ and all $\omega\in\Omega\backslash N$.
\begin{proof}
Since $X$ is a finite variation process, $\int_0^t\vert\d X_s\vert$ is an increasing process with continuous sample paths. For every $n\in\mathbb{N}$, define the stopping time 
\begin{align*}
	\tau_n=\inf\left\{t\geq 0:\int_0^t\vert\d X_s\vert\geq n\right\},
\end{align*}
and set $Y_t = X_t^{\tau_n}$. Then $Y_t\leq\int_0^{t\wedge\tau_n}\vert\d X_s\vert\leq n$. By \hyperref[prop:3.54]{Proposition 3.54 (ii)}, $Y$ is a uniformly integrable martingale. Let $0=t_0<t_1<\cdots<t_k=t$ be a partition of $[0,t]$. By \hyperref[prop:3.39]{Proposition 3.39}, we have
\begin{align*}
	\E\left[Y_t^2\right] = \sum_{j=1}^p\E\biggl[\left(Y_{t_j}-Y_{t_{j-1}}\right)^2\biggr]\leq\E\biggl[\sup_{1\leq j\leq k}\left\vert Y_{t_j}-Y_{t_{j-1}}\right\vert\sum_{j=1}^p\left\vert Y_{t_j}-Y_{t_{j-1}}\right\vert\biggr]\leq n\E\biggl[\sup_{1\leq j\leq k}\left\vert Y_{t_j}-Y_{t_{j-1}}\right\vert\biggr].
\end{align*}

Now we take a sequence of increasing partitions $0=t_0^m<t_1^m<\cdots<t_{k_m}^m=t$ of $[0,t]$ whose mesh converges to $0$. By continuity of sample paths of $Y$ and Lebesgue dominated convergence theorem, we have
\begin{align*}
\sup_{1\leq j\leq k_m}\left\vert Y_{t_j^m}(\omega)-Y_{t_{j-1}^m}(\omega)\right\vert \to 0\ \ \textit{as}\ \ m\to\infty,\ \forall \omega\in\Omega\quad\Rightarrow\quad \E\left[\sup_{1\leq j\leq k_m}\left\vert Y_{t_j^m}-Y_{t_{j-1}^m}\right\vert\right]\to 0.
\end{align*}

Note we are able to use dominated convergence theorem because $Y$ is bounded by $n$. Hence $\E[Y_t^2]=0$, and $X_t^{\tau_n}=0\ a.s.$. Letting $n\to\infty$, we then have $X_t=0\ a.s.$ for all $\in\mathbb{R}_+$. To show that $X(\omega)\equiv 0$ for $a.s.$ $\omega\in\Omega$, we take a countable dense subset $D\subset\mathbb{R}_+$. Then $N=\left\{\omega\in\Omega: \exists t\in D,\ X_t(\omega)\neq 0\right\}$ is a negligible set. By continuity of sample paths of $X$, we have $X_t(\omega)=0$ for all $t\in\mathbb{R}_+$ and all $\omega\in\Omega\backslash N$.
\end{proof}

\paragraph{Remark.} Let $X=(X_t)_{t\geq 0}$ and $Y=(Y_t)_{t\geq 0}$ be two stochastic processes. If there exists a negligible set $N\subset\Omega$ such that $X_t(\omega)=Y_t(\omega)$ for all $t\in\mathbb{R}_+$ and all $\omega\in\Omega\backslash N$, then $X$ and $Y$ are said to be \textit{indistinguishable}. Note this is a stronger condition than $a.s.$ modification.

From now on we assume completeness of the filtration $(\mathscr{F}_t)_{t\geq 0}$.

\paragraph{Theorem 3.56\label{thm:3.56}} (Quadratic variation). Let $X=(X_t)_{t\geq 0}$ be a continuous local martingale. There exists an increasing process denoted by $\left(\langle X,X\rangle_t\right)_{t\geq 0}$, which is unique up to indistinguishability, such that $X_t^2-\langle X,X\rangle_t$ is a continuous local martingale. Furthermore, for every fixed $t\in\mathbb{R}_{++}$, if $0=t_0^n<t_1^n<\cdots<t_{k_n}^n=t$ is an increasing sequence of partitions of $[0,t]$ with the mesh $\max_{1\leq j\leq k_n}\vert t_j^n-t_{j-1}^n\vert\to 0$, then
\begin{align*}
	\langle X,X\rangle_t = \lim_{n\to\infty}\sum_{j=1}^{k_n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\tag{3.10}\label{eq:3.10}
\end{align*}
in probability. The process $\langle X,X\rangle$ is said to be the \textit{quadratic variation} of $X$.
\begin{proof}
\textit{Step I:} Let $Y_t$ and $Y^\prime_t$ be two processes satisfying the conditions given in the statement. Then the process $Y_t^\prime-Y_t=(X_t^2-Y_t)-(X_t^2-Y_t^\prime)$ is both a finite variation process and a continuous local martingale. According to \hyperref[prop:3.55]{Proposition 3.55}, $Y_t^\prime - Y_t = 0\ a.s.$, and the statement of uniqueness follows. \vspace{0.1cm}

\textit{Step II:} Now we prove existence. We first assume that $X_0=0$ and $X$ is bounded. Hence $X$ is a uniformly  integrable martingale by \hyperref[prop:3.54]{Proposition 3.54 (ii)}. We fix $T>0$ and an increasing sequence of partitions of $[0,T]$ with the mesh $\max_{1\leq j\leq k_n}\vert t_j^n-t_{j-1}^n\vert\to 0$. The for every $s>r\geq 0$ and every bounded $\mathscr{F}_r$-measurable random variable $Z$, the process $
\left(Z\left(X_{s\wedge t}-X_{r\wedge t}\right)\right)_{t\geq 0}$ is adapted and $L^1$, and for all $0\leq t^\prime < t$,
\begin{align*}
	\E\left[Z\left(X_{s\wedge t}-X_{r\wedge t}\right)|\mathscr{F}_{t^\prime}\right] = \begin{cases}
		\E\left[\E[Z\left(X_{s\wedge t}-X_{r\wedge t}\right)|\mathscr{F}_r]|\mathscr{F}_{t^\prime}\right] = 0\ &\textit{if}\ \ t^\prime < r\\
		Z\,\E\left[X_{s\wedge t} - X_r|\mathscr{F}_{t^\prime}\right] = Z(X_{s\wedge t^\prime}-X_r)\ &\textit{if}\ \ t^\prime\geq r
	\end{cases}.
\end{align*}
Hence $\left(Z\left(X_{s\wedge t}-X_{r\wedge t}\right)\right)_{t\geq 0}$ is a bounded martingale. Following this, the process
\begin{align*}
	M_t^n = \sum_{j=1}^{k_n} X_{t_{j-1}^n}\left(X_{t_j^n\wedge t}-X_{t_{j-1}^n\wedge t}\right),\quad \textit{satisfying}\ X_{t_j^n}^2-2M_{t_j^n}^n = \sum_{i=1}^{j}\left(X_{t_i^n}-X_{t_{i-1}^n}\right)^2,
\end{align*}
is also a bounded martingale. 

\paragraph{Claim.\label{claim:3.57}} $\lim_{n,m\to\infty}\E\left[(M_T^n-M_T^m)^2\right] = 0$.
\begin{proof}[Proof of the Claim]
We fix $m\leq n$ and evaluate the product $\E[M_T^nM_T^m]$:
\begin{align*}
	\E[M_T^nM_T^m] &= \sum_{i=1}^{k_m}\sum_{j=1}^{k_n}\E\left[X_{t_{i-1}^m}\left(X_{t_i^m}-X_{t_{i-1}^m}\right)X_{t_{j-1}^n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)\right]\\
	&=\sum_{j=1}^{k_n}\,\sum_{i:(t_{i-1}^m,t_i^m]\supset(t_{j-1}^n,t_j^n]}\E\left[X_{t_{i-1}^m}\left(X_{t_i^m}-X_{t_{i-1}^m}\right)X_{t_{j-1}^n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)\right]\\
	&=\sum_{j=1}^{k_n}\,\sum_{i:(t_{i-1}^m,t_i^m]\supset(t_{j-1}^n,t_j^n]}\,\sum_{l:(t_{l-1}^n,t_l^n]\subset(t_{i-1}^m,t_i^m]}\E\left[X_{t_{i-1}^m}\left(X_{t_l^n}-X_{t_{l-1}^n}\right)X_{t_{j-1}^n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)\right]\\
	&=\sum_{j=1}^{k_n}\,\sum_{i:(t_{i-1}^m,t_i^m]\supset(t_{j-1}^n,t_j^n]}\E\left[X_{t_{i-1}^m}X_{t_{j-1}^n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\right]\tag{3.11}\label{eq:3.11}
\end{align*}
The second equality holds because once $t_{j-1}^n\geq t_i^m$ (resp. $t_j^n\leq t_{i-1}^m$), we can take conditional expectation with respect to $\mathscr{F}_{t_{j-1}^n}$ (resp. $\mathscr{F}_{t_{i-1}^m}$) to eliminate the corresponding term in the double sum. The fourth equality because once $l<j$ (resp. $l>j$), we can take conditional expectation with respect to $\mathscr{F}_{t_{j-1}^n}$ (resp. $\mathscr{F}_{t_{l-1}^n}$) to eliminate the corresponding term in the triple sum. As a special case of \hyperref[eq:3.11]{(3.11)}, we have
\begin{align*}
	\E\left[(M_T^n)^2\right] &= \sum_{j=1}^{k_n}\E\left[X_{t_{j-1}^n}^2\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\right].\tag{3.12}\label{eq:3.12}
\end{align*}
And by \hyperref[prop:3.39]{Proposition 3.39},
\begin{align*}
	\E\left[(M_T^m)^2\right] = \sum_{i=1}^{k_m}\E\left[X_{t_{i-1}^m}^2\left(X_{t_i^m}-X_{t_{i-1}^m}\right)^2\right] &=\sum_{i=1}^{k_m}\E\left[X_{t_{i-1}^m}^2\E\left[\left.\left(X_{t_i^m}-X_{t_{i-1}^m}\right)^2\right|\mathscr{F}_{t_{i-1}^m}\right]\right]\\
	&= \sum_{i=1}^{k_m}\,\sum_{j:(t_{j-1}^n,t_j^n]\subset(t_{i-1}^m,t_i^m]}\E\left[X_{t_{i-1}^m}^2\E\left[\left.\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\right|\mathscr{F}_{t_{i-1}^m}\right]\right]\\
	&= \sum_{j=1}^{k_n}\,\sum_{i:(t_{i-1}^m,t_i^m]\supset(t_{j-1}^n,t_j^n]}\E\left[X_{t_{i-1}^m}^2\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\right].\tag{3.13}\label{eq:3.13}
\end{align*}
Note that for every $j\in\{1,\cdots,k_n\}$, there is a unique $i\in\{1,\cdots,k_m\}$ such that $(t_{i-1}^m,t_i^m]\supset(t_{j-1}^n,t_j^n]$. Combining \hyperref[eq:3.11]{(3.11)}, \hyperref[eq:3.12]{(3.12)} and \hyperref[eq:3.13]{(3.13)}, we have
\begin{align*}
	\E\left[(M_T^n-M_T^m)^2\right] &= \sum_{j=1}^{k_n}\,\sum_{i:(t_{i-1}^m,t_i^m]\supset(t_{j-1}^n,t_j^n]}\E\left[\left(X_{t_{j-1}^n}-X_{t_{i-1}^m}\right)^2\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\right]\\
	&\leq \E\left[\sup_{1\leq j\leq k_n,\ (t_{i-1}^m,t_i^m]\supset(t_{j-1}^n,t_j^n]}\left(X_{t_{j-1}^n}-X_{t_{i-1}^m}\right)^2 \sum_{j=1}^{k_n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\right]\\
	&\leq \E\left[\sup_{1\leq j\leq k_n,\ (t_{i-1}^m,t_i^m]\supset(t_{j-1}^n,t_j^n]}\left(X_{t_{j-1}^n}-X_{t_{i-1}^m}\right)^4\right]^{\frac{1}{2}} \E\left[\biggl(\sum_{j=1}^{k_n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\biggr)^2\right]^{\frac{1}{2}}\tag{3.14}\label{eq:3.14}
\end{align*}
By continuity of the sample paths of $X$ and the fact that $X$ is bounded (so we can use dominated convergence theorem), the first term in \hyperref[eq:3.14]{(3.14)} converges to $0$ as $n,m\to\infty$. Hence our result follows if we can bound the second term with a finite constant independent of $n$. Suppose that $\vert X_t\vert\leq K$ for all $t\geq 0$. Then
\begin{align*}
\E&\left[\biggl(\sum_{j=1}^{k_n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\biggr)^2\right] = \sum_{j=1}^{k_n}\E\left[\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^4\right] + 2\sum_{1\leq j < i\leq k_n}\E\left[\left(X_{t_i^n}-X_{t_{i-1}^n}\right)^2\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\right]\\
&\leq 4K^2\sum_{j=1}^{k_n}\E\left[\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\right] + 2\sum_{j=1}^{k_n-1}\E\left[\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\sum_{i=j+1}^{k_n}\E\left[\left(X_{t_i^n}-X_{t_{i-1}^n}\right)^2|\mathscr{F}_{t_j^n}\right]\right]\\
&= 4K^2\E\left[\left(X_T-X_0\right)^2\right] + 2\sum_{j=1}^{k_n-1}\E\left[\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\E\left[\left(X_{T}-X_{t_j^n}\right)^2|\mathscr{F}_{t_j^n}\right]\right] \tag{By \hyperref[prop:3.39]{Proposition 3.39}}\\
&\leq 12K^2\E\left[\left(X_T-X_0\right)^2\right]\leq 48K^4.
\end{align*}
Hence we can bound the second term in \hyperref[eq:3.14]{(3.14)} by $4\sqrt{3}K^2$, which completes the proof.
\end{proof}

\textit{Proof of \hyperref[thm:3.56]{Theorem 3.56} (Cont). Step III:} By Doob's $L^p$-inequality [\hyperref[prop:3.40]{Proposition 3.40 (ii)}] and our claim, 
\begin{align*}
	0\leq \lim_{n,m\to\infty}\E\left[\sup_{0\leq t\leq T}\left(M_t^n-M_t^m\right)^2\right] \leq \lim_{n,m\to\infty} 4\E\left[\left(M_T^n-M_T^m\right)^2\right] = 0.
\end{align*}

Hence for all $t\in[0,T]$, $(M_t^n)_{n=1}^\infty$ is a Cauchy sequence in $L^2(\Omega,\mathscr{F},\P)$ and thus converges in $L^2$. We choose a subsequence $n_k\nearrow\infty$ such that
\begin{align*}
	\E\left[\sup_{0\leq t\leq T}\left(M_t^{n_{k+1}}-M_t^{n_k}\right)^2\right] < 2^{-k},\ \forall k\in\mathbb{N}.
\end{align*}
As a result,
\begin{align*}
	\E\left[\sum_{k=1}^\infty\sup_{0\leq t\leq T}\left\vert M_t^{n_{k+1}}-M_t^{n_k}\right\vert\right] < \sum_{k=1}^\infty 2^{-k} < \infty\quad\Rightarrow\quad \sum_{k=1}^\infty\sup_{0\leq t\leq T}\left\vert M_t^{n_{k+1}}-M_t^{n_k}\right\vert < \infty\ \textit{a.s.}.
\end{align*}

Therefore, except on a negligible set $N$ where the series in the above display diverges, the function sequence $t\mapsto M_t^{n_k}(\omega)$ converges uniformly on $[0,T]$ as $k\to\infty$. Let $Y_t(\omega) = \lim_{k\to\infty}M_t^{n_k}(\omega)$ for all $t\in[0,T]$ if $\omega\notin N$, and otherwise $Y_t(\omega)=0$ for all $t\in[0,T]$. Then $(Y_t)_{t\geq 0}$ has continuous sample paths. Also, $Y_t$ is adapted by completeness of our filtration $(\mathscr{F}_t)_{t\geq 0}$. Moreover, since $(M_n^t)_{n=1}^\infty$ converges in $L^2$, it must converges to the $a.s.$ limit $Y_t$ in $L^2$. Also, since the conditional expectation is a bounded linear operator in $L^2(\Omega,\mathscr{F},\P)$, we can pass the martingale property of $M_t^n$ to $Y_t$ to obtain that $\E[Y_t|\mathscr{F}_s]=Y_s$ for all $0\leq s<t\leq T$. Hence $(Y_{t\wedge T})_{t\geq 0}$ is a sample-continuous martingale. 

Meanwhile, the process $X_t^2 - 2M_t^n$ restricted to the finite sequence $(t_j^n)_{j=1}^{k_n}$ is increasing. Take the limit $n_k\nearrow\infty$, we have $X_t^2-2M_t^{n_k}\rightrightarrows X_t^2-Y_t$ on $[0,T]$ except possibly on the negligible set $N$. Set $V_t^T = X_t^2-2Y_t$ on $\Omega\backslash N$, and $V_t^T=0$ on $N$. Then $V_0^T=0$, $V_t^T$ is $\mathscr{F}_t$-measurable for all $t\in[0,T]$, and $V^T$ has increasing continuous sample paths. Also, $X_{t\wedge T}^2-V_{t\wedge T}^T$ is a sample-continuous martingale. 

For every $T\in\mathbb{N}$, by the uniqueness argument proposed in \textit{Step I}, we have $V_{t\wedge T}^T=V_{t\wedge T}^{T+1}\ a.s.$. Hence we can define an increasing process $\langle X,X\rangle_t=V_t^T$ for all $t\in[0,T]$ and all $T\in\mathbb{N}$. Clearly, $X_t^2-\langle X,X\rangle_t$ is a sample-continuous martingale. To obtain \hyperref[eq:3.10]{(3.10)}, note that $X_{t\wedge T}^2-V_{t\wedge T}^T$ and $X_{t\wedge T}^2-\langle X,X\rangle_{t\wedge T}$ are martingales. Again by the uniqueness argument, we have $V_{t\wedge T}^T=\langle X,X\rangle_{t\wedge T}\ a.s.$, and particularly, $V_T^T=\langle X,X\rangle_T\ a.s.$. Note that $M_T^n\to Y_T = \frac{1}{2}(X_T^2-V_T^T)$ in $L^2$, we have
\begin{align*}
	X_T^2 - 2M_T^n = \sum_{j=1}^{k_n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)^2\ \overset{L^2}{\to}\ V_T^T = \langle X,X\rangle_T\ a.s..
\end{align*}
Then the proof for the case where $X_0=0$ and $X$ is bounded is completed.\vspace{0.1cm}

\textit{Step IV:} If $X_0=0$, but $X$ is not bounded, let $\tau_n=\inf\{t\geq 0:\vert X_t\vert\geq n\}$. By \hyperref[prop:3.54]{Proposition 3.54 (iii)}, the stopped process $X^{\tau_n}$ is a bounded martingale, and we set $V^{[n]}=\langle X^{\tau_n},X^{\tau_n}\rangle$. Again, the uniqueness argument shows that $V_t^{[n]}$ and $V_{t\wedge\tau_n}^{[n+1]}$ are indistinguishable. Then there exists an increasing and continuous process $V$ such that $V_{t\wedge\tau_n}=V_t^{[n]}\ a.s.$ for all $t\geq 0$, and $X_{t\wedge\tau_n}^2-V_{t\wedge\tau_n}$ is a martingale for every $n\in\mathbb{N}$. As a result, $X_t^2-V_t$ is a continuous local martingale, and taking $\langle X,X\rangle_t = V_t$ suffices. 

To obtain \hyperref[eq:3.10]{(3.10)} (in probability), note that for all $\eta>0$,
\begin{align*}
	\P\left(\left\vert\sum_{j=1}^{k_m}\left(X_{t_j}-X_{t_{j-1}}\right)^2 - \langle X,X\rangle_t\right\vert\geq\eta\right)\leq\frac{1}{\eta^2}\biggl\Vert\sum_{j=1}^{k_m}\left(X_{t_j}^{\tau_n}-X_{t_{j-1}}^{\tau_n}\right)^2 - \langle X,X\rangle_{t\wedge\tau_n}\biggr\Vert_2^2 + \P(\tau_n<t). \tag{3.15}\label{3.15}
\end{align*}

In \hyperref[eq:3.15]{(3.15)}, the first term converges to $0$ as $m\to\infty$, because \hyperref[eq:3.10]{(3.10)} holds in $L^2$ when we replace $X$ and $\langle X,X\rangle_t$ by $X^{\tau_n}$ and $\langle X,X\rangle_{t\wedge\tau_n}$, respectively. Also, the second term converges to $0$ as $n\to\infty$ by definition.
\vspace{0.1cm}

\textit{Step V:}  For the general case, we write $X_t=X_0+Z_t$, so $X_t^2=X_0^2 + 2X_0Z_t + Z_t^2$. By \hyperref[prop:3.54]{Proposition 3.54 (iv)}, the process $X_0Z_t$ is also a continuous local martingale. Hence $X_t^2-\langle Z,Z\rangle_t$ remains a continuous local martingale. Meanwhile, \hyperref[eq:3.10]{(3.10)} does not change by adding a $\mathscr{F}_0$-measurable variable $X_0$.
\end{proof}
\renewcommand{\proofname}{Proof}

\paragraph{Remark.} According to our proof in \textit{Step V}, the quadratic variation of a continuous local martingale $X=(X_t)_{t\geq 0}$ does not depend on the initial value $X_0$, i.e. if we write $X_t=X_0+Z_t$, then we have $\langle X,X\rangle_t = \langle Z,Z\rangle_t$.

\paragraph{Proposition 3.57.\label{prop:3.57}} Let $X=(X_t)_{t\geq 0}$ be a continuous local martingale.
\begin{itemize}
	\item[(i)] If $\tau$ is a stopping time, then $\langle X^\tau,X^\tau\rangle_t=\langle X,X\rangle_{t\wedge\tau}$.
	\item[(ii)] Assume $X_0=0$. Then $\langle X,X\rangle=0$ if and only if $X=0\ a.s.$.
\end{itemize}
\begin{proof}
(i) Since the stopped process $X_{t\wedge\tau}^2-\langle X,X\rangle_{t\wedge\tau}$ is a continuous local martingale, the result follows.

(ii) Assume $\langle X,X\rangle_t=0$ for all $t\geq 0$. Then $X_t^2 - 0$ is a nonnegative continuous local martingale, hence a supermartingale by \hyperref[prop:3.54]{Proposition 3.54}. This implies $\E[X_t^2]\leq\E[X_0]=0$, and $X_t=0\ a.s.$. To prove that $X=0\ a.s.$, take the intersection of $\{X_t=0,\ t\in D\}$ for a dense set $D\subset\mathbb{R}$, then use sample-path continuity.
\end{proof}

\paragraph{Theorem 3.58.\label{thm:3.58}} Let $X=(X_t)_{t\geq 0}$ be a continuous local martingale such that $X_0\in L^2(\Omega,\mathscr{F},\P)$. Then the following are equivalent:
\begin{itemize}
	\item[(i)] $X$ is a martingale, and $\sup_{t\geq 0}\E[\vert X_t\vert^2]<\infty$.
	\item[(ii)] $\E\left[\langle X,X\rangle_\infty\right]<\infty$.
\end{itemize}
Furthermore, if these properties hold, then $X_t^2-\langle X,X\rangle_t$ is a uniformly integrable martingale, and in particular we have $\E[X_\infty^2]=\E[X_0^2]+\E[\langle X,X\rangle_\infty]$.
\begin{proof}
Without loss of generality let $X_0=0$. 

(i) $\Rightarrow$ (ii): By Doob's $L^p$-inequality [\hyperref[prop:3.40]{Proposition 3.40}] and monotone convergence theorem, we have
\begin{align*}
	\E\left[\sup_{0\leq t\leq T}\vert X_t\vert^2\right]\leq 4\E[\vert X_T\vert^2],\ \forall T\in\mathbb{R}_+ \quad\Rightarrow\quad \E\left[\sup_{t\geq 0}\vert X_t\vert^2\right]\leq 4\sup_{t\geq 0}\E[\vert X_t\vert^2] < \infty
\end{align*}

Let $\sigma_n=\inf\{t\geq 0:\langle X,X\rangle_t\geq n\}\nearrow\infty$. Then the continuous local martingale $X_{t\wedge\sigma_n}^2-\langle X,X\rangle_{t\wedge\sigma_n}$ is dominated by the integrable variable $\sup_{t\geq 0}\vert X_t\vert^2 + n$. According to \hyperref[prop:3.54]{Proposition 3.54 (ii)}, this is a uniformly integrable martingale, and
\begin{align*}
	\E\left[\langle X,X\rangle_{t\wedge\sigma_n}\right]=\E\left[X_{t\wedge\sigma_n}^2\right]\leq\E\left[\sup_{t\geq 0}\vert X_t\vert^2\right]\leq 4\sup_{t\geq 0}\E[\vert X_t\vert^2].
\end{align*}
Let $n\to\infty$ and $t\to\infty$, we have $\E[\langle X,X\rangle_\infty]\leq4\sup_{t\geq 0}\E[\vert X_t\vert^2]<\infty$ by monotone convergence theorem. \vspace{0.1cm}

(ii) $\Rightarrow$ (i): Let $\tau_n=\left\{t\geq 0:\vert X_t\vert\geq n\right\}$. Then the continuous local martingale $X_{t\wedge\tau_n}^2-\langle X,X\rangle_{t\wedge\tau_n}$ is dominated by the integrable variable $\langle X,X\rangle_\infty + n^2$. According to \hyperref[prop:3.54]{Proposition 3.54 (ii)}, this is a uniformly integrable martingale. By Fatou's lemma, we have
\begin{align*}
	\E\left[X_{t\wedge\tau_n}^2\right]=\E\left[\langle X,X\rangle_{t\wedge\tau_n}\right]\leq\E\left[\langle X,X\rangle_\infty\right] < \infty\quad\Rightarrow\quad \E[X^2_t]\leq\liminf_{n\to\infty}\E\left[X_{t\wedge\tau_n}^2\right]\leq\E\left[\langle X,X\rangle_\infty\right]<\infty.
\end{align*}
Meanwhile, the sequence $\vert X_{t\wedge\tau_n}\vert\nearrow\vert X_t\vert$ as $n\to\infty$, and $(X_{t\wedge\tau_n})_{n=1}^\infty$ is uniformly integrable:
\begin{align*}
	\lim_{M\to\infty}\sup_{n\in\mathbb{N}}\E\left[\vert X_{t\wedge\tau_n}\vert\mathds{1}_{\{\vert X_{t\wedge\tau_n}\vert\geq M\}}\right]&\leq \lim_{M\to\infty}\sup_{n\in\mathbb{N}}\E\left[X_{t\wedge\tau_n}^2\right]\E\left[\mathds{1}_{\{\vert X_{t\wedge\tau_n}\vert\geq M\}}\right]\\
	&\leq\E\left[\langle X,X\rangle_\infty\right]\lim_{M\to\infty}\E\left[\mathds{1}_{\{\vert X_t\vert\geq M\}}\right]=0.
\end{align*}

As a result, $X_{t\wedge\tau_n}\to X_t\ a.s.$ and in $L^1$. By \hyperref[prop:3.54]{Proposition 3.54 (iii)}, $(X_{t\wedge\tau_n})_{t\geq 0}$ is a martingale, and we have $\E[X_{t\wedge\tau_n}|\mathscr{F}_s]=X_{s\wedge\tau_n}$ for all $t>s\geq 0$. Convergence in $L^1$ implies $\E[X_t|\mathscr{F}_s]=X_s$, hence $X$ is a martingale.

Finally, if (i) and (ii) hold, the continuous local martingale $X_t^2-\langle X,X\rangle_t$ is dominated by the integrable variable $\langle X,X\rangle_\infty + \sup_{t\geq 0}\vert X_t\vert^2$. By \hyperref[prop:3.54]{Proposition 3.54 (ii)}, this is a uniformly integrable martingale.
\end{proof}

The following corollary is derived by applying \hyperref[thm:3.58]{Theorem 3.58} on $(X_{t\wedge T})_{t\geq 0}$ for each $T\geq 0$.

\paragraph{Corollary 3.59.\label{cor:3.59}} Let $X=(X_t)_{t\geq 0}$ be a continuous local martingale such that $X_0\in L^2(\Omega,\mathscr{F},\P)$. Then the following are equivalent:
\begin{itemize}
	\item[(i)] $X$ is a martingale, and $X_t\in L^2(\Omega,\mathscr{F},\P)$ for all $t\in\mathbb{R}_+$.
	\item[(ii)] $\E\left[\langle X,X\rangle_t\right]<\infty$ for all $t\in\mathbb{R}_+$.
\end{itemize}
Furthermore, if these properties hold, then $X_t^2-\langle X,X\rangle_t$ is a martingale.

\paragraph{Definition 3.60\label{def:3.60}} (Bracket). Given $X=(X_t)_{t\geq 0}$ and $Y=(Y_t)_{t\geq 0}$ are two continuous local martingales, their bracket $\langle X,Y\rangle$ is defined as the following finite variation process:
\begin{align*}
	\langle X,Y\rangle_t = \frac{1}{2}\left(\langle X+Y,X+Y\rangle_t - \langle X,X\rangle_t - \langle Y,Y\rangle_t\right),\ \forall t\geq 0.
\end{align*}

\paragraph{Proposition 3.61.\label{prop:3.61}}  Let $X=(X_t)_{t\geq 0}$ and $Y=(Y_t)_{t\geq 0}$ be two continuous local martingales.
\begin{itemize}
	\item[(i)] $\langle X,Y\rangle$ is the unique (up to indistinguishability) finite variation process such that $X_tY_t-\langle X,Y\rangle_t$ is a continuous local martingale.
	\item[(ii)] The mapping $(X,Y)\mapsto\langle X,Y\rangle$ is bilinear and symmetric.
	\item[(iii)] For any increasing sequence of partitions $0=t_0^n<t_1^n<\cdots<t_{k_n}^n=t$ of $[0,t]$ with mesh tending to $0$,
	\begin{align*}
		\langle X,Y\rangle_t = \lim_{n\to\infty}\sum_{j=1}^{k_n}(X_{t_j^n}-X_{t_{j-1}^n})(Y_{t_j^n}-Y_{t_{j-1}^n})\ \ \textit{in probability}.
	\end{align*}
    \item[(iv)] For every stopping time $\tau$, $\langle X^\tau,Y^\tau\rangle_t = \langle X^\tau,Y\rangle_t = \langle X,Y\rangle_{t\wedge\tau}$.
    \item[(v)] For every stopping time $\tau$, $X^\tau(Y-Y^\tau)$ is a continuous martingale.
    \item[(vi)] If $X$ and $Y$ are two $L^2$-bounded sample-continuous martingales, $X_tY_t-\langle X,Y\rangle_t$ is a uniformly integrable martingale. Consequently, $\langle X,Y\rangle_\infty$ is well-defined as the $a.s.$ limit of $\langle X,Y\rangle_t$ as $t\to\infty$, and $\E[X_\infty Y_\infty]=\E[X_0Y_0]+\E[\langle X,Y\rangle_\infty]$.
    \item[(vii)] $\langle X,Y\rangle_t=0\ a.s.$ for all $t\geq 0$ if and only if $XY$ is a continuous local martingale. In this case, the two continuous local martingales $X$ and $Y$ are said to be \textit{orthogonal}.
\end{itemize}
\begin{proof}
(i) Clearly, $X_tY_t-\langle X,Y\rangle_t$ is a continuous local martingale, and the uniqueness argument is similar to \hyperref[thm:3.56]{Theorem 3.56}. (ii) is a consequence of the uniqueness argument. (iii) follows from \hyperref[eq:3.10]{(3.10)}. (v) is a consequence of (iv), since $X_t^\tau(Y_t-Y_t^\tau) = X_t^\tau Y_t - \langle X^\tau,Y\rangle_t - (X_t^\tau Y_t^\tau - \langle X^\tau, Y^\tau\rangle_t)$. For (iv), according to (iii), we have
\begin{align*}
	\langle X^\tau,Y^\tau\rangle_t = \langle X^\tau,Y\rangle_t = \langle X,Y\rangle_t\ &\textit{on}\ \{\tau\geq t\},\\
	\langle X^\tau,Y^\tau\rangle_t - \langle X^\tau,Y^\tau\rangle_\tau = \langle X^\tau,Y\rangle_t - \langle X^\tau,Y\rangle_\tau=0\ \ &\textit{on}\ \{\tau<t\}.
\end{align*}

(vi) is a consequence of \hyperref[thm:3.58]{Theorem 3.58}. 

(vii) If $XY$ is a continuous local martingale, so is $\langle X,Y\rangle=XY-\left(XY-\langle X,Y\rangle\right)$, which is also a finite variation process. Conversely, if $\langle X,Y\rangle_t=0\ a.s.$ for all $t\geq 0$, then $XY = (XY - \langle X,Y\rangle)+\langle X,Y\rangle$ is a continuous local martingale.
\end{proof}

\paragraph{Theorem 3.62\label{thm:3.62}} (Kunita-Watanabe). Let $X$ and $Y$ be two continuous local martingales, and let $H$ and $K$ be two measurable processes. Then
\begin{align*}
	\int_0^\infty\left\vert H_s\right\vert\left\vert K_s\right\vert\left\vert\d\langle X,Y\rangle_s\right\vert\leq\left(\int_0^\infty H_s^2\,\d \langle X,X\rangle_s\right)^{1/2}\left(\int_0^\infty K_s^2\,\d \langle Y,Y\rangle_s\right)^{1/2}.
\end{align*}
\begin{proof}
Given $t>s\geq 0$, we abuse the notation $\langle X,Y\rangle_s^t = \langle X,Y\rangle_t-\langle X,Y\rangle_s$. Let $s=t_0^n<t_1^n<\cdots<t_{k_n}^n=t$ be a increasing sequence of partitions of $[s,t]$ with the mesh tending to $0$. Let $S_{XX}^n=\sum_{j=1}^{k_n}\left(X_{t_j^n} - X_{t_{j-1}^n}\right)^2$, $S_{YY}^n=\sum_{j=1}^{k_n}\left(Y_{t_j^n} - Y_{t_{j-1}^n}\right)^2$, and $S_{XY}^2=\sum_{j=1}^{k_n}\left(X_{t_j^n} - X_{t_{j-1}^n}\right)\left(Y_{t_j^n} - Y_{t_{j-1}^n}\right)$. By Cauchy-Schwarz inequality, we have $S_{XY}^n\leq\sqrt{S_{XX}^nS_{YY}^n}$. Note that $\langle X,Y\rangle_s^t - \sqrt{\langle X,X\rangle_s^t\langle Y,Y\rangle_s^t}\overset{\P}{\to}S_{XY}^n-\sqrt{S_{XX}^nS_{YY}^n}$. For all $\eta>0$,
\begin{align*}
	\P\left(\langle X,Y\rangle_s^t - \sqrt{\langle X,X\rangle_s^t\langle Y,Y\rangle_s^t} > \eta\right)\leq\P\left(\langle X,Y\rangle_s^t - \sqrt{\langle X,X\rangle_s^t\langle Y,Y\rangle_s^t} - S_{XY}^n + \sqrt{S_{XX}^nS_{YY}^n}> \eta\right)\to 0.
\end{align*}

By taking the union of all rationals $\eta>0$, we have $\langle X,Y\rangle_s^t - \sqrt{\langle X,X\rangle_s^t\langle Y,Y\rangle_s^t}\leq 0\ a.s.$. Since $t>s\geq 0$ are arbitrary, $\langle X,Y\rangle_s^t - \sqrt{\langle X,X\rangle_s^t\langle Y,Y\rangle_s^t}\leq 0$ holds for all rationals $t>s\geq 0$ for $a.s.$ $\omega\in\Omega$. By continuity of $X$ and $Y$, we have for $a.s.\ \omega\in\Omega$ that $\langle X,Y\rangle_s^t - \sqrt{\langle X,X\rangle_s^t\langle Y,Y\rangle_s^t}\leq 0$ for all reals $t>s\geq 0$.

Now we fix $\omega\in\Omega$ with $\langle X,Y\rangle_s^t - \sqrt{\langle X,X\rangle_s^t\langle Y,Y\rangle_s^t}\leq 0$ for all reals $t>s\geq 0$. Then all remaining results are deterministic. For any subdivisions $s=t_0<t_1<\cdots<t_k=t$, we have
\begin{align*}
	\sum_{j=1}^k\left\vert\langle X,Y\rangle_{t_{j-1}}^{t_j}\right\vert\leq\sum_{j=1}^k\sqrt{\langle X,X\rangle_{t_{j-1}}^{t_j}}\sqrt{\langle Y,Y\rangle_{t_{j-1}}^{t_j}}\leq\sqrt{\sum_{j=1}^k\langle X,X\rangle_{t_{j-1}}^{t_j}}\sqrt{\sum_{j=1}^k\langle Y,Y\rangle_{t_{j-1}}^{t_j}}=\sqrt{\langle X,X\rangle_s^t}\sqrt{\langle Y,Y\rangle_s^t}.
\end{align*}
Let the mesh of our partition tends to $0$, we obtain
\begin{align*}
	\int_s^t\left\vert\d\langle X,Y\rangle_u\right\vert \leq\sqrt{\langle X,X\rangle_s^t}\sqrt{\langle Y,Y\rangle_s^t}
\end{align*}
Fix $T>0$, and let $\mathscr{M}_T$ be the collection of all $A\in\mathscr{B}([0,T])$ such that
\begin{align*}
\int_A\left\vert\d\langle X,Y\rangle_u\right\vert \leq\sqrt{\int_A d\langle X,X\rangle_u}\sqrt{\int_A d\langle Y,Y\rangle_u}.\tag{3.16}\label{eq:3.16}
\end{align*}
By monotone convergence theorem, $\mathscr{M}_T$ is a monotone class, and it contains the collection of all finite intersections of closed intervals in $[0,T]$, which is an algebra. By monotone class theorem [\hyperref[thm:1.11]{Theorem 1.11}], we have $\mathscr{M}_T=\mathscr{B}([0,T])$. As a result, \hyperref[eq:3.16]{(3.16)} holds for all bounded Borel sets $A\in\mathscr{B}(\mathbb{R}_+)$. Also, for all nonnegative simple functions $h,k$ on $[0,T]$, choose finite partition $A_1,\cdots,A_m$ of $[0,T]$ such that $h=\sum_{i=1}^m \alpha_i\mathds{1}_{A_i}$ and $k=\sum_{i=1}^m \beta_j\mathds{1}_{A_i}$. Then we have
\begin{align*}
	\int h(s)k(s)\,\vert\d\langle X,Y\rangle_s\vert = \sum_{i=1}^m\alpha_i\beta_i \int_{A_i}\vert\d\langle X,Y\rangle_s\vert &\leq\sqrt{\sum_{i=1}^m\alpha_i^2\int_{A_i} d\langle X,X\rangle_u}\sqrt{\sum_{i=1}^m\beta_i^2\int_{A_i} d\langle Y,Y\rangle_u}\\
	&= \sqrt{\int h(s)^2\,\d \langle X,X\rangle_s}\sqrt{\int k(s)^2\,\d \langle Y,Y\rangle_s}
\end{align*}
Note that every nonnegative measurable function on $[0,T]$ is the limit of an increasing sequence of nonnegative simple functions $[0,T]$, and every nonnegative measurable function $h$ on $\mathbb{R}_+$ is the increasing limit of $h\mathds{1}_{[0,T]}$ as $T\to\infty$. Hence an application of monotone convergence theorem finishes the proof.
\end{proof}

\paragraph{Definition 3.63\label{def:3.63}} (Continuous semimartingales). A process $X=(X_t)_{t\geq 0}$ is said to be a \textit{continuous semimartingale} if it can be written as
\begin{align*}
	X_t = M_t + A_t,\ \forall t\in\mathbb{R}_+, \tag{3.17}\label{eq:3.17}
\end{align*}
where $M=(M_t)_{t\geq 0}$ is a continuous local martingale and $A=(A_t)_{t\geq 0}$ is a continuous finite variation process.
\paragraph{Remark.} Thanks to \hyperref[prop:3.55]{Proposition 3.55}, the decomposition \hyperref[eq:3.17]{(3.17)} is unique up to indistinguishability. We call this the \textit{canonical decomposition} of a continuous semimartingale $X$.

\paragraph{Definition 3.64\label{def:3.64}} (Brackets). Given two continuous semimartingale $X=M+A$ and $Y=M^\prime+A^\prime$ (which are canonical decompositions), we define the bracket $\langle X,Y\rangle=\langle M,M^\prime\rangle$, which is a finite variation process.

\paragraph{Proposition 3.65.\label{prop:3.65}} Let $X=(X_t)_{t\geq 0}$ and $Y=(Y_t)_{t\geq 0}$ be two continuous semimartingales. Let $t>0$. Let $0=t_0^n<t_1^n<\cdots<t_{k_n}^n=t$ be any increasing sequence of partitions of $[0,t]$ whose mesh tends to $0$. Then
\begin{align*}
	\langle X,Y\rangle_t = \lim_{n\to\infty}\sum_{j=1}^{k_n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)\left(Y_{t_j^n}-Y_{t_{j-1}^n}\right)\ \textit{in probability}.
\end{align*}
\begin{proof}
Let $X=M+A$ and $Y=M^\prime + A^\prime$ be the canonical decompositions. Then
\begin{align*}
	\sum_{j=1}^{k_n}\left(X_{t_j^n}-X_{t_{j-1}^n}\right)\left(Y_{t_j^n}-Y_{t_{j-1}^n}\right) &= \sum_{j=1}^{k_n}\left(M_{t_j^n}-M_{t_{j-1}^n}\right)\left(M^\prime_{t_j^n}-M^\prime_{t_{j-1}^n}\right) + \sum_{j=1}^{k_n}\left(M_{t_j^n}-M_{t_{j-1}^n}\right)\left(A^\prime_{t_j^n}-A^\prime_{t_{j-1}^n}\right)\\
	&\quad + \sum_{j=1}^{k_n}\left(A_{t_j^n}-A_{t_{j-1}^n}\right)\left(M^\prime_{t_j^n}-M^\prime_{t_{j-1}^n}\right) + \sum_{j=1}^{k_n}\left(A_{t_j^n}-A_{t_{j-1}^n}\right)\left(A^\prime_{t_j^n}-A^\prime_{t_{j-1}^n}\right)
\end{align*}
According to \hyperref[prop:3.61]{Proposition 3.61 (iii)},
\begin{align*}
	\lim_{n\to\infty}\sum_{j=1}^{k_n}\left(M_{t_j^n}-M_{t_{j-1}^n}\right)\left(M^\prime_{t_j^n}-M^\prime_{t_{j-1}^n}\right) = \langle M,M^\prime\rangle_t = \langle X,Y\rangle_t\ \textit{in probability}.
\end{align*}
Also, note that
\begin{align*}
	\left\vert\sum_{j=1}^{k_n}\left(M_{t_j^n}-M_{t_{j-1}^n}\right)\left(A^\prime_{t_j^n}-A^\prime_{t_{j-1}^n}\right)\right\vert&\leq\left(\int_0^t\vert \d A_s^\prime\vert\right)\sup_{1\leq j\leq k_n}\left\vert M_{t_j^n} - M_{t_{j-1}^n}\right\vert\to 0,\ a.s.,\\
	\left\vert\sum_{j=1}^{k_n}\left(A_{t_j^n}-A_{t_{j-1}^n}\right)\left(M^\prime_{t_j^n}-M^\prime_{t_{j-1}^n}\right)\right\vert&\leq\left(\int_0^t\vert\d A_s\vert\right)\sup_{1\leq j\leq k_n}\left\vert M_{t_j^n}^\prime - M_{t_{j-1}^n}^\prime\right\vert\to 0,\ a.s.,\\
	\left\vert\sum_{j=1}^{k_n}\left(A_{t_j^n}-A_{t_{j-1}^n}\right)\left(A^\prime_{t_j^n}-A^\prime_{t_{j-1}^n}\right)\right\vert&\leq\left(\int_0^t\vert\d A_s\vert\right)\sup_{1\leq j\leq k_n}\left\vert A_{t_j^n}^\prime - A_{t_{j-1}^n}^\prime\right\vert\to 0,\ a.s.,
\end{align*}
where the $a.s.$ convergence holds by continuity of sample paths of $M$, $M^\prime$ and $A^\prime$.
\end{proof}

\newpage
\section{Brownian Motions}
\subsection{Pre-Brownian Motions and Brownian Motions}
\paragraph{Definition 4.1\label{def:4.1}} (Gaussian spaces). A (centered) \textit{Gaussian space} $H$ is a closed subspace of $L^2(\Omega,\mathscr{F},\P)$ that contains only centered Gaussian variables.
\paragraph{Remark.} To justify the closedness of a Gaussian space $H\subset L^2(\Omega,\mathscr{F},\P)$, we let $H\ni X_n\sim N(0,\sigma_n^2)\overset{L^2}{\to} X$. Convergence in $L^2$ implies $\E[X_n^2]=\sigma^2_n\to\sigma^2$. Then for all $\lambda\in\mathbb{R}$, by dominated convergence theorem,
\begin{align*}
	\E\left[\e^{\i\lambda X}\right] = \lim_{n\to\infty}\E\left[\e^{\i\lambda X_n}\right] = \lim_{n\to\infty}\exp\left(-\frac{\sigma_n^2}{2}\lambda^2\right)=\exp\left(-\frac{\sigma^2}{2}\lambda^2\right).
\end{align*}
Hence $X\sim N(\mu,\sigma^2)\in H$. Furthermore, since $L^2(\Omega,\mathscr{F},\P)$ is a complete space, so is a Gaussian subspace.

We can make $H$ a Hilbert space by define the inner product $\langle X,Y\rangle = \E[XY]$ for $X,Y\in H$. In this space, orthogonality and independence are equivalent. To be specific, in the Gaussian space $H$, two variables $X$ and $Y$ are independent if and only if they are orthogonal, i.e. $\E[XY]=0$. To see the ``if'' case, note that $X,Y$ are jointly Gaussian. Then for all $s,t\in\mathbb{R}$,
\begin{align*}
	\E\left[\e^{\i(sX+tY)}\right] &= \exp\left(-\frac{s^2\E[X^2] + 2st\,\E[XY] + t^2\E[Y^2]}{2}\right)\\
	&= \exp\left(-\frac{s^2\E[X^2]}{2}\right)\exp\left(-\frac{t^2\E[Y^2]}{2}\right) = \E\left[\e^{\i sX}\right]\E\left[\e^{\i tY}\right].
\end{align*}
By \hyperref[cor:2.44]{Corollary 2.44}, $X$ and $Y$ are independent.

Likewise, assume that $G,K$ are two subspaces of the Gaussian space $H$. Then $G\perp K$ if and only if the sub $\sigma$-algebras $\sigma(G)$ and $\sigma(K)$ generated by $G$ and $K$ are independent.

We also point out the equivalence between orthogonal projection onto a Gaussian space and conditional expectation. If $H$ is a Gaussian space, and $G$ is a closed subspace of $H$, then for all $X\in H$, the conditional expectation $\E[X|\sigma(G)]$ is the projection of $X$ onto $G$. To see this, let $\xi$ be the orthogonal projection of $X$ onto $G$, so that $X-\xi\perp G$. As a result, $\E[X|\sigma(G)]=\E[\xi + (X-\xi)|\sigma(G)] = \E[\xi|\sigma(G)] = \xi$.

\paragraph{Definition 4.2\label{def:4.2}} (Gaussian white noise). Let $(E,\mathscr{E})$ be a measurable space, and let $\mu$ be a $\sigma$-finite measure on $(E,\mathscr{E})$. A \textit{Gaussian white noise with intensity $\mu$} is an isometry $W$ from $L^2(E,\mathscr{E},\mu)$ into a Gaussian space.

\paragraph{Remark I.} According to the polarization identity, an isometry $W$ also preserves inner product. Therefore, if $f,g\in L^2(E,\mathscr{E},\mu)$, then we have
\begin{align*}
	\E[W(f)W(g)] = \langle f,g\rangle = \int fg\,\d \mu,\quad \textit{and in particular}\ \ \E[W(f)^2]=\Vert f\Vert_2 = \int \vert f\vert^2\,\d \mu.
\end{align*}
If $f=\mathds{1}_A$ with $\mu(A)<\infty$, we write $W(A)=W(\mathds{1}_A)$, and $W(A)\sim N(0,\mu(A))$.

\paragraph{Remark II.} Given any $\sigma$-finite measure $\mu$ on $(E,\mathscr{E})$, we can always find a Gaussian white noise with intensity $\mu$ on an appropriate probability space $(\Omega,\mathscr{F},\P)$. Let $\{e_\lambda,\lambda\in\Lambda\}$ be an orthonormal basis of $L^2(E,\mathscr{E},\mu)$. According to \hyperref[cor:4.15]{Corollary 4.15}, we define $(\Omega,\mathscr{F},\P)=(\mathbb{R}^\Lambda,\mathscr{B}(\mathbb{R})^{\otimes\Lambda},\mathbb{G})$, where $\mathbb{G}$ extends Gaussian measures
\begin{align*}
	\mathbb{G}_{(t_1,\cdots,t_n)}(A)=\frac{1}{(2\pi)^{n/2}}\int_A\e^{-\frac{1}{2}(z_1^2+\cdots+z_n^2)}\,\d z,\ \forall t_1,\cdots,t_n\in\mathcal{T},\ \forall A\in\mathscr{B}(\mathbb{R}^n).
\end{align*}
Then the coordinate maps $(\pi_\lambda)_{\lambda\in\Lambda}$ is a collection of independent standard Gaussian variables.

Now for every $f\in L^2(E,\mathscr{E},\mu)$, we define
\begin{align*}
	W(f)=\sum_{\lambda\in\Lambda}\langle f,e_\lambda\rangle \pi_\lambda.
\end{align*}
This series converges in $L^2$ since $\{e_\lambda,\lambda\in\Lambda\}$ is an orthonormal basis of $L^2(E,\mathscr{E},\mu)$. Hence $W$ takes values in the Gaussian space $H=\overline{\mathrm{span}}\left(\pi_\lambda\right)_{\lambda\in\Lambda}$. Since $W$ maps an orthonormal basis in $L^2(E,\mathscr{E},\mu)$ to one in $H$, it is an isometry. Thus we find a Gaussian white noise $W$ with intensity $\mu$.

\paragraph{Remark III.} Given a measurable set $A$ in $(E,\mathscr{E},\mu)$ with $\mu(A)<\infty$, we can approximate $\mu(A)$ with a Gaussian white noise $W$ with intensity $\mu$. Let $A=A_1^n\amalg\cdots\amalg A_{k_n}^n$ be a sequence of partitions of $A$ such that
\begin{align*}
	\lim_{n\to\infty}\left(\max_{j\in\{1,\cdots,k_n\}}\mu(A_j^n)\right) = 0.
\end{align*}
Then $W(A_j^n),\ j=1,\cdots,k_n$ are independent Gaussian variables, and $\E[W(A_j^n)^2]=\mu(A_j)$. Furthermore,
\begin{align*}
	\E\left[\left(\sum_{j=1}^{k_n} W(A_j^n)^2-\mu(A)\right)^2\right] &= \sum_{j=1}^{k_n}\E\left[\left(W(A_j^n)^2-\mu(A_j^n)\right)^2\right] = 2\sum_{j=1}^{k_n}\mu(A_j^n)^2 \leq 2\mu(A)\max_{1\leq j\leq k_n}\mu(A_j^n)\to 0.
\end{align*}
This implies
\begin{align*}
	\lim_{n\to\infty}\sum_{j=1}^{k_n} W(A_j^n)^2 = \mu(A)\quad \textit{in}\ L^2.
\end{align*}

Now we introduce pre-Brownian motion.

\paragraph{Definition 4.3\label{def:4.3}} (Pre-Brownian motion). Give $\mathbb{R}_+$ the Borel $\sigma$-algebra $\mathscr{B}(\mathbb{R}_+)$ and the Lebesgue measure $m$, and let $W$ be a Gaussian white noise on $\mathbb{R}_+$ with intensity $m$. The process $(B_t)_{t\geq 0}$ defined by
\begin{align*}
	B_t = W(\mathds{1}_{[0,t]}),\ \forall t\in\mathbb{R}_+
\end{align*}
is said to be a \textit{pre-Brownian motion}.

\paragraph{Remark.} By definition, a pre-Brownian motion $B=(B_t)_{t\geq 0}$ is a Gaussian process, i.e. the linear combination of any finitely many observations $B_{t_1},\cdots,B_{t_n}$ is Gaussian. The covariance function of this process is given by
\begin{align*}
	K(s,t) = \E\left[B_sB_t\right] = \int\mathds{1}_{[0,s]\cap[0,t]}\,\d m = s\wedge t,\quad \forall s,t\in\mathbb{R}_+.
\end{align*}

\paragraph{Proposition 4.4\label{prop:4.4}} (Characterization of pre-Brownian motions). Let $B=(B_t)_{t\geq 0}$ be a (real-valued) stochastic process. The following are equivalent:
\begin{itemize}
\item[(i)] $(B_t)_{t\geq 0}$ is a pre-Brownian motion.
\item[(ii)] $(B_t)_{t\geq 0}$ is a centered Gaussian process with covariance $K(s,t)=s\wedge t$.
\item[(iii)] $B_0=0\ a.s.$, and for every $t>s\geq 0$, the random variable $B_t-B_s$ is independent of $\sigma(B_r,r\in[0,s])$ and distributed according to $N(0,t-s)$.
\item[(iv)] $B_0=0\ a.s.$, and for every choice of $0=t_0<t_1<\cdots<t_n$, the variables $\{B_{t_j}-B_{t_{j-1}},j=1,\cdots,n\}$ are independent, and for every $j=1,\cdots,p$, the variable $B_{t_j}-B_{t_{j-1}}$ is distributed according to $N(0,t_j-t_{j-1})$.
\end{itemize}
\begin{proof}
The facts that (i) $\Rightarrow$ (ii) and that (iii) $\Rightarrow$ (iv) are clear. To prove (ii) $\Rightarrow$ (iii), let $H$ be the Gaussian space spanned by $\{B_r,r\in[0,s]\}$ and $X_t$. Then $B_t-B_s\in H$ is a centered Gaussian variable, and
\begin{align*}
	&\E\left[(B_t-B_s)^2\right] = t - 2 (s\wedge t) + s = t-s,\\
	&\E\left[(B_t-B_s)B_r\right] = t\wedge r - s\wedge r = r-r=0,\ \forall r\in[0,s].
\end{align*} 
Hence $X_t-X_s\sim N(0,t-s)$, and $X_t$ is independent of all $X_r$ with $r\in[0,s]$.

To prove (iii) $\Rightarrow$ (iv), it suffices to show that there exists an isometry $W$ between $L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$ and a Gaussian space $H$. For all step functions $f=\sum_{j=1}^n\lambda_j\mathds{1}_{(t_{j-1},t_j]}$ in $L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$, define
\begin{align*}
	W(f) = \sum_{j=1}^n\lambda_j(B_{t_j}-B_{t_{j-1}}).
\end{align*}
If $f,g\in L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$ are two step functions, we can find a partition $0=t_0<t_1<\cdots<t_n$ such that $f=\sum_{j=1}^n\lambda_j\mathds{1}_{(t_{j-1},t_j]}$ and $g=\sum_{j=1}^n\nu_j\mathds{1}_{(t_{j-1},t_j]}$. According to (iii), we have
\begin{align*}
	\E[W(f)W(g)] = \E\left[\sum_{j=1}^n\sum_{k=1}^n\lambda_j\nu_k(B_{t_j}-B_{t_{j-1}})(B_{t_k}-B_{t_{k-1}})\right] = \sum_{j=1}^n\lambda_j\nu_j(t_j-t_{j-1}) = \int fg\,\d m.
\end{align*}

Therefore $W$ is an isometry from the vector space of all step functions in $L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$ into the Gaussian space spanned by $\{B_t,t\in\mathbb{R}_+\}$. Since the step functions are dense in $L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$, we immediately extend $W$ to an isometry between $L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$ and $\mathrm{span}\left\{B_t,t\geq 0\right\}$.
\end{proof}

\paragraph{Remark.} According to our proof of (iii) $\Rightarrow$ (iv), we can determine a Gaussian noise $W$ with intensity $m$ given a pre-Brownian motion $B=(B_t)_{t\geq 0}$. For all $f\in L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$, we write the notation
\begin{align*}
	W(f)=\int_0^\infty f(s)\,\d B_s,\quad W(f\mathds{1}_{[0,t]})=\int_0^t f(s)\,\d B_s,\quad \textit{and}\quad W(f\mathds{1}_{(s,t]})=\int_s^t f(r)\,\d B_r,\ \forall t>s\geq 0.
\end{align*}

The mapping $W:f\mapsto \int_0^\infty f(s)\,\d B_s$ is called the \textit{Wiener integral} with respect to $B$. Clearly, we have $W(f)\sim N\left(0,\int_0^\infty\vert f\vert^2\,\d m\right)$.

\paragraph{Proposition 4.5.\label{prop:4.5}} Let $B=(B_t)_{t\geq 0}$ be a pre-Brownian motion. The following statements are true:
\begin{itemize}
	\item[(i)] (Symmetry). $-B$ is also a pre-Brownian motion.
	\item[(ii)] (Scale invariance). For all $\lambda>0$, the process $B_t^\lambda=\frac{1}{\lambda}B_{\lambda^2t}$ is a pre-Brownian motion.
	\item[(iii)] (Simple Markov property). For all $s\geq 0$, the process $B_t^{(s)}=B_t-B_s$ is a pre-Brownian motion that is independent of $\sigma(B_r,r\in[0,s])$.
	\item[(iv)] (Time inversion). The process $\widehat{B}$ defined by $\widehat{B}_0=0$ and $\widehat{B}_t=tB_{\frac{1}{t}}$ is (indistinguishably) a pre-Brownian motion.
\end{itemize}
\begin{proof}
The statement (i) is clear. (ii) follows from \hyperref[prop:4.4]{Proposition 4.4 (iv)}. For (iii), \hyperref[prop:4.4]{Proposition 4.4 (iv)} implies that $B_t^{(s)}$ is a pre-Brownian motion, and the independence argument follows from \hyperref[prop:4.4]{Proposition 4.4 (iii)}. The statement (iv) follows from \hyperref[prop:4.4]{Proposition 4.4 (ii)}.
\end{proof}

Before introducing Brownian motions, we first discuss continuity of sample paths of a stochastic process.
\paragraph{Theorem 4.6\label{thm:4.6}} (Kolmogorov's lemma). Let $X=(X_t)_{t\in I}$ be a stochastic process indexed by a bounded interval $I\subset\mathbb{R}$ and taking values in a complete metric space $(E,d)$. Assume there exist $\epsilon,q,C>0$ such that
\begin{align*}
	\E\left[d(X_s,X_t)^q\right]\leq C\vert t-s\vert^{1+\epsilon},\quad \forall s,t\in I.
\end{align*}
Then there exists an $a.s.$ modification $\widetilde{X}$ of $X$ whose sample paths are Hölder continuous of exponent $\alpha$ for all $\alpha\in(0,\frac{\epsilon}{q})$. That is, for each $\omega\in\Omega$ and each $\alpha\in(0,\frac{\epsilon}{q})$, there exists $C_\alpha(\omega)>0$ such that
\begin{align*}
	d(\widetilde{X}_s(\omega),\widetilde{X}_t(\omega)) \leq C_\alpha(\omega)\vert t-s\vert^{\alpha},\quad\forall s,t\in I.
\end{align*}
\begin{proof}
Without loss of generality, we take $I=[0,1]$ and fix $\alpha\in(0,\frac{\epsilon}{q})$. Then for all $\eta>0$,
\begin{align*}
	\P\left(d(X_s,X_t)\geq\eta\right)\leq\eta^{-q}\E\left[d(X_s,X_t)^q\right]\leq C\eta^{-q}\vert t-s\vert^{1+\epsilon},\quad\forall s,t\in I.
\end{align*}
Take $s=(j-1)2^{-n}$, $t=j2^{-n}$ and $\eta=2^{-n\alpha}$, where $n\in\mathbb{N}$ and $j\in\{1,\cdots,2^n\}$. Then
\begin{align*}
	\P\left(d(X_{(j-1)2^{-n}},X_{j2^{-n}})\geq2^{-n\alpha}\right)\leq C2^{-n(1+\epsilon-\alpha q)},\quad \forall n\in\mathbb{N},\forall j\in\{1,\cdots,2^n\}.
\end{align*}
By summing over $j$, we obtain
\begin{align*}
	\P\left(\bigcup_{j=1}^{2^n}\left\{d(X_{(j-1)2^{-n}},X_{j2^{-n}})\geq2^{-n\alpha}\right\}\right)\leq C2^{-n(\epsilon-\alpha q)},\quad \forall n\in\mathbb{N}.
\end{align*}
Since $\epsilon-\alpha q>0$, we can also sum over $n$ and get
\begin{align*}
	\sum_{n=1}^\infty\P\left(\bigcup_{j=1}^{2^n}\left\{d(X_{(j-1)2^{-n}},X_{j2^{-n}})\geq2^{-n\alpha}\right\}\right)<\infty.
\end{align*}
According to Borel-Cantelli lemma, for $a.s.\ \omega\in\Omega$, there exists $n_0(\omega)\in\mathbb{N}$ such that
\begin{align*}
	d\left(X_{(j-1)2^{-n}}(\omega),X_{j2^{-n}}(\omega)\right)< 2^{-n\alpha},\quad \forall n\geq n_0(\omega),\ \forall j\in\{1,\cdots,2^n\}.
\end{align*}
As a result, for $a.s.\ \omega\in\Omega$, we have
\begin{align*}
	K_\alpha(\omega) = \sup_{n\in\mathbb{N}}\sup_{1\leq j\leq 2^n}\frac{d\left(X_{(j-1)2^{-n}}(\omega),X_{j2^{-n}}(\omega)\right)}{2^{-n\alpha}}<\infty.
\end{align*}

Now we take $D=\left\{j2^{-n}:n\in\mathbb{N},j\in\{1,\cdots,2^n\}\right\}$, which is a countable dense subset of $I=[0,1]$. Then for $a.s.\ \omega\in\Omega$, we have the following conclusion:
\begin{align*}
	d(X_s,X_t)\leq C_\alpha(\omega)\vert t-s\vert^\alpha,\ \forall s,t\in D,\quad\textit{where}\ \ C_\alpha(\omega)=\frac{2K_\alpha(\omega)}{1-2^{-\alpha}}<\infty.
\end{align*}
This is an immediately corollary the \hyperref[claim:4.6*]{Claim 4.6*}:
\paragraph{Claim 4.6*.\label{claim:4.6*}} If there exists $\alpha,K>0$ such that $f:D\to E$ satisfies
\begin{align*}
	d\left(f((j-1)2^{-n}),f(j2^{-n})\right)\leq K2^{-n\alpha},\quad \forall n\in\mathbb{N},\ \forall j\in\{1,\cdots,2^n\},
\end{align*}
then we have
\begin{align*}
	d\left(f(s),f(t)\right)\leq2K\left(1-2^{-\alpha}\right)^{-1}\vert t-s\vert^\alpha.
\end{align*}
\begin{proof}
We fix $t>s>0$, and choose $p\in\mathbb{N}$ such that $t-s\leq 2^{-p}$. We also choose $k$ such that $s\leq k2^{-p} < t$. Then we can write $s$ and $t$ as
\begin{align*}
	s=k2^{-p}-\sum_{j=1}^l \delta_j 2^{-p-j},\quad t=k2^{-p}+\sum_{j=1}^m \delta_j^\prime 2^{-p-j},\quad \textit{where}\ \ \delta_1,\cdots,\delta_l,\delta_1^\prime,\cdots,\delta_m^\prime\in\{0,1\}.
\end{align*}
We take finite sequences $s_j\searrow s_l=s$ and $t_j\nearrow t_m=t$ defined by partial sums. Then $s_0=t_0=k2^{-p}$, and
\begin{align*}
	d\left(f(s),f(t)\right)\leq \sum_{j=1}^l d\left(f(s_{j-1}),f(s_j)\right)+\sum_{j=1}^m d\left(f(t_{j-1}),t(s_j)\right) \leq \frac{2K2^{-\alpha p}}{1-2^{-\alpha}}.
\end{align*}
Since $\vert t-s\vert\leq 2^{-p}$, the result follows.
\end{proof}

\textit{Proof of \hyperref[thm:4.6]{Theorem 4.6 (Cont).}} According to our previous discussion, the mapping $t\mapsto X_t(\omega)$ is Hölder continuous of exponent $\alpha$ on $D$ for all $\omega\in\Omega\backslash N$. By completeness of $(E,d)$, we define
\begin{align*}
\widetilde{X}_t(\omega)=\begin{cases}
\lim_{D\ni s\to t}X_s(\omega)\ &\textit{if}\ \ K_\alpha(\omega)<\infty\\
x_0\ &\textit{otherwise},
\end{cases}
\end{align*}
where $x_0\in E$ is an arbitrary fixed point. Then $(\widetilde{X}_t)_{t\in I}$ has Hölder continuous sample paths of exponent $\alpha$.

It remains to show that $\widetilde{X}$ is an $a.s.$ modification of $X$. Fix $t\in I=[0,1]$. By assumption in this theorem, we have $X_{t_n}\overset{\P}{\to}X_t$ for $D\ni t_n\to t$, and $X_{t_n}\to\widetilde{X}_t\ a.s.$ by definition of $\widetilde{X}$. Then $X_t=\widetilde{X}_t\ a.s.$.
\end{proof}

\paragraph{Remark.} If $I$ is unbounded, for instance $I=\mathbb{R}_+$, we apply \hyperref[thm:4.6]{Theorem 4.6} repeatedly on $I_n=[0,n]$ for $n\in\mathbb{N}$. Then $(X_t)_{t\geq 0}$ has an $a.s.$ modification $(\widetilde{X}_t)_{t\geq 0}$ whose sample paths are locally Hölder continuous of exponent $\alpha$ for all $\alpha\in(0,\frac{\epsilon}{q})$. We can apply \hyperref[thm:4.6]{Theorem 4.6} to a pre-Brownian motion.

\paragraph{Corollary 4.7.\label{cor:4.7}} Let $B=(B_t)_{t\geq 0}$ be a pre-Brownian motion. Then it has an $a.s.$ modification whose sample paths are locally Hölder continuous with exponent $\alpha$ for all $\alpha\in(0,\frac{1}{2})$.
\begin{proof}
Take $\delta>0$. For all $s,t\geq 0$, we have
\begin{align*}
	\E\left[\vert B_t-B_s\vert^{2+\delta}\right]=\vert t-s\vert^{1+\frac{\delta}{2}}\E\left[\vert Z\vert^{2+\delta}\right],\quad \textit{where}\ Z\in N(0,1).
\end{align*}

By \hyperref[thm:4.6]{Theorem 4.6}, process $B$ has an $a.s.$ modification $\widetilde{B}$ whose sample paths are locally Hölder continuous of exponent $\alpha$ for all $\alpha\in(0,\frac{\delta}{4+2\delta})$. If $\delta$ is great enough we can take $\alpha$ arbitrarily close to $\frac{1}{2}$.
\end{proof}

\paragraph{Definition 4.8\label{def:4.8}} (Brownian motion/Wiener process). If $B=(B_t)_{t\geq 0}$ is a pre-Brownian motion and $B$ is sample-continuous, then process $B$ is said to be a \textit{Brownian motion/Wiener process}.

\paragraph{Remark.} The \hyperref[cor:4.7]{Corollary 4.7} is in fact a justification for the existence of a Brownian motion. Furthermore, the \hyperref[prop:4.5]{Proposition 4.5} still holds if pre-Brownian motion is everywhere replaced by Brownian motion. In addition, we have $\lim_{t\to\infty}\frac{1}{t}B_t=0$ by continuity of sample paths and the fact that $(\frac{1}{t}B_t)_{t>0}\overset{d}{=}(B_{1/t})_{t>0}$.

\newpage
\subsection{Canonical Construction and Wiener's Construction}
\paragraph{Definition 4.9.\label{def:4.9}} Let $\{(\Omega_\alpha,\mathscr{F}_\alpha),\alpha\in J\}$ be a collection of measurable spaces. We define the collection of all \textit{measurable rectangles} by
\begin{align*}
	\prod_{\alpha\in J}\mathscr{F}_\alpha = \left\{\prod_{\alpha\in J}A_\alpha:A_\alpha\in\mathscr{F}_\alpha\ \textit{for all}\ \alpha\in J,\textit{and}\ A_\alpha=\Omega_\alpha\ \textit{except for finitely many}\ \alpha\in J\right\}.
\end{align*}
Akin to the proof (i) of \hyperref[thm:1.25]{Theorem 1.25}, we can prove that $\prod_{\alpha\in J}\mathscr{F}_\alpha$ is a semi-ring. Similar to the definition of product of two measurable spaces, we define the product $\sigma$-algebra:
\begin{align*}
	\bigotimes_{\alpha\in J}\mathscr{F}_\alpha = \sigma\left(\prod_{\alpha\in J}\mathscr{F}_\alpha\right)
\end{align*}
The measurable space $\left(\bigcup_{\alpha\in J}\Omega_\alpha,\bigotimes_{\alpha\in J}\mathscr{F}_\alpha\right)$ is said to be the \textit{product space} of $\{(\Omega_\alpha,\mathscr{F}_\alpha),\alpha\in J\}$.
\paragraph{Remark.} Every coordinate mapping $\pi_\beta:(\omega_\alpha)_{\alpha\in J}\mapsto\omega_\beta$ is measurable when defined on $\{(\Omega_\alpha,\mathscr{F}_\alpha),\alpha\in J\}$. Furthermore, for all finite subset $I\subset J$, the projection mapping $\pi_I:(\omega_\alpha)_{\alpha\in J}\mapsto(\omega_\alpha)_{\alpha\in I}$ is measurable.

\paragraph{Proposition 4.10.\label{prop:4.10}} Let $\mathcal{I}_F$ be the collection of all \uwave{finite} subsets of $J$. For $I\in\mathcal{I}_F$, define $\bigotimes_{\alpha\in I}^J\mathscr{F}_\alpha$ to be the sub $\sigma$-algebra consisting of all \textit{measurable cylinders $A$ with base in} $\prod_{\alpha\in I}\Omega_\alpha$. That means, $A=B\times\prod_{\alpha\in I^c}\Omega_\alpha$ for some measurable $B\subset\prod_{\alpha\in I}\Omega_\alpha$. Then
\begin{align*}
	\bigotimes_{\alpha\in J}\mathscr{F}_\alpha=\sigma\left(\bigcup_{I\in\mathcal{I}_F}\bigotimes_{\alpha\in I}^J\mathscr{F}_\alpha\right)
\end{align*}
\begin{proof}
Clearly $\bigotimes_{\alpha\in I}^J\mathscr{F}_\alpha\subset\bigotimes_{\alpha\in J}\mathscr{F}_\alpha$ for all $I\in\mathcal{I}_F$. On the other hand, every element of $\prod_{\alpha\in J}\mathscr{F}_\alpha$ is contained by some $\bigotimes_{\alpha\in I}^J\mathscr{F}_\alpha$.
\end{proof}

\paragraph{Proposition 4.11.\label{prop:4.11}} Let $\left\{\P_I,I\in\mathcal{I}_F\right\}$ be a collection of probability measures defined on finite product spaces $\left\{\left(\prod_{\alpha\in I}\Omega_\alpha,\bigotimes_{\alpha\in I}\mathscr{F}_\alpha\right),I\in\mathcal{I}_F\right\}$. The following compatibility condition is necessary and sufficient for the existence of a \uwave{finitely additive} probability measure $\P$ on $\bigotimes_{\alpha\in J}\mathscr{F}_\alpha$ such that the pushforward $(\pi_{I})_*\P=\P_I$.

\textit{Compatibility:} If $I_1\subset I_2$ are two finite subsets of $J$, then $(\pi_{I_2\to I_1})_*\P_{I_2}=\P_{I_1}$.
\begin{proof}
We only prove sufficiency, since necessity is clear. For every cylinder $A=B\times\prod_{\alpha\in I^c}\Omega_\alpha$ such that $B\in\bigotimes_{\alpha\in I}\mathscr{F}_\alpha$, define $\P(A)=\P_I(B)$. By compatibility condition, we obtain a finitely additive function $\P$ on $\mathscr{A}=\bigcup_{I\in\mathcal{I}_F}\bigotimes_{\alpha\in I}^J\mathscr{F}_\alpha$, which is an algebra. We extend $\P$ to $\bigotimes_{\alpha\in J}\mathscr{F}_\alpha=\sigma(\mathscr{A})$. For $A\in\bigotimes_{\alpha\in J}\mathscr{F}_\alpha$, define
\begin{align*}
	\P(A)=\sup\left\{\P(F):F\subset A,\ F\in\mathscr{A}\right\}\in[0,1].
\end{align*}
Then for any collection of disjoint sets $A_1,\cdots,A_n\in\bigotimes_{\alpha\in J}\mathscr{F}_\alpha$, we have
\begin{align*}
	\sum_{j=1}^n\P(A)&=\sum_{j=1}^n\sup\left\{\P(F_j):F_j\subset A_j,\ F\in\mathscr{A}\right\}\\
	&= \sup\left\{\P\left(\bigcup_{j=1}^n F_j\right):F_j\subset A_j,F_j\in\mathscr{A},\ \forall j\in\{1,\cdots,n\}\right\} = \P\left(\bigcup_{j=1}^n A_j\right).
\end{align*}
Thus we complete the proof of finite additivity of $\P$ on $\bigotimes_{\alpha\in J}\mathscr{F}_\alpha$. 
\end{proof}

\paragraph{Proposition 4.12\label{prop:4.12}} (Compact class). A class $\mathscr{C}$ of subsets of $\Omega$ is said to be \textit{compact}, if all its countable subclasses with \textit{finite intersection property} have nonempty intersection. That is, for all sequences $(C_n)_{n=1}^\infty\subset\mathscr{C}$ such that $\bigcap_{k=1}^n C_k\neq\emptyset$ for all $n\in\mathbb{N}$, it holds $\bigcap_{n=1}^\infty C_n\neq\emptyset$.

If $\mathscr{C}$ is a compact class, so are the following: (i) The class $\mathscr{C}_\delta$ containing all countable intersections of elements of $\mathscr{C}$; (ii) The class $\mathscr{C}_s$ containing all finite unions of elements of $\mathscr{C}$.

\begin{proof}
(i) Since every countable intersection of elements of $\mathscr{C}_\delta$ is also a countable intersection of elements of the compact class $\mathscr{C}$, the result follows immediately.

(ii) We take a sequence $D_n=\bigcup_{j=1}^{m_n}C_j^n\in\mathscr{C}_s$ such that $\bigcap_{k=1}^n D_k\neq\emptyset$, and prove that $\bigcap_{n=1}^\infty D_n\neq\emptyset$. For each $n\in\mathbb{N}$, define the multi-index set $I_n=\prod_{k=1}^n\{1,\cdots,m_k\}$. Then according to distributivity law, we have
\begin{align*}
	\bigcap_{k=1}^n D_k = \bigcap_{k=1}^n\left(\bigcup_{j=1}^{m_k}C_j^k\right) = \bigcup_{\alpha\in I_n}\left(\bigcap_{k=1}^n C_{\alpha_k}^k\right)\neq\emptyset.
\end{align*}
Hence for every $n\in\mathbb{N}$, there exists a multi-index $\alpha\in I_n$ such that $\bigcap_{k=1}^nC_{\alpha_k}^k\neq\emptyset$, and we define
\begin{align*}
	J_n=\left\{\alpha\in\prod_{k=1}^\infty\{1,\cdots,m_k\}:\bigcap_{k=1}^nC_{\alpha_k}^k\neq\emptyset\right\},\quad \forall n\in\mathbb{N}.
\end{align*}

Clearly, the definition of $J_n$ only concerns about the first $n$ elements. Then $J_n\neq\emptyset$ for all $n\in\mathbb{N}$, and $(J_n)_{n=1}^\infty$ is monotone decreasing. Now we choose a sequence $\alpha^{[n]}\in J_n$ for each $n\in\mathbb{N}$. By induction on $k$, we are able to determine a sequence $\alpha_k^*\in\{1,\cdots,m_k\}$ such that $\alpha_{1:k}^*=\alpha_{1:k}^{[n]}$ for infinitely many $n$. As a result, for each $k\in\mathbb{N}$, we can find $n\geq k$ such that $\alpha_{1:k}^*=\alpha_{1:k}^{[n]}$, which implies $\alpha^*\in J_n\subset J_k$. Hence $\alpha^*\in\bigcup_{k=1}^\infty J_k$, and by compactness of $\mathscr{C}$ we have $\bigcap_{k=1}^\infty D_k\supset \bigcap_{k=1}^\infty C_{\alpha_k^*}^k\neq\emptyset$, completing the proof.
\end{proof}

\paragraph{Remark.} This definition is also in accordance with compactness in topology. If $X$ is a topological space, and $\mathscr{K}$ is the collection of all compact subspaces of $X$. If $(K_n)_{n=1}^\infty\subset\mathscr{K}$ is a sequence such that $\bigcap_{n=1}^\infty K_j=\emptyset$, define $L_n=\bigcap_{j=1}^n K_j$. Then the increasing sequence $(K_1\backslash L_n)_{n=1}^\infty$ forms an open cover of $K_1$. By compactness of $K_1$, there is a finite subcover, and we can find $N\in\mathbb{N}$ such that $L_N=\emptyset$.

\paragraph{Theorem 4.13\label{thm:4.13}} (Daniell-Kolmogorov extension). Let $\left\{\P_I,I\in\mathcal{I}_F\right\}$ be a collection of probability measures defined on finite product spaces $\left\{\left(\prod_{\alpha\in I}\Omega_\alpha,\bigotimes_{\alpha\in I}\mathscr{F}_\alpha\right),I\in\mathcal{I}_F\right\}$ that satisfies the compatibility condition in \hyperref[prop:4.11]{Proposition 4.11}. If for each $\alpha\in J$, there exists a compact class $\mathscr{C}_\alpha\subset\mathscr{F}_\alpha$ such that 
\begin{align*}
\P_{\alpha}(A)=\sup\left\{\P_{\alpha}(C):C\in\mathscr{C}_\alpha,C\subset A\right\},\quad\forall A\in\mathscr{F}_\alpha.	
\end{align*}
Then there exists a unique probability measure $\P$ on $\left(\prod_{\alpha\in J}\Omega_\alpha,\bigotimes_{\alpha\in J}\mathscr{F}_\alpha\right)$ that extends each $\P_I$.
\begin{proof}
\textit{Step I:} Let $\P$ be the finitely additive set function found in \hyperref[prop:4.11]{Proposition 4.11}. We first prove that there exists a compact subclass $\mathscr{C}$ of the semiring $\mathscr{S}=\prod_{\alpha\in J}\mathscr{F}_\alpha$ of all measurable rectangles such that
\begin{align*}
	\P(A)=\sup\left\{\P(C):C\in\mathscr{C},C\subset A\right\},\quad \forall A\in\mathscr{S}.\tag{4.1}\label{eq:4.1}
\end{align*}

Let $\mathscr{D}=\{C\times\prod_{\alpha\neq\beta}\Omega_\alpha:\beta\in J,\ C\in\mathscr{C}_\beta\}$. Then every countable intersection $D=\bigcap_{n\in\mathbb{N}}(C_n\times\prod_{\alpha\neq\beta_n}\Omega_\alpha)$ of elements of $\mathscr{D}$ has the form $\prod_{\beta\in J}B_\beta$, where $B_\beta=\bigcap_{\{n:\beta_n=\beta\}}C_n$. If the countable intersection $D$ is empty, let $\beta\in J$ be such that $B_\beta=\emptyset$. By compactness of $\mathscr{C}_\beta$, there exists a finite subset $I_\beta\subset\{n:\beta_n=\beta\}$ such that $\bigcap_{n\in I_\beta}C_n=\emptyset$, which implies $\bigcap_{n\in I_\beta}(C_n\times\prod_{\alpha\neq\beta_n}\Omega_\alpha)=\emptyset$. Therefore $\mathscr{D}$ is a compact subclass of $\mathscr{S}$. Again, the class $\mathscr{C}$ of all finite intersections of elements of $\mathscr{D}$ is compact.

Now we prove \hyperref[eq:4.1]{(4.1)}. Take any $\epsilon>0$. If $A$ is a measurable rectangle with base $\prod_{j=1}^n A_{\alpha_j}\subset\prod_{j=1}^n \Omega_{\alpha_j}$, choose $\mathscr{C}_{t_j}\ni C_j\subset A_{\alpha_j}$ such that $\P_{\alpha_j}(C_j)\geq\P_{\alpha_j}(A_{t_j})-\frac{\epsilon}{n}$. Then $\mathscr{C}\ni C=\bigcap_{j=1}^n(C_j\times\prod_{\alpha\neq\alpha_j}\Omega_{\alpha})\subset A$, and 
\begin{align*}
	\P(A\backslash C)=\P\left(\bigcup_{j=1}^n \biggl((A_{\alpha_j}\backslash C_j)\times\prod_{\alpha\neq\alpha_j}\Omega_\alpha\biggr)\right)\leq\sum_{j=1}^n\P_{\alpha_j}(A_{\alpha_j}\backslash C_j) = \epsilon\downarrow 0.
\end{align*}

\textit{Step II:} We prove the $\sigma$-additivity of $\P$ on $\mathscr{S}$. We take the class $\mathscr{C}_s$ consisting of all finite intersection of elements of $\mathscr{C}$, which is again a compact class by \hyperref[prop:4.12]{Proposition 4.12 (ii)} and is contained in the ring $\mathscr{R}$ generated by $\mathscr{S}$ according to the Remark under \hyperref[def:1.24]{Definition 1.24}. Similar to the proof of \hyperref[eq:4.1]{(4.1)} in Step I, we can prove that $\P(A)=\sup\{\P(C):C\in\mathscr{C}_s,C\subset A\}$ for all $A=\coprod_{k=1}^n A_k\in\mathscr{R}$ by taking $C=\coprod_{k=1}^n C_k\in\mathscr{C}_s$ with $\P(A_k\backslash C_k)\leq\epsilon/n$ for arbitrarily small $\epsilon$. We prove that $\P$ is $\sigma$-additive on $\mathscr{R}$, hence on $\mathscr{S}$.

Given $\epsilon>0$, we take a sequence $\mathscr{R}\ni A_n\downarrow\emptyset$, and take $\mathscr{C}_s\ni C_n\subset A_n$ with $\P(A_n)\leq\P(C_n)+\epsilon2^{-n}$. Then $\bigcap_{n=1}^\infty C_n\subset\bigcap_{n=1}^\infty A_n=\emptyset$, and there exists $N\in\mathbb{N}$ such that $\bigcap_{n=1}^N C_n=\emptyset$ by compactness of $\mathscr{C}_s$. As a result,
\begin{align*}
	\P(A_N)=\P\left(A_N\backslash\left(\bigcap_{n=1}^N C_n\right)\right)=\P\left(\bigcup_{n=1}^N(A_N\backslash C_n)\right)\leq\sum_{n=1}^N\P(A_n\backslash C_n) \leq\epsilon\downarrow 0.
\end{align*}

Therefore $\P$ is continuous at $\emptyset$. If $(B_n)_{n=1}^\infty\subset\mathscr{R}$ is a sequence of disjoint sets, take $A_n=\bigcup_{k=n+1}^\infty B_k$. Then we have $A_n\downarrow \emptyset$. Finite additivity of $\P$ implies
\begin{align*}
	\P\left(\bigcup_{n=1}^\infty B_n\right) - \sum_{n=1}^N\P(B_n) = \P(A_N)\to 0\quad \textit{as}\ N\to\infty.
\end{align*}

\textit{Step III:} According to Step II, $\P$ is a finite pre-measure on the semiring $\mathscr{S}$ which generates $\bigotimes_{\alpha\in J}\mathscr{F}_\alpha$. By Carathéodory's extension theorem, $\P$ can be uniquely extended to a probability measure $\bigotimes_{\alpha\in J}\mathscr{F}_\alpha$. On the other hand, the finite additive function $\P$ is uniquely defined on $\mathscr{S}$, which is specified by the family of measures $\{\P_I,I\in\mathcal{I}_F\}$ on finite-dimensional subspaces. Therefore the extension $\P$ is unique.
\end{proof}

\paragraph{Proposition 4.14.\label{prop:4.14}} Let $\Omega$ be a Hausdorff topological space, and equip $\Omega$ with the Borel $\sigma$-algebra $\mathscr{B}$. Let $\mathscr{C}$ be the collection of all closed sets in $\Omega$, and $\mathscr{K}$ the collection of all compact sets in $\Omega$. Let $\P$ be a probability measure on $(\Omega,\mathscr{B})$. We define the collections $\mathscr{R}_c$ of \textit{closed regular sets} and $\mathscr{R}_k$ of \textit{regular sets} as
\begin{align*}
	\mathscr{R}_c=\left\{B\in\mathscr{B}: \P(B)=\sup_{C\in\mathscr{C}:C\subset B}\P(C)\right\},\quad \mathscr{R}_k=\left\{B\in\mathscr{B}: \P(B)=\sup_{K\in\mathscr{K}:K\subset B}\P(K)\right\}.
\end{align*}
We say $\P$ is tight if $\Omega\in\mathscr{R}_k$. We say $\P$ is \textit{closed regular} (resp. \textit{regular}) if $\mathscr{R}_c=\mathscr{B}$ (resp. $\mathscr{R}_k=\mathscr{B}$).
\begin{itemize}
\item[(i)] The collection $\mathscr{R}_c^*=\{B\in\mathscr{R}_c:\Omega\backslash B\in\mathscr{R}_c\}$ is a $\sigma$-algebra. In addition, if $\P$ is tight, then the collection $\mathscr{R}_k^*=\{B\in\mathscr{R}_k:\Omega\backslash B\in\mathscr{R}_k\}$ is also a $\sigma$-algebra.

\item[(ii)] If $\Omega$ is metrizable, then $\P$ is closed regular. In addition, if $\P$ is tight, then it is regular.

\item[(iii)] (Ulam). If $\Omega$ is a \textit{Polish space} (a separable completely metrizable space), then $\P$ is regular.
\end{itemize}
\begin{proof}
(i) Clearly, $\Omega\in\mathscr{R}_c^*$, and $B\in\mathscr{R}_c^*$ implies $\Omega\backslash B\in\mathscr{R}_c^*$. Given any sequence $(A_n)_{n=1}^\infty\subset\mathscr{R}_c^*$, we prove $A=\bigcup_{n=1}^\infty A_n\in\mathscr{R}_c^*$. Let $\epsilon>0$. Take closed sets $C_n\subset A_n$ such that $\P(C_n)\geq\P(A_n)-\epsilon 3^{-n}$, and $D_n\subset\Omega\backslash A_n$ such that $\P(D_n)\geq\P(\Omega\backslash A_n)-\epsilon 2^{-n}$. Then there exists $N\in\mathbb{N}$ such that $\P(A)>\P\left(\bigcup_{n=1}^\infty A_n\right)-\epsilon/2$. Note that $\bigcup_{n=1}^N C_n\subset A$ is closed, and
\begin{align*}
	\P(A)-\P\left(\bigcup_{n=1}^N C_n\right)<\frac{\epsilon}{2}+\P\left(\bigcup_{n=1}^N A_n\right)-\P\left(\bigcup_{n=1}^N C_n\right)\leq\frac{\epsilon}{2}+\P\left(\bigcup_{n=1}^N(A_n\backslash C_n)\right)<\epsilon.
\end{align*}
Meanwhile, $\bigcap_{n=1}^\infty D_n\subset\Omega\backslash A$ is also closed, and
\begin{align*}
	\P\left(\Omega\backslash A\right)-\P\left(\bigcap_{n=1}^\infty D_n\right)=\P\left(\bigcup_{n=1}^\infty\left((\Omega\backslash A)\backslash D_n\right)\right)\leq\sum_{n=1}^\infty\P\left((\Omega\backslash A_n)\backslash D_n\right) < \epsilon.
\end{align*}
Since $\epsilon>0$ is arbitrary, we have $A\in\mathscr{R}_c^*$. Hence $\mathscr{R}_c^*$ is a $\sigma$-algebra.

If $\P$ is tight, we have $\Omega,\emptyset\in\mathscr{R}_k^*$, and $B\in\mathscr{R}_k^*$ implies $\Omega\backslash B\in\mathscr{R}_k^*$. Similar to the above proof, since finite unions and countable intersections of compact sets are still compact, we conclude $\mathscr{R}_k^*$ is a $\sigma$-algebra. \vspace{0.1cm}

(ii) Let $d$ be the metric of $\Omega$. If $U\subset\Omega$ is an open set, take its complement $F=\Omega\backslash U$, and define $F_n=\left\{x\in\Omega:d(x,F)\geq 1/n\right\}$. Then $F_n\uparrow U$, and $U\in\mathscr{R}_c^*$ defined in (i). Since $\mathscr{R}_c^*$ is a $\sigma$-algebra containing all open sets in $\Omega$, we have $\mathscr{R}_c^*=\mathscr{B}$, and $\P$ is closed regular.

In addition, if $\P$ is tight, we can take a compact set $K$ such that $\P(\Omega\backslash K)<\epsilon/2$ for every $\epsilon>0$. For any Borel set $B\in\mathscr{B}$, take closed set $F\subset B$ with $\mu(B\backslash F)<\epsilon/2$. Then $F\cap K\subset B$ is a compact set, and $\P(B\backslash(K\cap F))\leq\P(B\backslash F)+\P(\Omega\backslash K)<\epsilon$. Since $\epsilon>0$ is arbitrary, $B\in\mathscr{R}_k$. Hence $\P$ is regular.\vspace{0.1cm}

(iii) Following (ii), it suffices to show that $\P$ is tight. Let $(\omega_n)_{n=1}^\infty$ be a dense sequence in $\Omega$. For any $\eta>0$ and $\omega\in\Omega$, let $B(\omega,\eta)$ be the closed ball centered at $\omega$ of radius $\eta$. Given $\epsilon>0$, by density of $(\omega_n)_{n=1}^\infty$, we are able to take $N_m\in\mathbb{N}$ such that
\begin{align*}
	\P\left(\Omega\backslash\bigcup_{n=1}^{N_m}B\left(\omega_n,\frac{1}{m}\right)\right)<\frac{\epsilon}{2^m},\quad\forall m\in\mathbb{N}.
\end{align*}
Let $K=\bigcap_{m=1}^\infty\bigcup_{n=1}^{N_k}B\left(\omega_n,\frac{1}{k}\right)$. Then $K$ is both closed and totally bounded. Since $\Omega$ is a complete metric space, $K$ is compact. Furthermore, we have
\begin{align*}
	\P(\Omega\backslash K)\leq\sum_{m=1}^\infty\frac{\epsilon}{2^m}=\epsilon\downarrow 0.
\end{align*}
Hence $\P$ is tight, and the result follows from (ii).
\end{proof}

\hyperref[thm:3.13]{Theorem 3.13} and \hyperref[prop:3.13]{Proposition 3.14} together imply the following conclusion.

\paragraph{Corollary 4.15.\label{cor:4.15}} Let $\left\{(\Omega_\alpha,\mathscr{B}_\alpha),\alpha\in J\right\}$ be a family of Polish spaces equipped with their Borel $\sigma$-algebras. For any compatible family $\left\{\P_I,I\in\mathcal{I}_F\right\}$ of probability measures defined on $\left\{\left(\prod_{\alpha\in I}\Omega_\alpha,\bigotimes_{\alpha\in I}\mathscr{B}_\alpha\right),I\in\mathcal{I}_F\right\}$, there exists a unique probability measure $\P$ on $\left(\prod_{\alpha\in J}\Omega_\alpha,\bigotimes_{\alpha\in J}\mathscr{B}_\alpha\right)$ that extends each $\P_I$.

\paragraph{Remark.} Let $E$ be a metric space equipped with its Borel $\sigma$-algebra $\mathscr{B}$. The product space $(E^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}})$ is called the \textit{canonical space}. Since every evaluation map $\pi_t:(E^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}})\to(E,\mathscr{B}),\ x\mapsto x(t)$ is measurable, we can define a process $(\pi_t)_{t\in\mathscr{T}}$ on $(E^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}})$, which is called the \textit{canonical process}. Given a probability measure $\mu$ on $(E^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}})$, the sample paths of the canonical process $(\pi_t)_{t\in\mathcal{T}}$ are distributed according to $\mu$.

Given $(X_t)_{t\in\mathcal{T}}$ is a stochastic process defined on $(\Omega,\mathscr{F},\P)$ whose state space is the metric space $E$, the mapping $\Phi:(\Omega,\mathscr{F})\to(E^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}})$ defined to map $\omega$ to its sample path $t\mapsto X_t(\omega)$ is measurable. In fact, for every measurable rectangle $A\in E^\mathcal{T}$ with basis $\prod_{j=1}^n A_{t_j}$, its pre-image $\Phi^{-1}(A)=\bigcap_{j=1}^n X_{t_j}^{-1}(A_{t_j})\in\mathscr{F}$. Since $\Phi^{-1}$ preserves complement and countable union operations, and $\mathscr{B}^{\otimes\mathcal{T}}$ is generated by all measurable rectangles, we obtain that $\Phi$ is measurable. As a result, the process $(X_t)_{t\in\mathcal{T}}$ determines a pushforward probability measure $\mu=\Phi_*\P$ on the canonical space $(E^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}})$. Furthermore, the canonical process $(\pi_t)_{t\in\mathcal{T}}$ defined on $(E^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}},\mu)$ is identically distributed to $(X_t)_{t\in\mathcal{T}}$.

According to our previous discussion, we can construct a stochastic process indexed by $\mathcal{T}$ and taking values in $E$ by constructing a probability measure on the canonical space $(E^\mathcal{T},\mathscr{B}^{\otimes\mathcal{T}})$.

\paragraph{Example: the canonical construction of Brownian motion.} Let $C(\mathbb{R}_+)$ be the space of all real-valued continuous function defined on $\mathbb{R}_+$. We give $C(\mathbb{R}_+)$ the metric
\begin{align*}
d(f,g)=\sum_{n=1}^\infty\frac{1}{2^n}\frac{\sup_{t\in[0,n]}\vert f(t)-g(t)\vert}{1+\sup_{t\in[0,n]}\vert f(t)-g(t)\vert},\quad \forall f,g\in C(\mathbb{R}^+).
\end{align*}
This metric induces the compact convergence (c.c.) topology on $C(\mathbb{R}_+)$, because a sequence $f_n\in C(\mathbb{R}_+)$ converges uniformly on each compact set to $f\in C(\mathbb{R}_+)$ if and only if $d(f_n,f)\to 0$. Then for each $t\in\mathbb{R}_+$, the coordinate mapping $\pi_t:C(\mathbb{R}_+)\to\mathbb{R},\ f\mapsto f(t)$ is continuous. Consequently, if we give $C(\mathbb{R}_+)$ the Borel $\sigma$-algebra $\mathscr{C}$ generated by c.c.topology, then all coordinate mappings $\{\pi_t,\ t\in\mathbb{R}_+\}$ are measurable.

In fact, $\mathscr{C}$ coincides the smallest $\sigma$-algebra $\mathscr{B}_p$ on $C(\mathbb{R}_+)$ for which the coordinate mappings $\pi_t:f\mapsto f(t)$ are measurable for all $t\in\mathbb{R}_+$. To see this, note that $C(\mathbb{R}_+)$ is separable with respect to the c.c. topology, because we can approximate each $f\in C(\mathbb{R}_+)$ within arbitrary precision with a polynomial with rational coefficients on some $[0,n]$. As a result, every open set in $C(\mathbb{R}_+)$ with respect to the c.c.topology is a countable union of basic sets of the form $B_{[0,n]}(f_0,\epsilon)=\{f\in C(\mathbb{R}_+):\sup_{t\in[0,n]}\vert f(t)-f_0(t)\vert<\epsilon\}$, where $f_0\in C(\mathbb{R}_+)$ and $\epsilon>0$. By continuity, every basic set $B_{[0,n](f_0,\epsilon)}=\bigcap_{t\in\mathbb{Q}\cap[0,n]}\pi_t^{-1}(O(f_0(t),\epsilon))\in\mathscr{B}_p$. Hence every open set in $C(\mathbb{R}_+)$ with respect to the c.c.topology is contained in $\mathscr{B}_p$. Therefore $\mathscr{B}_p$ coincides $\mathscr{C}$.

For each measurable rectangle $A=\bigcap_{j=1}^n \pi_{t_j}^{-1}(A_j)$ [note every coordinate map $\pi_{t_j}$ is defined on $C(\mathbb{R}_+)$], where $0=t_0<t_1<\cdots<t_n$ and $A_0,A_1,\cdots,A_n\in\mathscr{B}(\mathbb{R})$, define
\begin{align*}
	W(A)=\frac{\mathds{1}_{A_0}(0)}{\sqrt{(2\pi)^n\prod_{j=1}^n(t_j-t_{j-1})}}\int_{A_1\times\cdots\times A_n}\exp\biggl(-\sum_{j=1}^n\frac{(z_j-z_{j-1})^2}{2(t_j-t_{j-1})}\biggr)\,\d z_1\cdots \d z_n,\quad\textit{where}\ z_0=0.
\end{align*}
This is a pre-measure on the semi-ring $\mathscr{S}$ of all measurable rectangles, and we can extend it to a measure the $\sigma$-algebra $\mathscr{C}$ generated by $\mathscr{S}$. This is called the \textit{Wiener measure} on $(C(\mathbb{R}_+),\mathscr{C})$.

If we choose $(\Omega,\mathscr{F},\P)=(C(\mathbb{R}_+),\mathscr{C},W)$, then the canonical process $(\pi_t)_{t\in\mathbb{R}_+}$ is a Brownian motion. This is a consequence of \hyperref[prop:4.4]{Proposition 4.4 (iv)} and the fact that $(\pi_t)_{t\in\mathbb{R}_+}$ has continuous sample paths. In fact, the distribution law of every Brownian motion $(B_t)_{t\geq 0}$ is determined, which is $W$. 

\paragraph{Example: Wiener's construction of Brownian Process.} Let $\{e_n,n\in\mathbb{N}\}$ be a countable orthonormal basis of $L^2([0,1])$, which is a separable Hilbert space. According to \hyperref[cor:4.15]{Corollary 4.15}, it is possible to construct a collection of independent standard Gaussian variables $(Z_n)_{n=1}^\infty$ on an appropriate probability space $(\Omega,\mathscr{F},\P)$.  We define a process $(B_t)_{t\in[0,1]}$ as follows:
\begin{align*}
	B_t=\sum_{n=1}^\infty \langle \mathds{1}_{[0,t]}, e_n\rangle\,Z_n,\quad \forall t\in[0,1].\tag{4.2}\label{eq:4.2}
\end{align*}

Since $\sum_{n=1}^\infty\vert \langle\mathds{1}_{[0,t]},e_n\rangle\vert^2=t<\infty$, the series \hyperref[eq:4.2]{(4.2)} converges in $L^2$. Consequently, $(B_t)_{t\in[0,1]}$ is a Gaussian process.  Furthermore, for any partition $0=t_0<t_1<\cdots<t_p=1$, we have
\begin{align*}
	\E\left[(B_{t_j}-B_{t_{j-1}})(B_{t_k}-B_{t_{k-1}})\right]&=\sum_{m=1}^\infty\sum_{n=1}^\infty\langle\mathds{1}_{(t_{j-1},t_j]},e_m\rangle\langle\mathds{1}_{(t_{k-1},t_k]},e_n\rangle\underbrace{\langle e_m,e_n\rangle}_{=\E[Z_mZ_n]}\\
	&=\left\langle\sum_{m=1}^\infty\langle\mathds{1}_{(t_{j-1},t_j]},e_m\rangle e_m,\sum_{n=1}^\infty\langle\mathds{1}_{(t_{k-1},t_k]},e_n\rangle e_n\right\rangle\\
	&= \langle\mathds{1}_{(t_{j-1},t_j]},\mathds{1}_{(t_{k-1},t_k]}\rangle=\begin{cases}
		t_j-t_{j-1},\ &\textit{if}\ j=k;\\
		0,\ &\textit{if}\ j\neq k.
	\end{cases}.
\end{align*}

Consequently, the process $(B_t)_{t\in[0,1]}$ has independent increments, and $B_{t_j}-B_{t_{j-1}}\sim N(0,t_j-t_{j-1})$ for each $j$. By \hyperref[prop:4.4]{Proposition 4.4 (iv)}, $(B_t)_{t\in[0,1]}$ is a pre-Brownian motion on $[0,1]$. If we choose a particular basis for $L^2([0,1])$: $e_0=1$, and $e_n(t)=\sqrt{2}\cos(n\pi t),\ \forall n\in\mathbb{N}$, we obtain a process
\begin{align*}
	B_t = tZ_0 + \sum_{n=1}^\infty\frac{\sqrt{2}\sin(n\pi t)}{n}Z_n=tZ_0+\sum_{n=1}^\infty\sum_{k=2^{n-1}}^{2^n - 1}\frac{\sqrt{2}\sin(k\pi t)}{k}Z_k,\quad \forall t\in[0,1].
\end{align*}
We set $S_m(t)=\sum_{k=m}^{2m-1}\frac{\sqrt{2}\sin(k\pi t)}{k}Z_k$, and write $B_t=tZ_0+\sum_{n=0}^\infty S_{2^n}(t)$. Let $T_m=\sup_{t\in[0,1]}\vert S_m(t)\vert$. Then
\begin{align*}
	T_m^2\leq \sqrt{2}\sup_{t\in[0,1]}\left\vert\sum_{k=m}^{2m-1}\frac{\e^{\i k\pi t}}{k} Z_k\right\vert^2 &= \sqrt{2}\sup_{t\in[0,1]}\sum_{j=m}^{2m-1}\sum_{k=m}^{2m-1}\frac{\e^{\i(k-j)\pi t}}{jk} Z_jZ_k\\
	&\leq \sqrt{2}\sum_{k=m}^{2m-1}\frac{Z_k^2}{k^2} + 2\sqrt{2}\sup_{t\in[0,1]}\left\vert\sum_{k=m}^{2m-1}\sum_{l=1}^{2m-k-1}\e^{\i l\pi t}\frac{Z_k Z_{k+l}}{k(k+l)}\right\vert\\
	&= \sqrt{2}\sum_{k=m}^{2m-1}\frac{Z_k^2}{k^2} + 2\sqrt{2}\sup_{t\in[0,1]}\left\vert\sum_{l=1}^{m-1}\e^{\i l\pi t}\sum_{k=m}^{2m-l-1}\frac{Z_k Z_{k+l}}{k(k+l)}\right\vert\\
	&\leq \sqrt{2}\sum_{k=m}^{2m-1}\frac{Z_k^2}{k^2} + 2\sqrt{2}\sum_{l=1}^{m-1}\left\vert\sum_{k=m}^{2m-l-1}\frac{Z_k Z_{k+l}}{k(k+l)}\right\vert
\end{align*}
Let us bound the expectation of the second term:
\begin{align*}
	\E\left[\left\vert\sum_{k=m}^{2m-l-1}\frac{Z_k Z_{k+l}}{k(k+l)}\right\vert\right]&\leq\sqrt{\E\left[\left(\sum_{k=m}^{2m-l-1}\frac{Z_k Z_{k+l}}{k(k+l)}\right)^2\right]}=\left(\sum_{k=m}^{2m-l-1}\frac{1}{k^2(k+l)^2}\right)^{1/2}\leq\frac{1}{m^{3/2}},\\
	&\Rightarrow\quad \E\left[\sum_{l=1}^{m-1}\left\vert\sum_{k=m}^{2m-l-1}\frac{Z_k Z_{k+l}}{k(k+l)}\right\vert\right]\leq\frac{1}{\sqrt{m}}.
\end{align*}
Since $\sum_{k=m}^{2m-1}k^{-2}\leq m^{-1}$, we have $\E[T_m^2]<c/\sqrt{m}$ for some constant $c>0$ not dependent on $m$. Consequently,
\begin{align*}
	\E\left[\sum_{n=0}^\infty T_{2^n}\right]\leq\sum_{n=0}^\infty\sqrt{\E[T_{2^n}^2]}\leq c\sum_{n=0}^\infty\frac{1}{2^{n/4}}<\infty.
\end{align*}
By Weierstrass M-test, with probability $1$, the mapping $t\mapsto B_t(\omega)$ converges uniformly on $[0,1]$, and the uniform limit is continuous on $[0,1]$. By redefine the sample path of $(B_t)_{t\in[0,1]}$ on a negligible set, we obtain a Brownian motion $(B_t)_{t\in[0,1]}$ on $[0,1]$. To construct a Brownian motion $(B_t)_{t\geq 0}$ on $\mathbb{R}_+$, we concatenate Brownian processes defined on each $[n-1,n]$:
\begin{align*}
	B_t=B_{n-1}+(t-n+1)Z_0^{(n)} + \sum_{k=1}^\infty\frac{\sqrt{2}\sin(k\pi t)}{k}Z_k^{(n)},\quad t\in[n-1,n],
\end{align*}
where $(Z_k^{(n)})_{k=1}^\infty$ is a family of independent standard Gaussian variables.

\newpage
\subsection{Sample Paths of Brownian Motion}
Let $(B_t)_{t\geq 0}$ be a Brownian motion. We take the canonical filtration $(\mathscr{F}_t)_{t\geq 0}$ of $(B_t)_{t\geq 0}$:
\begin{align*}
	\mathscr{F}_t = \sigma\left(B_s,0\leq s\leq t\right),\ \forall t\geq 0.
\end{align*}
Then $(B_t)_{t\geq 0}$ is a martingale with respect to $(\mathscr{F}_t)_{t\geq 0}$.

\paragraph{Theorem 4.16\label{thm:4.16}} (Blumenthal's 0-1 law). The $\sigma$-algebra $\mathscr{F}_{0+}=\bigcap_{t>0}\mathscr{F}_t$ is $\P$-trivial.
\begin{proof}
Let $0<t_1<\cdots<t_n$, and let $g$ be a bounded continuous function on $\mathbb{R}^n$. According to continuity and dominated convergence theorem, for all $A\in\mathscr{F}_{0+}$, we have
\begin{align*}
	\E\left[\mathds{1}_A g(B_{t_1},\cdots,B_{t_n})\right] = \lim_{\epsilon\downdownarrows 0}\E\left[\mathds{1}_A g(B_{t_1}-B_\epsilon,\cdots,B_{t_n}-B_\epsilon)\right].
\end{align*}

By simple Markov property of Brownian motions [\hyperref[prop:4.5]{Proposition 4.5 (iii)}], $B_{t_1}-B_\epsilon,\cdots,B_{t_n}-B_\epsilon$ is independent of $\mathscr{F}_\epsilon\supset\mathscr{F}_{0+}$ whenever $0<\epsilon<t_1$. Hence
\begin{align*}
	\E\left[\mathds{1}_A g(B_{t_1},\cdots,B_{t_n})\right]&=\P(A)\,\lim_{\epsilon\downdownarrows 0}\E\left[g(B_{t_1}-B_\epsilon,\cdots,B_{t_n}-B_\epsilon)\right] =\P(A)\,\E[g(B_{t_1},\cdots,B_{t_n})].
\end{align*}

For any open set $U\in\mathscr{B}(\mathbb{R}^n)$, take a sequence $g_n(x)=d(x,U^c)/(d(x,U^c)+n^{-1})$ of bounded continuous functions such that $g_n\uparrow\uparrow\mathds{1}_U$ pointwise. Then
\begin{align*}
\E\left[\mathds{1}_A \mathds{1}_U(B_{t_1},\cdots,B_{t_n})\right]=\P(A)\,\E[\mathds{1}_U(B_{t_1},\cdots,B_{t_n})].
\end{align*}

Since $\mathscr{B}(\mathbb{R}^n)$ is generated by all open sets in $\mathbb{R}^n$, an argument of Sierpiński-Dynkin $\pi$-$\lambda$ theorem implies that $\mathscr{F}_{0+}$ is independent of $\sigma(B_{t_1},\cdots,B_{t_n})$. This holds for all finite marginals $0<t_1<\cdots<t_n$, hence $\mathscr{F}_{0+}$ is independent of $\sigma(B_t,t>0)$. By right-continuity of $t\mapsto B_t(\omega)$, $B_0=\lim_{t\to 0}B_t$ is measurable with respect to $\sigma(B_t,t>0)$, and we have $\sigma(B_t,t>0)=\sigma(B_t,t\geq 0)\supset\mathscr{F}_{0+}$. Consequently, $\mathscr{F}_{0+}$ is independent of itself, and the result follows.
\end{proof}

\paragraph{Proposition 4.17.\label{prop:4.17}}  Let $(B_t)_{t\geq 0}$ be a Brownian motion.
\begin{itemize}
\item[(i)] Almost surely, we have
\begin{align*}
	\sup_{0\leq s\leq\epsilon} B_s>0\ \ \textit{and}\ \ \inf_{0\leq s\leq\epsilon} B_s<0,\quad \forall\epsilon>0.
\end{align*}
\item[(ii)] For every $\alpha\in\mathbb{R}$, define a stopping time $\tau_\alpha=\inf\{t\geq 0:B_t=\alpha\}$ (with respect to the canonical filtration, with the convention $\inf\emptyset=\infty$). Then we have $\tau_\alpha<\infty\ a.s.$. Consequently, it holds
\begin{align*}
	\limsup_{t\to\infty} B_t=\infty\ \ \textit{and}\ \  \liminf_{t\to\infty} B_t=-\infty\quad a.s..
\end{align*}
\end{itemize}
\begin{proof}
(i) We choose a sequence $0<\epsilon_n\downdownarrows 0$, and define the monotone decreasing intersection:
\begin{align*}
	A = \bigcap_{n=1}^\infty\left\{\sup_{0\leq s\leq\epsilon_n}B_s>0\right\}\quad\Rightarrow\quad \forall t>0,\exists \epsilon_n<t\ \textit{with}\ \bigcap_{k=n}^\infty\left\{\sup_{0\leq s\leq\epsilon_k}B_s>0\right\}\in\mathscr{F}_t\ \quad\Rightarrow\quad A\in\mathscr{F}_{0+}.
\end{align*}
Hence we have either $\P(A)=0$ or $\P(A)=1$. If $\P(A)=1$, the result follows. To show $\P(A)=1$, note that
\begin{align*}
	\P\left(\sup_{0\leq s\leq\epsilon_n}B_s>0\right)\geq\P\left(B_{\epsilon_n}>0\right)=\frac{1}{2},\ \forall n\in\mathbb{N}\quad\Rightarrow\quad \P(A)\geq\frac{1}{2}.
\end{align*}

(ii) By simple Markov property of Brownian motions [\hyperref[prop:4.5]{Proposition 4.5 (ii)}], for all $\lambda>0$, the process $B_t^\lambda=\frac{1}{\lambda}B_{\lambda^2t}$ is also a Brownian motion. Since all Brownian motions are identically distributed [according to the Wiener measure on $C(\mathbb{R}_+)$], we have.
\begin{align*}
	\P\left(\sup_{0\leq s\leq 1}B_s > \lambda\right) = \P\left(\sup_{0\leq s\leq 1/\lambda^2}B_s^\lambda > 1\right) = \P\left(\sup_{0\leq s\leq 1/\lambda^2}B_s > 1\right)
\end{align*}
Let $\lambda\downdownarrows 0$. By monotone convergence theorem, we have
\begin{align*}
	1=\P\left(\sup_{0\leq s\leq 1}B_s > 0\right) = \P\left(\sup_{s\geq 0}B_s > 1\right) = \P\left(\sup_{s\geq 0}B_s^\alpha > 1\right)=\P\left(\sup_{s\geq 0}B_s > \alpha\right)\leq\P(\tau_\alpha<\infty),\quad \forall \alpha>0.
\end{align*}
Therefore $\tau_\alpha<\infty\ a.s.$ for all $\alpha>0$, which holds only if $\limsup_{t\to\infty} B_t=\infty\ a.s.$.

The symmetric arguments of (i) and (ii) follow by replacing $B_s$ by $-B_s$.
\end{proof}

The following proposition follows immediately from Remark III under \hyperref[def:4.2]{Definition 4.2}, which is a property of Gaussian white noise.

\paragraph{Proposition 4.18.\label{prop:4.18}} Let $(B_t)_{t\geq 0}$ be a Brownian motion. Let $0=t_0^n<t_1^n<\cdots<t_{k_n}^n=t$ be a sequence of partitions of $[0,t]$ with the mesh $\max_{1\leq j\leq k_n}(t_j^n-t_{j-1}^n)\to 0$. Then
\begin{align*}
	\lim_{n\to\infty}\sum_{j=1}^{k_n}(B_{t_j^n}-B_{t_{j-1}^n})^2 = t\quad \textit{in}\ L^2.
\end{align*}

\paragraph{Corollary 4.19.\label{cor:4.19}} Let $(B_t)_{t\geq 0}$ be a Brownian motion.
\begin{itemize}
\item[(i)] Almost surely, the sample path $t\mapsto B_t$ is not monotone on any nontrivial interval $[a,b]$.
\item[(ii)] Almost surely, the sample path $t\mapsto B_t$ has infinite total variation on any nontrivial interval $[a,b]$.
\item[(iii)] $(B_t)_{t\geq 0}$ has finite quadratic variation $\langle B,B\rangle_t=t$.
\end{itemize}
\begin{proof}
(i) By simple Markov property of $(B_t)_{t\geq 0}$ and \hyperref[prop:4.17]{Proposition 4.17}, we have
\begin{align*}
	\sup_{q\leq t\leq q+\epsilon} B_t>B_q,\quad \inf_{q\leq t\leq q+\epsilon} B_t<B_q,\ \forall \epsilon>0,\ \forall q\in\mathbb{Q},\ a.s..
\end{align*}
For any nontrivial interval $[a,b]$, we just choose $q\in\mathbb{Q}$ and $\epsilon>0$ with $[q,q+\epsilon]\subset[a,b]$.\vspace{0.1cm}

(ii) By simple Markov property of $(B_t)_{t\geq 0}$, it suffices to consider intervals $[0,t]$. Choose an increasing sequence of partitions of $[0,t]$ as in \hyperref[prop:4.18]{Proposition 4.18}, we have
\begin{align*}
	\left(\sup_{1\leq j\leq k_n}\left\vert B_{t_j^n}-B_{t_{j-1}^n}\right\vert\right)\sum_{j=1}^{k_n}\left\vert B_{t_j^n}-B_{t_{j-1}^n}\right\vert\geq \sum_{j=1}^{k_n}\left(B_{t_j^n}-B_{t_{j-1}^n}\right)^2\overset{L^2}{\to}t.
\end{align*}
As $n\to\infty$, we have $\sup_{1\leq j\leq k_n}\vert B_{t_j^n}-B_{t_{j-1}^n}\vert\to 0$ by continuity, which implies $\sum_{j=1}^{k_n}\vert B_{t_j^n}-B_{t_{j-1}^n}\vert\to\infty\ a.s.$. The result follows by taking intersection of all $0<t\in\mathbb{Q}$. (iii) is a consequence of \hyperref[thm:3.56]{Theorem 3.56}.
\end{proof}

\paragraph{Proposition 4.20\label{prop:4.20}} (Non-differentiability). Let $(B_t)_{t\geq 0}$ be a Brownian motion. Then
\begin{align*}
	\limsup_{t\downdownarrows 0}\frac{B_t}{\sqrt{t}}=\infty\ \ \textit{and}\ \ \liminf_{t\downdownarrows 0}\frac{B_t}{\sqrt{t}}=-\infty\quad a.s..
\end{align*}
Consequently, by simple Markov property of $(B_t)_{t\geq 0}$, for every $s>0$, the function $t\mapsto B_t$ has $a.s.$ no right derivative, hence is non-differentiable at $s$.
\begin{proof}
We prove that for all $\alpha>0$, almost surely,
\begin{align*}
	\sup_{0\leq s\leq\epsilon}\frac{B_s}{\sqrt{s}}>\alpha,\ \ \forall \epsilon>0.\tag{4.3}\label{eq:4.3}
\end{align*}
This statement holds only if $\limsup_{t\downdownarrows 0}\frac{B_t}{\sqrt{t}}=\infty$. Take a decreasing sequence $0<\epsilon_n\downdownarrows 0$, and define the decreasing intersection
\begin{align*}
	A=\bigcap_{n=1}^\infty\left\{\sup_{0\leq s\leq\epsilon_n}\frac{B_s}{\sqrt{s}}>\alpha\right\}\in\mathscr{F}_{0+}.
\end{align*}
According to Blumenthal's 0-1 law, we have either $\P(A)=0$ or $\P(A)=1$. Note that
\begin{align*}
	&\P\left(\sup_{0\leq s\leq\epsilon_n}\frac{B_s}{\sqrt{s}}>\alpha\right)\geq \P\left(\frac{B_{\epsilon_n}}{\sqrt{\epsilon_n}}>\alpha\right)=\frac{1}{\sqrt{2\pi}}\int_\alpha^\infty\e^{-\frac{x^2}{2}}\,\d x\quad\Rightarrow\quad \P(A)\geq \frac{1}{\sqrt{2\pi}}\int_\alpha^\infty\e^{-\frac{x^2}{2}}\,\d x>0.
\end{align*}
Therefore $\P(A)=1$, and the result \hyperref[eq:4.3]{(4.3)} follows. A symmetric argument holds if we replace $B_s$ by $-B_s$.
\end{proof}

\paragraph{Proposition 4.21\label{prop:4.21}} (Global non-differentiability). Let $(B_t)_{t\geq 0}$ be a Brownian motion. Almost surely, the sample path $t\mapsto B_t$ is nowhere differentiable.
\begin{proof}
By continuity of sample paths of $(B_t)_{t\geq 0}$ on $[0,2]$, we have 
\begin{align*}
	\left\{\exists t_0\in[0,1]\ \textit{such that}\ \limsup_{h\downdownarrows 0}\frac{\vert B_{t_0+h}-B_{t_0}\vert}{h}<\infty\right\}\ \Rightarrow\ \left\{\exists t_0\in[0,1]\ \textit{such that}\ \sup_{h\in(0,1]}\frac{\vert B_{t_0+h}-B_{t_0}\vert}{h}<\infty\right\}
\end{align*}
To show that $t\mapsto B_t$ is $a.s.$ nowhere differentiable on $[0,1]$, it suffices to show that for all $M>0$,
\begin{align*}
	\P\left(\exists t_0\in[0,1]\ \textit{such that}\ \sup_{h\in(0,1]}\frac{\vert B_{t_0+h}-B_{t_0}\vert}{h}\leq M\right)=0.\tag{4.4}\label{eq:4.4}
\end{align*}
Fix $M>0$. Given $n>2$, choose $k\in\mathbb{Z}$ with $t_0\in\left[(k-1)2^{-n},k2^{-n}\right]$. Then for all $1\leq j\leq 2^n-k$, it holds
\begin{align*}
	\left\vert B_{(k+j)2^{-n}}-B_{(k+j-1)2^{-n}}\right\vert\leq\left\vert B_{(k+j)2^{-n}}-B_{t_0}\right\vert + \left\vert B_{t_0}-B_{(k+j-1)2^{-n}}\right\vert\leq M(2j+1)2^{-n}.
\end{align*}
Define events $E_{n,k}:=\left\{\left\vert B_{(k+j)2^{-n}}-B_{(k+j-1)2^{-n}}\right\vert\leq M(2j+1)2^{-n},\ \forall j=1,2,3\right\}$. Since the process $(B_t)_{t\geq 0}$ has independent increments, we have
\begin{align*}
	\P(E_{n,k})\leq&\prod_{j=1}^3\P\left(\left\vert B_{(k+j)2^{-n}}-B_{(k+j-1)2^{-n}}\right\vert\leq M(2j+1)2^{-n}\right)\leq\P\left(\vert B_1\vert\leq 7M2^{-n/2}\right)^3\leq\frac{(7M)^32^{-3n/2}}{27}\\
	&\Rightarrow\quad\P\left(\bigcup_{k=1}^{2^n-3}E_{n,k}\right)\leq 2^n\frac{(7M)^32^{-3n/2}}{27}=\frac{(7M)^32^{-n/2}}{27}\quad\Rightarrow\quad \sum_{n=1}^\infty \P\left(\bigcup_{k=1}^{2^n-3}E_{n,k}\right)<\infty.
\end{align*}
By Borel-Cantelli lemma, we have
\begin{align*}
	\P\left(\exists t_0\in[0,1]\ \textit{such that}\ \sup_{h\in(0,1]}\frac{\vert B_{t_0+h}-B_{t_0}\vert}{h}\leq M\right)\leq\P\left(\bigcup_{k=1}^{2^n-3}E_{n,k}\ \textit{infinitely occurs}\right) = 0.
\end{align*}
This completes the proof of \hyperref[eq:4.4]{(4.4)}. To show that $t\mapsto B_t$ is $a.s.$ nowhere differentiable on $\mathbb{R}_+$, take intersection of an analogue of \hyperref[eq:4.4]{(4.4)} on all intervals $[n-1,n],\ n\in\mathbb{N}$.
\end{proof}

\paragraph{Theorem 4.22\label{thm:4.22}} (Law of iterated logarithm). Let $(B_t)_{t\geq 0}$ be a Brownian motion. Then
\begin{align*}
	\limsup_{t\to\infty}\frac{B_t}{\sqrt{2t\log\log t}} = 1\quad a.s..
\end{align*}
\begin{proof}
We first give a tail bound of $B_1\sim N(0,1)$, which is a standard Gaussian variable:
\begin{align*}
	\frac{\alpha\e^{-\alpha^2/2}}{(1+\alpha^2)\sqrt{2\pi}}=\frac{1}{\sqrt{2\pi}}\int_\alpha^\infty \frac{\e^{-t^2/2}}{(1+t^2)^2}\,\d t\leq\frac{1}{\sqrt{2\pi}}\int_\alpha^\infty \e^{-t^2/2}\,\d t\leq \frac{1}{\sqrt{2\pi}}\int_\alpha^\infty\frac{t\e^{-t^2/2}}{\alpha}\,\d t=\frac{\e^{-\alpha^2/2}}{\alpha\sqrt{2\pi}},\quad \forall\alpha>0.
\end{align*}

Let $h(t)=\sqrt{2t\log\log t}$. Using the law of $M_t:=\sup_{0\leq s\leq t} B_s\overset{d}{=}\vert B_t\vert$ given in \hyperref[cor:4.25]{Corollary 4.25}, for all $r>1$ and all $\delta>0$, whenever $n\geq n_0:=1+\frac{\e}{\log r}$, we have
\begin{align*}
	\P\left(\frac{M_{r^n}}{h(r^{n-1})}>\sqrt{r+\delta}\right)&=\P\left(\frac{\vert B_{r^n}\vert}{\sqrt{r^n}}>\sqrt{2\left(1+\frac{\delta}{r}\right)\log\log r^{n-1}}\right)\leq C(n-1)^{-1-\frac{\delta}{r}},
\end{align*}
where $C=C(\delta,r)$ is some constant independent of $n$. Hence we have
\begin{align*}
	\sum_{n=n_0}^\infty \P\left(\sup_{s\in[r^{n-1},r^n]}\frac{B_s}{h(s)}>\sqrt{r+\delta}\right)\leq\sum_{n=n_0}^\infty \P\left(\frac{M_{r_n}}{h(r^{n-1})}>\sqrt{r+\delta}\right)\leq C\sum_{n= n_0-1}^\infty n^{-1-\frac{\delta}{r}} < \infty.
\end{align*}
By Borel-Cantelli lemma, we have
\begin{align*}
	\P\left(\limsup_{n\to\infty}\frac{B_s}{h(s)}\leq \sqrt{r+\delta}\right)=\P\left(\sup_{s\in[r^{n-1},r^n]}\frac{B_s}{h(s)}>\sqrt{r+\delta}\quad \textit{for finitely many}\ n\right)=1.
\end{align*}
Let $\delta\downdownarrows 0$ and $r\downdownarrows 1$, we have $\P\left(\limsup_{n\to\infty}\frac{B_s}{h(s)}\leq 1\right)=1$.\vspace{0.1cm}

Now we prove $\limsup_{n\to\infty}\frac{B_s}{h(s)}\geq 1\ a.s.$. Given $n\geq n_0$, we have $\sqrt{2\log(n\log r)}>1$, and
\begin{align*}
	\P\left(B_{r^n}-B_{r^{n-1}}\geq\sqrt{\frac{r-1}{r}}h(r^n)\right) &\geq \P\left(\frac{B_{r^n}-B_{r^{n-1}}}{\sqrt{r^{n-1}(r-1)}}\geq\sqrt{2\log(n\log r)}\right)\geq \frac{C}{n\sqrt{\log n}}
\end{align*}
for some constant $C=C(r)$ independent of $n$. Hence
\begin{align*}
	\sum_{n=n_0}^\infty\P\left(B_{r^n}-B_{r^{n-1}}\geq\sqrt{\frac{r-1}{r}}h(r^n)\right)\geq C\sum_{n=n_0}^\infty\frac{1}{n\sqrt{\log n}}=\infty.
\end{align*}
Since $(B_t)_{t\geq 0}$ has independent increments, the Borel-Cantelli lemma implies that $B_{r^n}-B_{r^{n-1}}\geq\sqrt{\frac{r-1}{r}}h(r^n)$ for infinitely many $n$. A symmetric argument of the first part of our proof implies $\liminf_{n\to\infty}\frac{B_s}{h(s)}\geq -1\ a.s.$. Hence for $a.s.$ $\omega\in\Omega$, we can find some $N(\omega)$ such that for all $n\geq N(\omega)$,
\begin{align*}
	B_{r^{n-1}}\geq -2h(r^{n-1})\geq -\frac{2}{\sqrt{r}}h(r^n)\quad \Rightarrow\quad B_{r^n}\geq \left(1-\frac{3}{\sqrt{r}}\right)h(r^n)\quad\textit{infinitely occurs}.
\end{align*}
Consequently, for all $r>1$, we have $\P\left(\limsup_{t\to\infty}\frac{B_t}{h(t)}\geq 1-\frac{3}{\sqrt{r}}\right)=1$. Letting $r\to\infty$ suffices.
\end{proof}

\newpage
\subsection{Strong Markov Property and Reflection Principle}
In this section we discuss the martingale properties of a Brownian motion $(B_t)_{t\geq 0}$. Let $\tau$ be a stopping time (with respect to the canonical filtration $(\mathscr{F}_t)_{t\geq 0}$ of $(B_t)_{t\geq 0}$). We define a random variable $\mathds{1}_{\{\tau<\infty\}}B_\tau$:
\begin{align*}
	\mathds{1}_{\{\tau<\infty\}}B_{\tau}(\omega)= 
	\mathds{1}_{\{\tau(\omega)<\infty\}}B_{\tau(\omega)}(\omega)+0\mathds{1}_{\{\tau(\omega)=\infty\}}.
\end{align*}

This is a $\mathscr{F}_\tau$-measurable variable. To see this, note that $(B_t)_{t\geq 0}$ is an adaptive and sample-continuous process, hence is progressive [\hyperref[prop:3.9]{Proposition 3.9}]. Then the desired result follows from \hyperref[prop:3.12]{Proposition 3.12}.

\paragraph{Theorem 4.23\label{thm:4.23}} (Strong Markov property). Let $\tau$ be a stopping time with $\P(\tau<\infty)>0$. Let
\begin{align*}
	B_t^{(\tau)}=\mathds{1}_{\{\tau<\infty\}}(B_{\tau+t}-B_\tau),\quad\forall t\in\mathbb{R}_+.
\end{align*}
Then $(B_t^{(\tau)})_{t\geq 0}$ is a Brownian motion under the measure $\P(\cdot|\tau<\infty)$, and is independent of $\mathscr{F}_\tau$.

\begin{proof}
We first deal with the case $\tau<\infty\ a.s.$. Fix $A\in\mathscr{F}_\tau$ and $0=t_0<t_1<\cdots<t_n$. We claim that
\begin{align*}
	\E\bigl[\mathds{1}_A g(B_{t_0}^{(\tau)},B_{t_1}^{(\tau)},\cdots,B_{t_n}^{(\tau)})\bigr] = \P(A)\,\E\left[g(B_{t_0},B_{t_1},\cdots,B_{t_n})\right]
\end{align*} 
for all bounded continuous functions $g:\mathbb{R}^n\to\mathbb{R}$. If we take $A=\Omega$, a similar argument to the proof of \hyperref[thm:4.16]{Theorem 4.16} implies that $(B_{t_0}^{(\tau)},B_{t_1}^{(\tau)},\cdots,B_{t_n}^{(\tau)})\overset{d}{=}(B_{t_0},B_{t_1},\cdots,B_{t_n})$ for all choices $0=t_0<t_1<\cdots<t_n$. Since sample paths of $(B_t^{(\tau)})_{t\geq 0}$ are continuous, \hyperref[prop:4.4]{Proposition 4.4} implies that $(B_t^{(\tau)})_{t\geq 0}$ is also a Brownian motion. Furthermore, $(B_{t_0}^{(\tau)},B_{t_1}^{(\tau)},\cdots,B_{t_n}^{(\tau)})$ is independent of $\mathscr{F}_\tau$, and $(B_t^{(\tau)})_{t\geq 0}$ is independent of $\mathscr{F}_\tau$.

Now we prove the claim. For $p\in\mathbb{N}$, take $[t]_p=\min\{k2^{-p}:k2^{-p}\geq t,k\in\mathbb{Z}\}$ with convention $[\infty]_p=\infty$, and write $\tau_p=[\tau]_p$. By continuity we have $B_t^{(\tau_p)}\to B_t^{(\tau)}\ a.s.$, and dominated convergence implies
\begin{align*}
\E&\left[\mathds{1}_A g(B_{t_0}^{(\tau)},B_{t_1}^{(\tau)},\cdots,B_{t_n}^{(\tau)})\right]=\lim_{p\to\infty}\E\left[\mathds{1}_A g(B_{t_0}^{(\tau_p)},B_{t_1}^{(\tau_p)},\cdots,B_{t_n}^{(\tau_p)})\right]\\
&=\lim_{p\to\infty}\sum_{k=0}^\infty\E\left[\mathds{1}_{A\,\cap\, \{(k-1)2^{-p}<\tau\leq k2^{-p}\}}g\left(B_{k2^{-p}+t_0}-B_{k2^{-p}},B_{k2^{-p}+t_1}-B_{k2^{-p}},\cdots,B_{k2^{-p}+t_n}-B_{k2^{-p}}\right)\right]\\
&=\lim_{p\to\infty}\sum_{k=0}^\infty\P(A\cap \{(k-1)2^{-p}<\tau\leq k2^{-p}\})\E\left[g\left(B_{t_0},B_{t_1},\cdots,B_{t_n}\right)\right]=\P(A)\E\left[g\left(B_{t_0},B_{t_1},\cdots,B_{t_n}\right)\right],
\end{align*}
where the last row follows from the fact that $$A\cap \{(k-1)2^{-p}<\tau\leq k2^{-p}\}=\left(A\cap\{\tau\leq k2^{-p}\}\right)\cap\{\tau\leq(k-1)2^{-p}\}^c\in\mathscr{F}_{k2^{-p}}$$ and simple Markovian property of $(B_t)_{t\geq 0}$. Thus we completes the proof for case $\tau<\infty\ a.s.$. For the general case $\P(\tau<\infty)>0$, we have
\begin{align*}
	\E\bigl[\mathds{1}_{A\cap\{\tau<\infty\}} g(B_{t_0}^{(\tau)},B_{t_1}^{(\tau)},\cdots,B_{t_n}^{(\tau)})\bigr] = \P(A\cap\{\tau<\infty\})\,\E\left[g(B_{t_0},B_{t_1},\cdots,B_{t_n})\right]
\end{align*} 
Then the desired result follows in a straightforward way.
\end{proof}

\paragraph{Theorem 4.24\label{thm:4.24}} (Reflection principle). Given $t>0$, let $M_t=\sup_{0\leq s\leq t}B_s$. For any $a>0$ and $b\in(-\infty,a]$,
\begin{align*}
	\P(M_t\geq a,B_t\leq b)=\P(B_t\geq 2a-b).\tag{4.5}\label{eq:4.5}
\end{align*}
In particular, we have $M_t\overset{d}{=}\vert B_t\vert$.
\begin{proof}
Define the hitting time $\tau_a=\inf\{s\geq 0:B_s=a\}$. By \hyperref[prop:4.17]{Proposition 4.17 (ii)}, we have $\tau_a<\infty\ a.s.$. And by \hyperref[thm:4.23]{Theorem 4.23}, the process $B_t^{(\tau_a)}=B_{\tau_a+t}-B_{\tau_a}=B_{\tau_a+t}-a$ is a Brownian process independent of $\mathscr{F}_{\tau_a}$. Consequently, $(\tau_a,B^{(\tau_a)})\overset{d}{=}(\tau_a,-B^{(\tau_a)})$, whose distribution equals the product of the law of $\tau_a$ and the Wiener measure $\mu_W$ on $C(\mathbb{R}_+)$. Let $H=\left\{(s,f)\in\mathbb{R}\times C(\mathbb{R}_+):s\leq t, f(t-s)\leq b-a\right\}$. Hence
\begin{align*}
	\P\left(M_t\geq a,B_t\leq b\right)&=\P\left(\tau_a\leq t,B_t\leq b\right)=\P\left(\tau_a\leq t,B_{t-\tau_a}^{(\tau_a)}\leq b-a\right)=\P\left((\tau_a,B^{(\tau_a)})\in H\right)\\
	&=\P\left((\tau_a,-B^{(\tau_a)})\in H\right)=\P(\tau_a\leq t,-B_{t-\tau_a}^{(\tau_a)}\leq b-a)\\
	&= \P(\tau_a\leq t,B_t\geq 2a-b) = \P(B_t\geq 2a-b).
\end{align*}
For the last assertion, note that
\begin{align*}
	\P(M_t\geq a)&=\P(M_t\geq a, B_t\geq a) + \P(M_t\geq a, B_t\leq a)
	=\P(B_t\geq a) + \P(B_t\geq 2a-a) = 2\P(B_t\geq a).
\end{align*}
Thus we complete the proof.
\end{proof}

\paragraph{Remark.} According to \hyperref[eq:4.5]{(4.5)}, we also have
\begin{align*}
	\P(M_t\leq y,B_t\leq x)=\begin{cases}
		\P(B_t\leq x)-\P(B_t\geq 2y-x)\ &\textit{if}\ y>0,\ x\leq y\\
		\P(M_t\leq y) &\textit{if}\ y>0,\ x>y.
	\end{cases}
\end{align*}
This gives the density of $(M_t,B_t)$:
\begin{align*}
	\rho_{M_t,B_t}(y,x)=\frac{\partial}{\partial y}\left(\frac{1}{\sqrt{2\pi t}}\left(\e^{-\frac{x^2}{2t}}+\e^{-\frac{(2y-x)^2}{2t}}\right)\right) = \frac{2(2y-x)}{t^{3/2}\sqrt{2\pi}}\e^{-\frac{(2y-x)^2}{2t}},\quad y>0,\ x\leq y.
\end{align*}

\paragraph{Corollary 4.25\label{cor:4.25}} (Law of hitting times). Given $a>0$, the hitting time $\tau_a=\inf\{s\geq 0:B_s=a\}$ is identically distributed to $a^2B_1^{-2}$.
\begin{proof}
By the last assertion of \hyperref[thm:4.24]{Theorem 4.24}, we have
\begin{align*}
	\P\left(\tau_a\leq t\right)=\P\left(M_t\geq a\right) = \P\left(\vert B_t\vert\geq a\right) = \P(B_t^2\geq a^2) = \P\left(\frac{a^2}{B_1^2}\leq t\right),
\end{align*}
which holds for all $t\geq 0$.
\end{proof}
\paragraph{Remark.} Since $B_1\sim N(0,1)$, we can derive the density of $\tau_a$:
\begin{align*}
	\rho_{\tau_a}(t)=\frac{2\mathds{1}_{\{t> 0\}}}{\sqrt{2\pi}}\e^{-\frac{a^2}{2t}}\frac{\d}{\d t}\left(\frac{a}{\sqrt{t}}\right) = \frac{a}{\sqrt{2\pi t^3}}\e^{-\frac{a^2}{2t}}\mathds{1}_{\{t>0\}}.
\end{align*}
This also implies $\E[\tau_a]= \infty$ for all $a>0$.

\paragraph{Proposition 4.26.\label{prop:4.26}} Let $(\mathscr{F}_t)_{t\geq 0}$ be the canonical filtration of a Brownian motion $(B_t)_{t\geq 0}$. All these processes are martingales with respect to $(\mathscr{F}_t)_{t\geq 0}$: (i) $B_t$; (ii) $B_t^2-t$; (iii) $\exp(\theta B_t-\frac{1}{2}\theta^2t)$, $\theta\in\mathbb{R}$.

\paragraph{Proposition 4.27\label{prop:4.27}} (Laplacian transform of hitting times). Given $a>0$, the hitting time $\tau_a=\inf\{s\geq 0:B_s=a\}$ satisfies
\begin{align*}
	\E\left[\e^{-\lambda\tau_a}\right]=\e^{-a\sqrt{2\lambda}},\quad\forall\lambda>0.
\end{align*}
\begin{proof}
We consider the martingale $N_t^\theta=\exp(\theta B_t-\frac{1}{2}\theta^2t)$, where $\theta>0$. By \hyperref[cor:3.50]{Corollary 3.50}, $N_{t\wedge\tau_a}^\theta$ is a martingale bounded by $\e^{\theta a}$ from above, hence is uniformly integrable. As a result,
\begin{align*}
	\E\left[N_{\tau_a}^\theta\right]=\lim_{t\to\infty}\E\left[N_{t\wedge\tau_a}^\theta\right] = \E[N_0^\theta] = 1.
\end{align*}
Since $\tau_a<\infty\ a.s.$, we have
\begin{align*}
	1=\E\left[N_{\tau_a}^\theta\right] = \E\left[\exp\left(\theta a-\frac{1}{2}\theta^2\tau_a\right)\right]\quad\Rightarrow\quad \E\left[\e^{-\lambda\tau_a}\right]=\e^{-a\sqrt{2\lambda}},
\end{align*}
where the implication follows by setting $\theta=\sqrt{2\lambda}$.
\end{proof}

\paragraph{Proposition 4.28\label{prop:4.28}} (Exit times). Given $a\in\mathbb{R}$, set the hitting time $\tau_a=\inf\{s\geq 0:B_s=a\}$.
\begin{itemize}
\item[(i)] (Law of the exit point from an interval). For every $a<0<b$, we have
\begin{align*}
	\P(\tau_a<\tau_b)=\frac{b}{b-a},\quad\textit{and}\quad \P(\tau_b<\tau_a)=\frac{-a}{b-a}.
\end{align*}
\item[(ii)] (First moment of exit times). For every $a<0<b$, the exit time $\tau=\tau_a\wedge\tau_b$ satisfies $\E[\tau]=-ab$.
\item[(iii)] (Laplacian transform of exit times). For every $a>0$ and every $\lambda>0$, the exit time $\tau=\tau_a\wedge\tau_b$ satisfies
\begin{align*}
	\E\left[\e^{-\lambda\upsilon_a}\right]=\frac{\cosh\left(\frac{b+a}{2}\sqrt{2\lambda}\right)}{\cosh\left(\frac{b-a}{2}\sqrt{2\lambda}\right)}.
\end{align*}
\end{itemize}
\begin{proof}
(i) We define a stopping time $\tau=\tau_a\wedge\tau_b$. By \hyperref[cor:3.50]{Corollary 3.50}, we choose the stopped martingale $(B_{t\wedge\tau})_{t\geq 0}$, which satisfies $\vert B_{t\wedge\tau}\vert\leq (-a)\vee b$, hence is uniformly integrable. As a result, $\E\left[B_\tau\right]=\E\left[B_0\right]=0$. Note that $\tau_a\neq\tau_b\ a.s.$, and $\E[B_\tau]=a\P(\tau_a<\tau_b)+b\P(\tau_b<\tau_a)$, the result follows.\vspace{0.1cm}

(ii) Consider the martingale $A_t=B_t^2-t$. Then $\E[A_{t\wedge\tau}]=\E[A_0]=0$, which gives $\E[B_{t\wedge\tau}^2]=\E[t\wedge\tau]$. On the other hand, the monotone convergence theorem implies $\E[t\wedge\tau]\to\E[\tau]$ as $t\to\infty$.  On the other hand, since $B_{t\wedge\tau}^2 \leq a^2\vee b^2$, we have $\E[B_{t\wedge\tau}^2]\to\E[B_\tau^2]$ as $t\to\infty$ by dominated convergence theorem. Note that
\begin{align*}
	\E\left[B_{\tau}^2\right]=\E\left[a^2\mathds{1}_{\{B_\tau=a\}}+b^2\mathds{1}_{\{B_\tau=b\}}\right] = a^2\P(\tau_a<\tau_b)+b^2\P(\tau_b<\tau_a)=-ab.
\end{align*}
(iii) Similar to \hyperref[prop:4.26]{Proposition 4.26 (iii)}, we take the following martingale:
\begin{align*}
	N_t &= \frac{1}{2}\exp\left(\sqrt{2\lambda}\left(B_t-\frac{a+b}{2}\right)-\lambda t\right)+\frac{1}{2}\exp\left(-\sqrt{2\lambda}\left(B_t-\frac{a+b}{2}\right)-\lambda t\right)\\
	&=\e^{-\lambda t}\cosh\left(\sqrt{2\lambda}\left(B_t-\frac{a+b}{2}\right)\right),\quad t\geq 0.
\end{align*}
Since $0\leq N_{t\wedge\tau}\leq \cosh\left(\frac{b-a}{2}\sqrt{2\lambda}\right)$, it is a uniformly integrable martingale. Consequently,
\begin{align*}
	\E\left[N_\tau\right]=\lim_{n\to\infty}\left[N_{t\wedge\tau}\right]=\E[N_0]=\cosh\left(\frac{a+b}{2}\sqrt{2\lambda}\right).
\end{align*}
On the other hand, since $B_{\tau}\in\{a,b\}\ a.s.$, we have
\begin{align*}
	\E[N_\tau]= \E\left[\e^{-\lambda\tau}\left(\mathds{1}_{\{B_\tau=a\}}\cosh\left(\frac{a-b}{2}\sqrt{2\lambda}\right)+\mathds{1}_{\{B_\tau=b\}}\cosh\left(\frac{b-a}{2}\sqrt{2\lambda}\right)\right)\right]=\E\left[\e^{-\lambda\tau}\right]\cosh\left(\frac{b-a}{2}\sqrt{2\lambda}\right).
\end{align*}
Then the desired result follows.
\end{proof}

\paragraph{Proposition 4.29\label{prop:4.29}} (Time reversal). Set $\widetilde{B}_t=B_1-B_{1-t}$ for every $t\in[0,1]$. Then $(\widetilde{B}_t)_{t\in[0,1]}$ is a Brownian motion on $[0,1]$, which has the same law as $(B_t)_{t\in[0,1]}$.
\begin{proof}
Clearly, $\widetilde{B}_0=0$, and $(\widetilde{B}_t)_{t\in[0,1]}$ has continuous sample paths. We show that $(B_t)_{t\in[0,1]}$ and $(\widetilde{B}_t)_{t\in[0,1]}$ has the same finite-dimensional marginal distributions, which are extended to the Wiener measure on $C([0,1])$. Take a partition $0=t_0<t_1<\cdots<t_n=1$. Then the increments $$(\widetilde{B}_{t_1},\widetilde{B}_{t_2}-\widetilde{B}_{t_1},\cdots,\widetilde{B}_{t_n}-\widetilde{B}_{t_{n-1}})=(B_1-B_{1-t_1},B_{1-t_1}-B_{1-t_2},\cdots,B_{1-t_{n-1}})$$
are jointly Gaussian and independent, and the desired result follows.
\end{proof}

\paragraph{Corollary 4.30.\label{cor:4.30}} Let $M_t=\sup_{0\leq s\leq t}B_t$. Then $M_t-B_t\overset{d}{=}M_t\overset{d}{=}\vert B_t\vert$ for every $t>0$.
\begin{proof}
Fix $t>0$. Akin to \hyperref[prop:4.29]{Proposition 4.29}, we define $\widetilde{B}_s=B_t-B_{t-s}$ for every $s\in[0,t]$. By symmetry and time reversal property of Brownian motions, all $(B_s)_{s\in[0,t]}$, $(\widetilde{B}_s)_{s\in[0,t]}$ and $(-\widetilde{B}_s)_{s\in[0,t]}$ have the same law. Consequently, $\sup_{0\leq s\leq t}B_t\overset{d}{=}\sup_{0\leq s\leq t}(-\widetilde{B}_t)$, which is in fact $M_t\overset{d}{=} M_t-B_t$.
\end{proof}

\paragraph{Proposition 4.31\label{prop:4.31}} (Arcsine law). We let $M_1=\sup_{0\leq s\leq 1}B_s$, and define $\tau=\inf\{t\geq 0:B_t=M_1\}$. Then $\tau<1\ a.s.$ and $\tau$ is not a stopping time. The density of $\tau$ is given by the following \textit{arcsine law}:
\begin{align*}
	\rho_\tau(t)=\frac{1}{\pi\sqrt{t(1-t)}}\mathds{1}_{(0,1)}(t).
\end{align*}
The results holds true if we replace $\tau$ by $\ell=\sup\left\{t\in[0,1]:B_t=0\right\}$.
\begin{proof}
Define $\widetilde{B}_t=B_1-B_{1-t}$ for all $t\in[0,1]$, which is a Brownian motion on $[0,1]$. Then for all $\epsilon>0$,
\begin{align*}
	B_1-M_1\leq B_1-\sup_{1-\epsilon\leq s\leq 1}B_s=\inf_{0\leq s\leq\epsilon}\widetilde{B}_s < 0\ a.s..
\end{align*}
Clearly, we have $0\leq\tau\leq 1$, and $\{\tau=1\}\subset\{B_1=M_1\}$. Hence $\P(\tau<1)=1$. 

Fix $0<t<1$, and let $M_t=\sup_{0\leq s\leq t}B_s$. By simple Markov property, $(B_{t+s}-B_t)_{s\in[0,1-t]}$ is independent of $(B_s)_{s\in[0,t]}$. Define $N_t=\sup_{0\leq s\leq 1-t}(B_{t+s}-B_t)$. Then $N_t$ is independent of $(M_t,B_t,M_t-B_t)$, and
\begin{align*}
	\P(\tau\leq t)=\P(M_t\geq B_t+N_t)=\P(N_t\leq M_t-B_t)=\P(\vert B_1-B_t\vert\leq\vert B_t\vert).
\end{align*}
Let $Z_1,Z_2$ be $N(0,1)\ i.i.d.$, and $\theta$ is uniformly distributed on $[0,2\pi)$. By calculus,
\begin{align*}
	\P(\tau\leq t)=\P\left(\sqrt{1-t}\vert Z_1\vert\leq\sqrt{t}\vert Z_2\vert\right)=\P\left(\frac{\vert Z_1\vert}{\sqrt{Z_1^2+Z_2^2}}\leq t\right)=\P\left(\vert\sin\theta\vert\leq\sqrt{t}\right)=\frac{2}{\pi}\arcsin(\sqrt{t}).
\end{align*}
The density of $\tau$ follows by finding the derivative of $\frac{2}{\pi}\arcsin(\sqrt{t})$. The case for $\ell$ is rather straightforward:
\begin{align*}
\P(\ell\leq t) &= 2\int_0^\infty\frac{1}{\sqrt{2\pi t}}\e^{-\frac{y^2}{2t}}\P\left(\sup_{s\in[0,1-t]}(B_t-B_{t+s})\geq y\right)\,\d y=2\int_0^\infty\frac{1}{\sqrt{2\pi t}}\e^{-\frac{y^2}{2t}}\P\left(\tau_y\geq 1-t\right)\,\d y\\
&=2\int_0^\infty\frac{1}{\sqrt{2\pi t}}\e^{-\frac{y^2}{2t}}\left(\int_{1-t}^\infty\frac{y}{\sqrt{2\pi z^3}}\e^{-\frac{y^2}{2z}}\d z\right)\d y = \frac{1}{\pi}\int_{1-t}^\infty\left(\int_0^\infty\frac{y}{\sqrt{tz^3}}\e^{-\frac{y^2}{2t}-\frac{y^2}{2z}}\d y\right)\d z\\
&=\frac{1}{\pi}\int_{1-t}^\infty\frac{1}{\sqrt{tz^3}}\frac{tz}{t+z}\,\d z \overset{s=\frac{t}{t+z}}{=}\frac{1}{\pi}\int_0^t\frac{s^{3/2}}{t\sqrt{1-s}}\frac{\partial z}{\partial s}\,\d s = \frac{1}{\pi}\int_0^t\frac{1}{\sqrt{s(1-s)}}\,\d s=\frac{2}{\pi}\arcsin(\sqrt{t}).
\end{align*}
Hence we complete the proof.
\end{proof}

\section{Stochastic Integration}
In this chapter, our discussion is based on a probability space $(\Omega,\mathscr{F},\P)$ and a complete filtration $(\mathscr{F}_t)_{t\geq 0}$. All processes we study are indexed by $\mathbb{R}_+$ and take real values. 
\subsection{Construction of Stochastic Integrals}
\paragraph{Preliminary: Space $\mathbb{H}^2$.} Given a filtered probability space $(\Omega,\mathscr{F},(\mathscr{F}_t)_{t\geq 0},\P)$, we denote by $\mathcal{H}^2$ the vector space of all sample-continuous martingales $M=(M_t)_{t\geq 0}$ that are bounded in $L^2$ [i.e. $\sup_{t\geq 0}\E[M_t^2]<\infty$] with $M_0=0$, and we write $M\sim N$ if $M,N\in\mathcal{H}^2$ are indistinguishable. Then we define $\mathbb{H}^2=\mathcal{H}^2/\sim$, and for brevity we write $[M]=M$ for all $M\in\mathcal{H}^2$. By \hyperref[thm:3.58]{Theorem 3.58}, a continuous local martingale falls in $\mathbb{H}^2$ if and only if $M_0=0$ and $\E\left[\langle M,M\rangle_\infty\right]<\infty$. Consequently, the martingale $M=(M_t)_{t\geq 0}$ has $a.s.$ and $L^2$ limit $X_\infty$ such that $\E[M_\infty|\mathscr{F}_t]=M_t$ for all $t\in\mathbb{R}_+$.

If $M,N\in\mathbb{H}^2$, the bracket $\langle M,N\rangle=\frac{1}{2}(\langle M+N,M+N\rangle-\langle M,M\rangle-\langle N,N\rangle)$ then satisfies $\E[\vert\langle M,N\rangle_\infty\vert]<\infty$. This gives rise to a bilinear form:
\begin{align*}
	\langle M,N\rangle_{\mathbb{H}^2}=\E\left[\langle M,N\rangle_\infty\right]=\E[M_\infty N_\infty]\quad\Rightarrow\quad \Vert M\Vert_{\mathbb{H}^2}=\E[\langle M,M\rangle_\infty]=\E[M_\infty^2].
\end{align*}

One can easily show that $\langle\cdot,\cdot\rangle_{\mathbb{H}^2}$ forms an inner product on $\mathbb{H}^2$, of which positive definiteness follows from \hyperref[prop:3.57]{Proposition 3.57}. Furthermore, $(\mathbb{H}^2,\langle\cdot,\cdot\rangle_{\mathbb{H}^2})$ is a Hilbert space.

\begin{proof}[Proof of completeness]
Let $M^{(n)}\in\mathbb{H}^2$ be a Cauchy sequence with respect to the norm $\Vert\cdot\Vert_{\mathbb{H}^2}$. Then
\begin{align*}
	\lim_{n,m\to\infty}\E\left[\left(M^{(n)}_\infty-M^{(m)}_\infty\right)^2\right] = \lim_{n,m\to\infty}\left\Vert M^{(n)}-M^{(m)}\right\Vert_{\mathbb{H}^2}^2=0.
\end{align*}
Then $M_\infty^{(n)}$ is a Cauchy sequence in $L^2$, and we denote $Z=\lim_{n\to\infty}M_\infty^{(n)}$ in $L^2$. On the other hand, the Doob's $L^2$-inequality [\hyperref[prop:3.40]{Proposition 3.40 (ii)}] and an argument of dominated convergence theorem imply
\begin{align*}
	\E\left[\sup_{t\geq 0}\left\vert M^{(n)}_t-M^{(m)}_t\right\vert^2\right]\leq 4\E\left[\left(M^{(n)}_\infty-M^{(m)}_\infty\right)^2\right].
\end{align*}
Hence $(M_t^{(n)})_{n=1}^\infty$ is a Cauchy sequence for every $t\geq 0$, which converges in $L^2$. To conclude the proof, it suffices to show that the limit process is in $\mathbb{H}^2$. We choose a subsequence $M^{(n_k)}$ such that 
\begin{align*}
	\E\left[\sup_{t\geq 0}\left\vert M^{(n_{k+1})}_t-M^{(n_k)}_t\right\vert^2\right]<2^{-k}\quad\Rightarrow\quad \E\left[\sup_{t\geq 0}\left\vert M^{(n_{k+1})}_t-M^{(n_k)}_t\right\vert\right]\leq 2^{-k/2}.
\end{align*}

Consequently, we have $\sum_{k=1}^\infty\sup_{t\geq 0}\vert M^{(n_{k+1})}_t-M^{(n_k)}_t\vert < \infty\ a.s.$. By Weierstrass M-test, the limit process $M_t=\lim_{k\to\infty} M_t^{(n_k)}$ is a $a.s.$ uniform limit on $\mathbb{R}_+$, hence has continuous sample paths. On the zero probability set where the uniform convergence does not
hold, we take $M_t=0$ for each $t>0$. By completeness of the filtration $(\mathscr{F}_t)_{t\geq 0}$, the process $M=(M_t)_{t\geq 0}$ is adapted. Also, the continuity of conditional expectation passes $M_t^{(n_k)}=\E[M_\infty^{(n_k)}|\mathscr{F}_t]$ to $M_t=\E[Z|\mathscr{F}_t]$ as $k\to\infty$, hence $(M_t)_{t\geq 0}$ is a uniformly integrable martingale, which converges to $M_\infty\ a.s.$ and in $L^2$. By uniform convergence, we have $M_\infty = Z\ a.s.$. Therefore
\begin{align*}
	\lim_{n\to\infty}\left\Vert M^{(n)}-M\right\Vert_{\mathbb{H}^2}^2 = \lim_{n\to\infty}\E\left[\left(M^{(n)}_\infty-M_\infty\right)^2\right] = \lim_{n\to\infty}\E\left[\left(M^{(n)}_\infty-Z_\infty\right)^2\right]=0.
\end{align*}
Thus $M\in\mathbb{H}^2$ is indeed the limit of sequence $M^{(n)}$, completing the proof.
\end{proof}

\paragraph{Preliminary: Progressive $\sigma$-fields.} Given $(\Omega,\mathscr{F},\P)$, we define the progressive $\sigma$-field on $\Omega\times\mathbb{R}_+$ as
\begin{align*}
	\mathscr{P}=\left\{A\subset\Omega\times\mathbb{R}_+: A\cap(\Omega\times[0,t])\in\mathscr{F}\otimes\mathscr{B}([0,t]),\ \forall t\in\mathbb{R}_+\right\}\subset\mathscr{F}\otimes\mathscr{B}(\mathbb{R}_+),
\end{align*}
where the inclusion holds since $A=\bigcup_{n=1}^\infty (A\cap(\Omega\times[0,n]))$. Clearly, if $A\in\mathscr{P}$, the process $X_t(\omega)=\mathds{1}_A(\omega,t)$ is a progressive process. Furthermore, one can verify that $\mathscr{P}$ is indeed a $\sigma$-algebra on $\Omega\times\mathbb{R}_+$, and a process $(X_t)_{t\geq 0}$ is progressive if and only if the mapping $(\omega,t)\mapsto X_t(\omega)$ is $\mathscr{P}$-measurable.

\paragraph{Preliminary: Space $L^2(M)$.} Given a process $M\in\mathbb{H}^2$, the \hyperref[thm:3.56]{Theorem 3.56} determines to an increasing process $(\langle M,M\rangle_s)_{s\geq 0}$, which is called the quadratic variation of $M$. Then for every $A\in\mathscr{P}$, one can define
\begin{align*}
	\mu_M(A)=\E\left[\int_0^\infty\mathds{1}_A(\cdot,s)\,\d \langle M,M\rangle_s\right].
\end{align*}
This is a measure on $(\Omega\times\mathbb{R}_+,\mathscr{P})$. We denote by $\mathcal{L}^2(M)$ the space of all progressive progresses $H$ such that
\begin{align*}
	\Vert H\Vert_{L^2(M)}^2=\E\left[\int_0^\infty H_s^2\,\d \langle M,M\rangle_s\right]<\infty,
\end{align*}
and choose the quotient space $L^2(M)$ that makes $\Vert\cdot\Vert_{L^2(M)}$ a proper norm. Then $L^2(M)=L^2(\Omega\times\mathbb{R}_+,\mathscr{P},\mu_M)$ can be viewed as an ordinary $L^2$-space, and we can define the inner product
\begin{align*}
	\langle H,K\rangle_{L^2(M)}=\E\left[\int_0^\infty H_sK_s\,\d \langle M,M\rangle_s\right]
\end{align*}

\paragraph{Remark.} Recall that $X_t^\tau = X_{t\wedge \tau}$ is the stopped process associated with a stopping time $\tau$. If $M\in\mathbb{H}^2$, then we have $\langle M^\tau,M^\tau\rangle_\infty = \langle M,M\rangle_\tau$, which implies that $M^\tau\in\mathbb{H}^2$. Furthermore, if $H\in L^2(M)$, the process $\mathds{1}_{[0,\tau]}H$ defined by $(\mathds{1}_{[0,\tau]}H)_s(\omega)=\mathds{1}_{[0,\tau(\omega)]}(s)H_s(\omega)$ also belongs to $L^2(M)$. Note that $\mathds{1}_{[0,\tau]}H$ is progressive since it has left-continuous sample paths.

\paragraph{Definition 5.1\label{def:5.1}} (Elementary processes). An \textit{elementary process} is a progressive process of the form
\begin{align*}
	H_s(\omega) = \sum_{j=1}^n H_{(j)}(\omega)\mathds{1}_{(t_{j-1},t_j]}(s),
\end{align*}
where $0=t_0<t_1<\cdots<t_n$, and $H_{(j)}$ is a \uwave{bounded} $\mathscr{F}_{t_{j-1}}$-measurable random variable for all $j\in\{1,\cdots,n\}$. Clearly, the set $\mathscr{E}$ of all (equivalence classes of) elementary processes is a subspace of $L^2(M)$.

\paragraph{Proposition 5.2.\label{prop:5.2}} For every $M\in\mathbb{H}^2$, $\mathscr{E}$ is dense in $L^2(M)$.
\begin{proof}
Fix $M\in\mathbb{H}^2$. By elementary Hilbert space theory, it suffices to show that $L^2(M)\ni K\perp\mathscr{E}$ implies $K=0$. Assume that $K\in L^2(M)$ is orthogonal to $\mathscr{E}$, and set
\begin{align*}
	X_t = \int_0^t K_s\,\d \langle M,M\rangle_s,\quad \forall t\geq 0.
\end{align*}
According to \hyperref[prop:3.52]{Proposition 3.52}, since
\begin{align*}
	\E\left[\int_0^t\vert K_s\vert\left\vert\d\langle M,M\rangle_s\right\vert\right] &\leq \left(\E\left[\int_0^t K_s^2\,\d \langle M,M\rangle_s\right]\right)^{1/2}\left(\E\left[\int_0^t d\langle M,M\rangle_s\right]\right)^{1/2} \leq \left\Vert K\right\Vert_{L^2(M)}\Vert M\Vert_{\mathbb{H}^2},
\end{align*}
the process $(X_t)_{t\geq 0}$ is a finite-variation process. In addition, it is bounded in $L^1$.

Now we prove that $(X_t)_{t\geq 0}$ is a continuous martingale. Given $0\leq s<t$, let $H_r=Y\mathds{1}_{(s,t]}$, where $Y$ is a bounded $\mathscr{F}_s$-measurable random variable. Then
\begin{align*}
	0=\langle H,K\rangle_{L^2(M)}=\E\left[Y\int_s^t K_u\,\d \langle M,M\rangle_u\right] = \E[Y(X_t-X_s)]
\end{align*}
Since $\E[Y(X_t-X_s)]=0$ for all bounded $\mathscr{F}_s$-measurable random variable $Y$, we have $\E[X_t-X_s|\mathscr{F}_s]=0$. Note that $X=(X_t)_{t\geq 0}$ is adapted, and by definition it has continuous sample paths. Hence $X$ is a continuous martingale. By \hyperref[prop:3.55]{Proposition 3.55}, we have $X=0\ a.s.$, i.e.
\begin{align*}
	\int_0^t K_s\,\d \langle M,M\rangle_s = 0\ \ \forall t\geq 0,\ \  a.s.\quad\Rightarrow\quad K_s=0\ \ d\langle M,M\rangle_s\textit{-}a.e.,\ \ a.s..
\end{align*}
Therefore $\Vert K\Vert_{L^2(M)}=0$, and the result follows.
\end{proof}

\paragraph{Theorem 5.3\label{thm:5.3}} (Stochastic integrals for $L^2$-bounded martingales). Let $M\in\mathbb{H}^2$. For every elementary process $H\in\mathscr{E}$, we define the following formula:
\begin{align*}
	H_s=\sum_{j=1}^nH_{(j)}\mathds{1}_{(t_{j-1},t_j]}(s)\quad\Rightarrow\quad
	(H\cdot M)_t = \sum_{j=1}^n H_{(j)}\left(M_{t_j\wedge t} - M_{t_{j-1}\wedge t}\right)
\end{align*}
This defines a process $H\cdot M\in\mathbb{H}^2$, and the mapping $H\mapsto H\cdot M$ extends to an isometry from $L^2(M)$ into $\mathbb{H}^2$. Furthermore, $H\cdot M$ is the unique martingale of $\mathbb{H}^2$ that satisfies the property
\begin{align*}
	\langle H\cdot M,N\rangle = H\cdot\langle M,N\rangle,\quad \forall N\in\mathbb{H}^2,\tag{5.1}\label{eq:5.1}
\end{align*}
where the quantity $H\cdot\langle M,N\rangle$ in the right-hand side is the integral with respect to a finite variation process. If $\tau$ is a stopping time, we then have
\begin{align*}
	(\mathds{1}_{[0,\tau]}H)\cdot M = (H\cdot M)^\tau = H\cdot M^\tau.\tag{5.2}\label{eq:5.2}
\end{align*}
The process $H\cdot M$ is called the \textit{stochastic integral of $H$ with respect to $M$}.
\begin{proof}
Since the process $H\cdot M$ does not depends on the choice of partition when $H$ is given, it is easy to see that $H\mapsto H\cdot M$ is a linear mapping. Then we verify that $H\mapsto H\cdot M$ is an isometry from $\mathscr{E}$ into $L^2(M)$.

We fix the process $H=(H_s)_{s\geq 0}$ of the form given in the theorem. For every $j\in\{1,\cdots,n\}$, define $M_t^{(j)}=H_{(j)}(M_{t_j\wedge t}-M_{t_{j-1}\wedge t})$ for all $t\geq 0$. Akin to our proof of \hyperref[thm:3.56]{Theorem 3.56} at Step II, the process $(M_t^{(j)})_{t\geq 0}$ is a continuous martingale, and it belongs to $\mathbb{H}^2$. Hence $H\cdot M=\sum_{j=1}^n M^{(j)}$ also belongs to $\mathbb{H}^2$. Moreover, note that
\begin{align*}
	\left\langle M^{(j)},M^{(j)}\right\rangle_t &=\sum_{j=1}^n H_{(j)}^2\left(\langle M,M\rangle_{t_j\wedge t}-\langle M,M\rangle_{t_{j-1}\wedge t}\right),\tag{By the approximation formula}\\
	\left\langle M^{(j)},M^{(k)}\right\rangle_{\mathbb{H}^2} &= \E\left[M_\infty^{(j)}M_\infty^{(k)}\right] = \E\left[H_{(j)}H_{(k)}(M_{t_j}-M_{t_{j-1}})(M_{t_k}-M_{t_{k-1}})\right]\\
	&= \E\left[\E\left[H_{(j)}H_{(k)}(M_{t_j}-M_{t_{j-1}})(M_{t_k}-M_{t_{k-1}})|\mathscr{F}_{t_{j-1}}\right]\right] = 0,\quad \forall 1\leq j<k\leq n.
\end{align*}
By orthogonality of $M^{(j)}$'s and bilinearity of quadratic variation, it holds
\begin{align*}
	\left\langle H\cdot M,H\cdot M\right\rangle_t = \sum_{j=1}^n H_{(j)}^2\left(\langle M,M\rangle_{t_j\wedge t} -\langle M,M\rangle_{t_{j-1}\wedge t}\right) = \int_0^t H_s^2\,\d \langle M,M\rangle_s.
\end{align*}
Consequently, we have
\begin{align*}
	\Vert H\cdot M\Vert_{\mathbb{H}^2}^2=\E\left[\langle H\cdot M,H\cdot M\rangle_\infty\right] = \E\left[\int_0^\infty H_s^2\,\d \langle M,M\rangle_s\right] = \Vert H\Vert_{L^2(M)}^2,\quad \forall t\geq 0.
\end{align*}
By linearity, if $H=H^\prime$ in $L^2(M)$, then $H\cdot M=H^\prime\cdot M$ in $\mathbb{H}^2$. Therefore the mapping $\mathscr{E}\to\mathbb{H}^2$ is well-defined. Since it is norm-preserving and linear, it is an isometry. By \hyperref[prop:5.2]{Proposition 5.2} and the fact that $\mathbb{H}^2$ is complete, we can uniquely extend this mapping to an isometry from $L^2(M)$ into $\mathbb{H}^2$.

Now we verify the property \hyperref[eq:5.1]{(5.1)}. We fix $N\in\mathbb{H}^2$. The Kunita-Watanabe inequality [\hyperref[thm:3.62]{Theorem 3.62}] implies
\begin{align*}
	\E\left[\int_0^\infty\vert H_s\vert\,\vert \d\langle M,N\rangle_s\vert\right]\leq\Vert H\Vert_{L^2(M)}\Vert N\Vert_{\mathbb{H}^2} < \infty,\quad\forall H\in L^2(M).
\end{align*}
Then the variable $\left(H\cdot\langle M,N\rangle\right)_\infty = \int_0^\infty H_s\,\d \langle M,N\rangle_s$ is well-defined and in $L^1$. For the case where $H$ is an elementary process, we have
\begin{align*}
	\begin{cases}
		\langle H\cdot M,N\rangle = \sum_{j=1}^n\langle M^{(j)},N\rangle\\
		\langle M^{(j)},N\rangle_t = H_{(j)}^2\left(\langle M,N\rangle_{t_j\wedge t}-\langle M,N\rangle_{t_{j-1}\wedge t}\right),\quad \forall t\geq 0.
	\end{cases}
\end{align*}
This gives \hyperref[eq:5.1]{(5.1)} in the case $H\in\mathscr{E}$:
\begin{align*}
	\langle H\cdot M,N\rangle_t=\sum_{j=1}^nH_{(j)}^2\left(\langle M,N\rangle_{t_j\wedge t}-\langle M,N\rangle_{t_{j-1}\wedge t}\right) = \int_0^t H_s\,\d \langle M,N\rangle_s=(H\cdot\langle M,N\rangle)_t,\quad\forall t\geq 0.
\end{align*}

To prove the general case where $H\in L^2(M)$, note the continuity of the linear mapping $X\mapsto\langle X,N\rangle_\infty$ from $\mathbb{H}^2$ into $L^1(\Omega,\mathscr{F},\P)$:
\begin{align*}
	\E\left[\vert\langle X,N\rangle_\infty\vert\right]=\E\left[\langle X,X\rangle_\infty\right]^{1/2}\E\left[\langle N,N\rangle_\infty\right]^{1/2}=\Vert X\Vert_{\mathbb{H}^2}\Vert N\Vert_{\mathbb{H}^2}.
\end{align*}
Let $H^{(n)}\in\mathscr{E}$ be a sequence that converges to $H$ in $L^2(M)$. Then $H^{(n)}\cdot M\to H\cdot M$ in $\mathbb{H}^2$, and
\begin{align*}
	\langle H\cdot M,N\rangle_\infty = \lim_{n\to\infty}\left\langle H^{(n)}\cdot M,N\right\rangle_\infty = \lim_{n\to\infty}\left(H^{(n)}\cdot\left\langle M,N\right\rangle\right)_\infty = (H\cdot\langle M,N\rangle)_\infty,
\end{align*}
where the last equality holds in $L^1$ by Kunita-Watanabe:
\begin{align*}
	\E\left[\left\vert\int_0^\infty(H^{(n)}_s-H_s)\,\d \langle M,N\rangle_s\right\vert\right]\leq\left\Vert H_s^{(n)}-H_s\right\Vert_{L^2(M)}\left\Vert N\right\Vert_{\mathbb{H}^2}.
\end{align*}
Hence we have $\langle H\cdot M,N\rangle_\infty=(H\cdot\langle M,N\rangle)_\infty$. By replacing $N$ with the stopped martingale $N^t$ for any $t\geq 0$, one obtain $\langle H\cdot M,N\rangle_t = (H\cdot\langle M,N\rangle)_t$. For uniqueness, let $X\in\mathbb{H}^2$ satisfy \hyperref[eq:5.1]{(5.1)}. Then $\langle H\cdot M-X,N\rangle= 0$ for all $N\in\mathbb{H}^2$, which implies $\langle H\cdot M-X,H\cdot M-X\rangle=0$. By \hyperref[prop:3.57]{Proposition 3.57 (ii)}, we have $H\cdot M-X=0\ a.s.$.

Finally it remains to show \hyperref[eq:5.2]{(5.2)}. By \hyperref[prop:3.61]{Proposition 3.61 (iv)}, for all $N\in\mathbb{H}^2$, we have
\begin{align*}
	\langle (H\cdot M)^\tau,N\rangle_t = \langle H\cdot M,N\rangle_{t\wedge\tau} = \left(H\cdot\langle M,N\rangle\right)_{t\wedge\tau}=\left(\mathds{1}_{[0,\tau]}H\cdot\langle M,N\rangle\right)_t,
\end{align*}
which implies the first inequality. The proof for the second one is similar:
\begin{align*}
	\langle H\cdot M^\tau,N\rangle = H\cdot\langle M^\tau,N\rangle = H\cdot\langle M,N\rangle^\tau = \mathds{1}_{[0,\tau]}H\cdot \langle M,N\rangle.
\end{align*}
Note that the property \hyperref[eq:5.1]{(5.1)} can be used as an alternative definition of the stochastic integral $H\cdot M$.
\end{proof}

\paragraph{Remark.} We use the following notation for a stochastic integral:
\begin{align*}
	\int_0^t H_s\,\d M_s = (H\cdot M)_t,\ \ \forall t\geq 0,\quad\textit{and}\quad \int_0^\infty H_s\,\d M_s = (H\cdot M)_\infty.
\end{align*}
The property \hyperref[eq:5.1]{(5.1)} gives commutativity of stochastic integral and bracket:
\begin{align*}
	\left\langle \int_0^\cdot H_s\,\d M_s, N\right\rangle_t = \int_0^t H_s\,\d \langle M,N\rangle_s
\end{align*}
The following proposition concerns about associativity.

\paragraph{Proposition 5.4\label{prop:5.4}} (Associativity). Let $K=(K_s)_{s\geq 0}$ and $H=(H_s)_{s\geq 0}$ be two progressive progresses.
\begin{itemize}
	\item[(i)] Let $A=(A_s)_{s\geq 0}$ be a finite variation process, and $\int_0^\infty\vert H_s\vert\,\vert\d A_s\vert<\infty\ a.s.$. If $\int_0^\infty\vert K_sH_s\vert\,\vert\d A_s\vert<\infty\ a.s.$, then $(KH)\cdot A = K\cdot(H\cdot A)$.
	\item[(ii)] Let $M\in\mathbb{H}^2$, and $H\in L^2(M)$. Then $KH\in L^2(M)$ if and only if $K\in L^2(H\cdot M)$. In this case, we have $(KH)\cdot M = K\cdot(H\cdot M)$.
\end{itemize}
\begin{proof}
The statement (i) follows from an analogous deterministic result. Using the property \hyperref[eq:5.1]{(5.1)} twice and (i) gives $\langle H\cdot M,H\cdot M\rangle = H^2\cdot\langle M,M\rangle$, and $K^2\cdot\langle H\cdot M,H\cdot M\rangle = K^2H^2\cdot\langle M,M\rangle$. Then the first assertion of (ii) follows from a monotone convergence argument:
\begin{align*}
	\E\left[\left(K^2H^2\cdot\langle M,M\rangle\right)_\infty\right] = \E\left[\left(K^2\cdot\langle H\cdot M,H\cdot M\rangle\right)_\infty\right]
\end{align*}
For the second assertion, note that
\begin{align*}
	\langle (KH)\cdot M,N\rangle = KH\cdot \langle M,N\rangle = K\cdot(H\cdot\langle M,N\rangle) = K\cdot\langle H\cdot M,N\rangle,\quad\forall N\in\mathbb{H}^2.
\end{align*}
The result immediately follows from the uniqueness argument in \hyperref[thm:5.3]{Theorem 5.3}.
\end{proof}
\paragraph{Remark.} Let $M,N\in\mathbb{H}^2$, $H\in L^2(M)$ and $K\in L^2(N)$. Using \hyperref[eq:5.1]{(5.1)} and (i) gives a more general result:
\begin{align*}
	\left\langle H\cdot M, K\cdot N\right\rangle_t = \left\langle \int_0^\cdot H_s\,\d M_s, \int_0^\cdot K_s\,\d N_s\right\rangle_t = \int_0^t H_sK_s\,\d \langle M,N\rangle_s.
\end{align*}
According to \hyperref[prop:3.61]{Proposition 3.61 (vi)}, we have
\begin{align*}
	\E\left[\left(\int_0^t H_s\,\d M_s\right)\left(\int_0^t K_s\,\d N_s\right)\right]=\E\left[\int_0^t H_sK_s\,\d \langle M,N\rangle_s\right].
\end{align*}
Note that $H\cdot M = \int_0^\cdot H_s\,\d M_s$ is a martingale of $\mathbb{H}^2$. For all $0\leq s < t\leq \infty$, we have
\begin{align*}
	\E\left[\int_0^t H_s\,\d M_s\right] = 0,\quad \textit{and}\quad \E\left[\left.\int_0^t H_u\,\d M_u\right|\mathscr{F}_s\right]=\int_0^s H_u\,\d M_u.
\end{align*}
According to \hyperref[thm:3.58]{Theorem 3.58}, the second moment of the stochastic integral is given by
\begin{align*}
	\E\left[\left(\int_0^t H_s\,\d M_s\right)^2\right]=\E\left[\int_0^t H_s^2\,\d \langle M,M\rangle_s\right].
\end{align*}
Next we discuss stochastic integrals for local martingales.
\newpage
\paragraph{Preliminaries: Space $L^2_{\mathrm{loc}}(M)$.}  Let $M$ be a continuous local martingale. Similar to the case $M\in\mathbb{H}^2$, we can define a Hilbert space $L^2(M)$ associated with $M$ containing all progressive processes $H$ such that $\E\left[\int_0^\infty H_s^2\,\d \langle M,M\rangle_s\right]<\infty$. Furthermore, we denote by $L^2_{\mathrm{loc}}(M)$ the set of all progressive processes such that
\begin{align*}
	\int_0^t H_s^2\,\d \langle M,M\rangle_s <\infty,\ \ \forall t\geq 0,\ \ \textit{a.s.}.
\end{align*}
Clearly, $L^2(M)$ is a subspace of $L^2_{\mathrm{loc}}(M)$.
\paragraph{Theorem 5.5\label{thm:5.5}} (Stochastic integrals for continuous local martingales). Let $M$ be a continuous local martingale. For every $H\in L^2_{\mathrm{loc}}(M)$, there exists a unique continuous local martingale starting from $0$, denoted by $H\cdot M$, such that for every continuous local martingale $N$,
\begin{align*}
	\langle H\cdot M,N\rangle = H\cdot\langle M,N\rangle.\tag{5.3}\label{eq:5.3}
\end{align*}
If $\tau$ is a stopping time, we then have
\begin{align*}
	(\mathds{1}_{[0,\tau]}H)\cdot M = (H\cdot M)^\tau = H\cdot M^\tau.\tag{5.4}\label{eq:5.4}
\end{align*}
In addition, if $K=(K_s)_{s\geq 0}$ is a progressive process, then $KH\in L^2_{\mathrm{loc}}(M)$ if and only if $K\in L^2_{\mathrm{loc}}(H\cdot M)$. In this case, we have $(KH)\cdot M = K\cdot(H\cdot M)$.
\begin{proof}
Without loss of generality, we assume that $M_0=0$, since we can replace $(M_t)_{t\geq 0}$ by $(M_t-M_0)_{t\geq 0}$. We also assume that the property $\int_0^sH_s^2\,\d \langle M,M\rangle_s<\infty$ for all $t\geq 0$ holds for all $\omega\in\Omega$ by resetting $H=0$ on a negligible set if required. For all $n\in\mathbb{N}$, we choose a sequence of stopping times $\tau_n$ increasing to $\infty$ as follows:
\begin{align*}
	\tau_n=\inf\left\{t\geq 0:\int_0^t (1+H_s^2)\,\d \langle M,M\rangle_s\geq n\right\}.
\end{align*}
By definition, $\langle M^{\tau_n},M^{\tau_n}\rangle_t=\langle M,M\rangle_{t\wedge\tau_n}\leq n$, hence the stopped martingale $M^{\tau_n}$ belongs to $\mathbb{H}^2$. Furthermore,
\begin{align*}
	\int_0^\infty H_s^2\,\d \langle M^{\tau_n},M^{\tau_n}\rangle_s = \int_0^{\tau_n}H_s^2\,\d \langle M,M\rangle_s \leq n
\end{align*}
Hence $H\in L^2(M^{\tau_n})$, and the definition of $H\cdot M^{\tau_n}$ make sense by \hyperref[thm:5.3]{Theorem 5.3}. Note that for all $n>m\geq 1$, the property \hyperref[eq:5.2]{(5.2)} implies $(H\cdot M^{\tau_n})^{\tau_m}=H\cdot M^{\tau_m}$. Let $(H\cdot M)_t=\lim_{n\to\infty} (H\cdot M^{\tau_n})_t$ for every $t\geq 0$, where the limit exists for all $\omega\in\Omega$ (we find $m$ with $\tau_{m}(\omega)\geq t$, then $(H\cdot M^{\tau_{n}})_t(\omega)=(H\cdot M^{\tau_{m}})_t(\omega)$ for all $n\geq m$).
Then $H\cdot M$ is an adapted process, and $(H\cdot M)^{\tau_n}=\lim_{m\to\infty}(H\cdot M^{\tau_m})^{\tau_n}= H\cdot M^{\tau_n}\in\mathbb{H}^2$. Consequently, $H\cdot M$ has continuous sample paths, and is a continuous local martingale.

Now we verify \hyperref[eq:5.3]{(5.3)}. Let $N$ be a continuous local martingale with $N_0=0$, and choose stopping times $\tau_n^\prime=\inf\{t\geq 0:\vert N_t\vert\geq n\}$, $\sigma_n=\tau_n\wedge\tau_n^\prime$. Then $N^{\tau_n^\prime}\in\mathbb{H}^2$. Note that $M^{\tau_n}\in\mathbb{H}^2$, and $H\in L^2(M^{\tau_n})$. Hence
\begin{align*}
	\langle H\cdot M,N\rangle^{\sigma_n} = \langle(H\cdot M)^{\tau_n},N^{\tau_n^\prime}\rangle = \langle H\cdot M^{\tau_n},N^{\tau_n^\prime}\rangle = H\cdot\langle M^{\tau_n},N^{\tau_n^\prime}\rangle = H\cdot \langle M,N\rangle^{\sigma_n} = (H\cdot\langle M,N\rangle)^{\sigma_n}.
\end{align*}
Since $\sigma_n\to\infty$ as $n\to\infty$, the equality \hyperref[eq:5.3]{(5.3)} follows. The uniqueness of $H\cdot M$ follows from a similar argument presented in the proof of \hyperref[thm:5.3]{Theorem 5.3}. The equality \hyperref[eq:5.4]{(5.4)} is a consequence of \hyperref[eq:5.3]{(5.3)}, as is shown in the proof of \hyperref[thm:5.3]{Theorem 5.3}. Finally, the associativity follows in an analogous way in \hyperref[prop:5.4]{Proposition 5.4}.
\end{proof}

\paragraph{Remark: Consistency of two definitions.} If $M\in\mathbb{H}^2$ and $H\in L^2(M)$, the two definitions of $H\cdot M$ given in Theorems \hyperref[thm:5.3]{5.3} and \hyperref[thm:5.5]{5.5} coincide. To see this, note that the definition of $H\cdot M$ in \hyperref[thm:5.5]{Theorem 5.5} satisfies $H\cdot M\in\mathbb{H}^2$. This is a consequence of the property \hyperref[eq:5.3]{(5.3)}, which gives $\langle H\cdot M,H\cdot M\rangle=H^2\cdot\langle M,M\rangle$.

\paragraph{Remark: Connection to Wiener's integral.} If $B=(B_t)_{t\geq 0}$ is a Brownian motion, we fix the filtration $\mathscr{F}_t=\sigma(B_s,0\leq s\leq t)$ so that $B$ is a continuous martingale. According to the Remark of \hyperref[prop:4.4]{Proposition 4.4}, for each $h\in L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$, we can define the Wiener integral $\int_0^t h(s)\,\d B_s = W(h\mathds{1}_{[0,t]})$, where $W$ is the Gaussian white noise associated with $B$. This definition coincides with the stochastic integral $(h\cdot B)_t$, where we view $h$ as a (deterministic) progressive process. For all step functions $h=\sum_{j=1}^n\lambda_j\mathds{1}_{(t_{j-1},t_j]}$, we have
\begin{align*}
	\int_0^t h(s)\,\d B_s = W(h\mathds{1}_{[0,t]}) = \sum_{j=1}^n \lambda_j\left(B_{t_j\wedge t} - B_{t_{j-1}\wedge t}\right).
\end{align*}
Then for all continuous local martingales $N$, we have
\begin{align*}
	\left\langle \int_0^\cdot h(s)\,\d B_s,N\right\rangle_t = \sum_{j=1}^n\lambda_j(\langle B_{t_j\wedge\cdot},N\rangle_t - \langle B_{t_{j-1}\wedge\cdot},N\rangle_t) = \sum_{j=1}^n\lambda_j(\langle B,N\rangle_{t_j\wedge t} - \langle B,N\rangle_{t_{j-1}\wedge t}) = (h\cdot\langle B,N\rangle)_t.
\end{align*}
Therefore we have $\int_0^t h(s)\,\d B_s=(h\cdot B)_t$ for all step functions $h$. For the general case $h\in L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$, the quadratic variation $\langle B,B\rangle_t=t$ implies $h\in L^2(B)$, and the result follows from a density argument.

\paragraph{Remark: Moment formulae.} In the setting of \hyperref[thm:5.5]{Theorem 5.5}, we again write $\int_0^t H_s\,\d M_s=(H\cdot M)_t$. If $M,N$ are continuous local martingales, $H\in L^2_{\mathrm{loc}}(M)$ and $K\in L^2_{\mathrm{loc}}(N)$, the first formula  in the Remark under \hyperref[prop:5.4]{Proposition 5.4} still holds true for $\langle H\cdot M,K\cdot N\rangle$:
\begin{align*}
	\langle H\cdot M,K\cdot N\rangle_t = \int_0^t H_sK_s\,\d \langle M,N\rangle_s,\quad\forall t\geq 0;
\end{align*}
whereas the formulae for moments of $\int_0^t H_s\,\d M_s$ may fail.

We can make an extension. For a continuous local martingale $M$ and a progressive process $H\in L^2_{\mathrm{loc}}(M)$, and for some fixed $t>0$, assume the following condition holds:
\begin{align*}
	\E\left[\int_0^t H_s^2\,\d \langle M,M\rangle_s\right]<\infty.\tag{5.5}\label{eq:5.5}
\end{align*}
According to \hyperref[thm:3.58]{Theorem 3.58}, the stopped process $(H\cdot M)^t$ is a martingale of $\mathbb{H}^2$. As a result, we have
\begin{align*}
	\E\left[\int_0^tH_s\,\d M_s\right]=0,\quad \E\left[\left(\int_0^t H_s\,\d M_s\right)^2\right]=\E\left[\int_0^t H_s^2\,\d \langle M,M\rangle_s\right].
\end{align*}
Consequently, regardless of whether the condition \hyperref[eq:5.5]{(5.5)} holds, we have the following bound:
\begin{align*}
	\E\left[\left(\int_0^t H_s\,\d M_s\right)^2\right]\leq\E\left[\int_0^t H_s^2\,\d \langle M,M\rangle_s\right]. \tag{5.6}\label{eq:5.6}
\end{align*}
This result remains true if we replace $t$ by a stopping time $\tau$.

\paragraph{Preliminary: Locally bounded processes.} A progressive process $H=(H_s)_{s\geq 0}$ is said to be \textit{locally bounded} if $\sup_{0\leq s\leq t}\vert H_s\vert <\infty$ $a.s.$ for all $t>0$. In this case, for every finite variation process $V$, one have
\begin{align*}
	\int_0^t \vert H_s\vert\left\vert \d V_s\right\vert\leq \sup_{0\leq s\leq t}\vert H_s\vert\left(\int_0^t\vert \d V_s\vert\right) < \infty\ \ a.s.,\quad\forall t>0.
\end{align*}
In particular, an adapted and sample-continuous process is locally bounded progressive process. 

\paragraph{Theorem 5.6\label{thm:5.6}} (Stochastic integrals for continuous semimartingales). Let $X$ be a continuous semimartingale with canonical decomposition $X=M+A$. If $H$ is a locally bounded progressive process, the stochastic integral $H\cdot X$ is the continuous semimartingale with canonical decomposition $H\cdot X=H\cdot M + H\cdot A$. The following properties hold for this stochastic integral:
\begin{itemize}
	\item[(i)] The mapping $(H,X)\mapsto H\cdot X$ is bilinear.
	\item[(ii)] If $K$ is another locally bounded progressive process, then $K\cdot (H\cdot A)=(KH)\cdot A$.
	\item[(iii)] For a stopping time $\tau$, we have $H\mathds{1}_{[0,\tau]}\cdot X = (H\cdot X)^\tau = H\cdot X^\tau$.
	\item[(iv)] If $X$ is a continuous local martingale, resp. if $X$ is a finite variation process, so is $H\cdot X$;
	\item[(v)] If $H$ is of the form $H_s(\omega)=\sum_{j=1}^nH_{(j)}(\omega)\mathds{1}_{(t_{j-1},t_j]}(s)$, where $0=t_0<t_1<\cdots<t_n$, and $H_{(j)}$ is a $\mathscr{F}_{t_{j-1}}$-measurable random variable for every $j\in\{1,\cdots,n\}$, then
	\begin{align*}
		(H\cdot X)_t = \sum_{j=1}^n H_{(j)}\left(X_{t_j\wedge t}-X_{t_{j-1}\wedge t}\right).
	\end{align*}
\end{itemize}
\begin{proof}
The properties (i)-(iv) follow from the results obtained when $X$ is a continuous local martingale, resp. a finite variation process. To obtain (iv), it suffices to consider the case where $X=M$ is a continuous local martingale with $M_0=0$. We may even assume that $M\in\mathbb{H}^2$ by stopping it at a suitable time and using \hyperref[eq:5.2]{(5.2)}. We choose the following sequence of stopping times, with the convention $\inf\emptyset=\infty$:
\begin{align*}
	\tau_k=\inf\left\{t\geq 0:\vert H_s\vert\geq k\right\} = \inf\left\{t_{j-1}:\vert H_{(j)}\vert\geq k\right\}.
\end{align*}
Then $\tau_k\uparrow\infty$ as $k\to\infty$. Furthermore, for every $k$,
\begin{align*}
	H_s\mathds{1}_{[0,\tau_k]}(s) = \sum_{j=1}^nH_{(j)}^k \mathds{1}_{(t_{j-1},t_j]}(s),\quad \textit{where}\ H_{(j)}^k = H_{(j)}\mathds{1}_{\{\tau_k\geq t_{j-1}\}}\leq k.
\end{align*}
Consequently, $H\mathds{1}_{[0,\tau_k]}$ is an elementary process, and its stochastic integral with respect to $M\in\mathbb{H}^2$ is
\begin{align*}
	(H\cdot M)_{t\wedge\tau_k}=(H\mathds{1}_{[0,\tau_n]}\cdot M)_t = \sum_{j=1}^n H_{(j)}^k\left(X_{t_j\wedge t}-X_{t_{j-1}\wedge t}\right).
\end{align*}
Then the desired result follows by letting $k\to\infty$.
\end{proof}

We introduce an important convergence result for stochastic integrals.

\paragraph{Theorem 5.7\label{thm:5.7}} (Dominated convergence theorem for stochastic integrals). Let $X=M+A$ be the canonical decomposition of a continuous semimartingale $X$, and let $t\geq 0$. Let $(H^n)_{n=1}^\infty$ be a sequence of locally bounded progressive processes such that $\lim_{n\to\infty} H^n_s= H_s\ a.s.$ for every $s\in[0,t]$, where $H$ is a locally bounded progressive process. Let $K$ be a nonnegative progressive process such that 
\begin{align*}
	\int_0^t K_s^2\,\d \langle M,M\rangle_s<\infty\ a.s.,\quad\textit{and}\quad \int_0^t K_s\left\vert \d A_s\right\vert<\infty\ a.s..\tag{5.7}\label{eq:5.7}
\end{align*}
If the sequence $(H^n)_{n=1}^\infty$ is dominated by $K$, i.e. $\vert H^n_s\vert\leq K_s\ a.s.$ for every $n\in\mathbb{N}$ and every $s\in[0,t]$, then the following convergence result holds in probability:
\begin{align*}
	\int_0^t H_s\,\d X_s = \lim_{n\to\infty}\int_0^t H_s^n\,\d X_s.
\end{align*}
Note the property \hyperref[eq:5.7]{(5.7)} holds if $K$ is a locally bounded progressive progress.
\begin{proof}
The convergence $\int_0^t H_s^n\,\d A_s\to\int_0^t H_s\,\d A_s\ a.s.$ holds by Lebesgue dominated convergence theorem. So we only need to deal with the case $X=M$ is a continuous local martingale. For every $p\in\mathbb{N}$, choose the following stopping time:
\begin{align*}
	\tau_p = \inf\left\{r\in[0,t]:\int_0^r K_s^2\,\d \langle M,M\rangle_s\geq p\right\}\wedge t.
\end{align*}
By assumption \hyperref[eq:5.7]{(5.7)}, we have $\P(\tau_p=t)\to 1$ as $p\to\infty$. The bound \hyperref[eq:5.6]{(5.6)} of stopping time version gives the following estimate of second moment:
\begin{align*}
	\E\left[\left(\int_0^{\tau_p} H_s^n\,\d M_s-\int_0^{\tau_p} H_s\,\d M_s\right)^2\right]=\E\left[\int_0^{\tau_p} (H_s^n- H_s)^2\,\d \langle M,M\rangle_s\right].
\end{align*}
Since $\int_0^{\tau_p} K_s^2\,\d \langle M,M\rangle_s\leq p$, and $\vert H_s^n-H_s\vert\leq 2K_s\ a.s.$, and $H_s^n\to H_s\ a.s.$ for all $s\in[0,t]$, by Lebesgue dominated convergence theorem, the right-hand side converges to $0$ as $n\to\infty$. Then
\begin{align*}
	\P\left(\left\vert\int_0^t H_s^n\,\d M_s-\int_0^t H_s\,\d M_s\right\vert\geq\eta\right)\leq\P(\tau_p<t) + \frac{1}{\eta^2}\E\left[\left(\int_0^{\tau_p} H_s^n\,\d M_s-\int_0^{\tau_p} H_s\,\d M_s\right)^2\right].
\end{align*}
Since the right-hand result can be controlled by arbitrarily small quantity, the result follows.
\end{proof}

For continuous integrands, we have the following useful approximation result.
\paragraph{Proposition 5.8.\label{prop:5.8}} Let $X$ be a continuous martingale, and let $H$ be an sample-continuous adapted process. For every $t>0$ and every sequence of partitions $0=t_0^n<t_1^n<\cdots<t_{k_n}^n=t$ whose mesh tends to $0$, we have
\begin{align*}
	\int_0^t H_s\,\d X_s = \lim_{n\to\infty}\sum_{j=1}^{k_n}H_{t_{j-1}^n}(X_{t_j^n}-X_{t_{j-1}^n})\quad\textit{in probability}.\label{eq:5.8}\tag{5.8}
\end{align*}
\begin{proof}
For every $n\in\mathbb{N}$, we define $H_0^n=H_0$ and $H_s^n=\sum_{j=1}^{k_n} H_{t_{j-1}^n}\mathds{1}_{(t_{j-1}^n,t_j^n]}$ for all $s>0$. Then $H^n$ is a left-continuous adapted process, hence is progressive. By \hyperref[thm:5.6]{Theorem 5.6 (v)}, we have
\begin{align*}
	\int_0^t H_s^n\,\d X_s = \sum_{j=1}^{k_n}H_{t_{j-1}^n}(X_{t_j^n}-X_{t_{j-1}^n}),\quad \forall n\in\mathbb{N}.
\end{align*}
We take $K_s=\max_{0\leq r\leq s}\vert H_r\vert$. This is a locally bounded progressive process dominating $H^n$. Since $H$ is sample continuous, we have $H_s^n\to H_s\ a.s.$ for all $s\in[0,t]$. Using \hyperref[thm:5.7]{Theorem 5.7} concludes the proof.
\end{proof}

\paragraph{Remark.} Note in \hyperref[eq:5.8]{(5.8)}, we evaluate $H$ at the \uwave{left end} of every interval $(t_{j-1}^n,t_j^n]$. The result fails if we replace $H_{t_{j-1}^n}$ by $H_{t_j^n}$. To see a counterexample, we take $H=Y$ to be another continuous martingale. Then
\begin{align*}
	\sum_{j=1}^{k_n}Y_{t_j^n}(X_{t_j^n}-X_{t_{j-1}^n})=\sum_{j=1}^{k_n}Y_{t_{j-1}^n}(X_{t_j^n}-X_{t_{j-1}^n}) + \sum_{j=1}^{k_n}(X_{t_j^n}-X_{t_{j-1}^n})(Y_{t_j^n}-Y_{t_{j-1}^n})
\end{align*}
The convergence results in \hyperref[eq:5.8]{(5.8)} and \hyperref[eq:3.10]{(3.10)} imply $\lim_{n\to\infty}\sum_{j=1}^{k_n}Y_{t_j^n}(X_{t_j^n}-X_{t_{j-1}^n}) = \int_0^t Y_s\,\d X_s + \langle X,Y\rangle_t$, and
\begin{align*}
	X_tY_t-X_0Y_0 = \int_0^t X_s\,\d Y_s+\int_0^t Y_s\,\d X_s+\langle X,Y\rangle_t.\tag{5.9}\label{eq:5.9}
\end{align*}
This is known as the \textit{formula of integration by parts}.

\subsection{Itô's Formula and its Consequences}
\paragraph{Theorem 5.9\label{thm:5.9}} (Itô's Lemma). Let $X^1,\cdots,X^p$ be $p$ continuous semimartingales, and let $F\in C^2(\mathbb{R}^p,\mathbb{R})$. Then for every $t\geq 0$, we have
\begin{align*}
	F(X^1_t,\cdots,X_t^p)=F(X_0^1,\cdots,X_0^p)+\sum_{j=1}^p\int_0^t\frac{\partial F(X_s^1,\cdots,X_s^p)}{\partial x^j}\,\d X_s^j + \frac{1}{2}\sum_{i,j=1}^p\int_0^t\frac{\partial^2 F(X_s^1,\cdots,X_s^p)}{\partial x^i\partial x^j}\,\d \langle X^i,X^j\rangle_s.
\end{align*}
\begin{proof}
We write $(X^1,\cdots,X^p)=X$ for brevity. Fix $t>0$, and consider an increasing sequence of partitions $0=t_0^n<t_1^n<\cdots<t_{k_n}^n=t$ whose mesh tends to $0$. According to Taylor's theorem,
\begin{align*}
	F(X_t)&=F(X_0)+\sum_{l=1}^{k_n}\left(F(X_{t_j^n})-F(X_{t_{j-1}^n})\right)\\
	&=F(X_0)+\underbrace{\sum_{j=1}^p\sum_{l=1}^{k_n}\frac{\partial F}{\partial x^j}(X_{t_{l-1}^n})(X_{t_l^n}^j-X_{t_{l-1}^n}^j)}_{\mathrm{(a)}} + \underbrace{\frac{1}{2}\sum_{i,j=1}^p\sum_{l=1}^{k_n} f_{n,l}^{i,j}(X_{t_l^n}^i-X_{t_{l-1}^n}^i)(X_{t_l^n}^j-X_{t_{l-1}^n}^j)}_{\mathrm{(b)}},
\end{align*}
where the quantity $f_{n,l}^{i,j}$ can be written as 
$$f_{n,l}^{i,j} = \frac{\partial^2 F}{\partial x^i\partial x^j}((1-\xi)X_{t_{l-1}^n}+\xi X_{t_l^n})$$
for some random variable $\xi:\Omega\to[0,1]$. By \hyperref[prop:5.8]{Proposition 5.8}, the term (a) converges to $\sum_{j=1}^p\int_0^t\frac{\partial F}{\partial x^j}(X_s)\,\d X_s^j$ in probability as $n\to\infty$. So it remains to find the limit of term (b). For brevity we write $D_{ij}F=\frac{\partial^2 F}{\partial x^i\partial x^j}$. By uniform continuity of the second derivatives of $F$ on compact intervals, we have for all $i,j\in\{1,\cdots,p\}$ that
\begin{align*}
	\sup_{1\leq l\leq k_n}\left\vert f_{n,l}^{i,j}-D_{ij}F(X_{t_{l-1}^n})\right\vert\leq\sup_{1\leq l\leq k_n}\left(\sup_{x\in\left[X_{t_{l-1}^n}\wedge X_{t_l^n},X_{t_{l-1}^n}\vee X_{t_l^n}\right]} \left\vert D_{ij}F(x)-D_{ij}F(X_{t_{l-1}^n})\right\vert\right)\to 0\quad a.s..
\end{align*}
By \hyperref[prop:3.65]{Proposition 3.65}, $\sum_{l=1}^{k_n}(X_{t_l^n}^i-X_{t_{l-1}^n}^i)(X_{t_l^n}^j-X_{t_{l-1}^n}^j)\overset{\P}{\to}\langle X^i,X^j\rangle_t < \infty$. This gives an estimate of (b):
\begin{align*}
	\left\vert\sum_{l=1}^{k_n} D_{ij}F(X_{t_{l-1}^n})(X_{t_l^n}^i-X_{t_{l-1}^n}^i)(X_{t_l^n}^j-X_{t_{l-1}^n}^j)-\sum_{l=1}^{k_n} f_{n,l}^{i,j}(X_{t_l^n}^i-X_{t_{l-1}^n}^i)(X_{t_l^n}^j-X_{t_{l-1}^n}^j)\right\vert\overset{\P}{\to} 0\quad \textit{as}\ \ n\to\infty.
\end{align*}
According to \hyperref[eq:5.9]{(5.9)}, the process $X^iX^j=(X_s^iX_s^j)_{s\geq 0}$ is also a semimartingale. We then transform (b) as
\begin{align*}
	\lim_{n\to\infty}&\sum_{l=1}^{k_n} D_{ij}F(X_{t_{l-1}^n})(X_{t_l^n}^i-X_{t_{l-1}^n}^i)(X_{t_l^n}^j-X_{t_{l-1}^n}^j)\\
	&= \int_0^t D_{ij}F(X_s)\,\d (X^iX^j)_s - \int_0^t D_{ij}F(X_s)X^i_s\,\d X^j_s - \int_0^t D_{ij}F(X_s)X^j_s\,\d X^i_s \tag{\textit{in probability}}\\
	&=\int_0^t D_{ij}F(X_s)\,\d (X^iX^j)_s - \int_0^t D_{ij}F(X_s)\,\d (X^i\cdot X^j)_s - \int_0^t D_{ij}F(X_s)\,\d (X^j\cdot X^i)_s \tag{\textit{by associativity}}\\
	&= \int_0^t D_{ij}F(X_s)\,\d \langle X^i,X^j\rangle_s. \tag{\textit{by linearity and \hyperref[eq:5.9]{(5.9)}}}
\end{align*}
Thus we finish the proof of Itô's formula.
\end{proof}
\noindent\textbf{Remark.} The formula \hyperref[eq:5.9]{(5.9)} of integration by parts is a special case of Itô's lemma.

\paragraph{Proposition 5.10.\label{prop:5.10}} Take a twice continuously differentiable function $F(r,x)$ in $\mathbb{R}^2$. Itô's formula implies
\begin{align*}
	F(\langle X,X\rangle_t,X_t)=F(0,X_0)+\int_0^t\frac{\partial F}{\partial x}(\langle X,X\rangle_s,X_s)\,\d X_s + \int_0^t\left(\frac{\partial F}{\partial r}+\frac{1}{2}\frac{\partial^2F}{\partial x^2}\right)(\langle X,X\rangle_s,X_s)\,\d \langle M,M\rangle_s
\end{align*}
For a continuous local martingale $M$, $F(\langle M,M\rangle_t,M_t)$ is a continuous local martingale if $\frac{\partial F}{\partial r}+\frac{1}{2}\frac{\partial^2F}{\partial x^2}=0$.

\paragraph{Remark.} We take $F(r,x)=\exp(\lambda x-\frac{\lambda^2}{2}r)$, where $\lambda\in\mathbb{C}$. Then both the real and imaginary parts of $F:\mathbb{R}^2\to\mathbb{C}$ satisfies the above condition, and $\frac{\partial F}{\partial x}=\lambda F$. We define
\begin{align*}
	\mathscr{E}(\lambda M)_t = \exp\biggl(\lambda M_t - \frac{\lambda^2}{2}\langle M,M\rangle_t\biggr),\quad \forall t\geq 0.
\end{align*}
Consequently, $\mathscr{E}(\lambda M)$ is a complex continuous local martingale (i.e. both its real and imaginary parts), and
\begin{align*}
	\mathscr{E}(\lambda M)_t = \mathrm{e}^{\lambda M_0} + \lambda\int_0^t\mathscr{E}(\lambda M)_s\,\d M_s.
\end{align*}

From now on, we are going to discuss the Brownian motions. A \textit{$d$-dimensional Brownian motion} is a stochastic process $\{B_t=(B_t^1,\cdots,B_t^d),t\geq 0\}$ with values in $\mathbb{R}^d$ whose component processes $B^1,\cdots,B^d$ are independent Brownian motions. A Brownian motion $(B_t)_{t\geq 0}$ is called a \textit{$(\mathscr{F}_t)$-Brownian motion} if it is adapted and has independent increments with respect to the filtration $(\mathscr{F}_t)_{t\geq 0}$.

\paragraph{Theorem 5.11\label{thm:5.11}} (Multi-dimensional Brownian motions and Lévy’s theorem). An adapted and sample-continuous process $B=(B^1,\cdots,B^d)$ is a $(\mathscr{F}_t)$-Brownian motion if and only if its component processes $B_1,\cdots,B_d$ are continuous local martingales such that $\langle X^i,X^j\rangle_t=\delta_{ij}t$ for all $i,j\in\{1,\cdots,d\}$ and all $t\geq 0$.
\begin{proof}
Necessity is clear by definition: If $B$ is a Brownian motion, we have $\langle B^j,B^j\rangle_t=t$ for all $j\in\{1,\cdots,d\}$ and all $t\in\mathbb{R}_+$, and independence of distinct components implies that $\langle B^i,B^j\rangle=0$ for all $i\neq j$.

Now we prove sufficiency. Take $\alpha=(\alpha^1,\cdots,\alpha_d)\in\mathbb{R}^d$ with $\vert\alpha\vert^2=\sum_{j=1}^d\alpha_j^2$. Then $\alpha^\top X_t = \sum_{j=1}^d\alpha_j B^j_t$ is a continuous local martingale with quadratic variation $\langle \alpha^\top B,\alpha^\top B\rangle = \vert\alpha\vert^2 t$. By \hyperref[prop:5.10]{Proposition 5.10}, the process $\left(\exp(\i\alpha^\top B_t-\frac{1}{2}\vert\alpha\vert^2t)\right)_{t\geq 0}$ is a continuous local martingale bounded on each compact interval $[0,t],\ t>0$, hence is a martingale. As a result, for all $t\geq s>0$,
\begin{align*}
	\E\left[\left.\exp\left(\i\alpha^\top B_t-\frac{1}{2}\vert\alpha\vert^2t\right)\right|\mathscr{F}_s\right] = \exp\left(\i\alpha^\top B_s-\frac{1}{2}\vert\alpha\vert^2s\right) \quad\Rightarrow\quad \E\left[\exp\bigl(\i\alpha^\top(B_t-B_s)\bigr)|\mathscr{F}_s\right]=\e^{-\frac{1}{2}\vert\alpha\vert^2(t-s)}.
\end{align*}

Given any $A\in\mathscr{F}_s$, we take the measure $\P_A(E)=\P(A\cap E)/\P(A),\ \forall E\in\mathscr{F}$. Comparing the characteristic functions, the law of $B_t-B_s$ does not change from $\P$ to $\P_A$. Then $\E\left[f(B_t-B_s)\mathds{1}_A\right]=\P(A)\E\left[f(B_t-B_s)\right]$ for all nonnegative measurable functions. Choosing $f$ to be indicator functions do we obtain that $B_t-B_s$ is independent of $\mathscr{F}_s$, and $X_t-X_s\sim N(0,(t-s)I_d)$. Since $B$ is adapted and sample-continuous and has independent Gaussian increments, it is a $(\mathscr{F}_t)$-Brownian motion.
\end{proof}

\paragraph{Remark.} Let $B=(B^1,\cdots,B^d)$ be a $d$-dimensional $(\mathscr{F}_t)$-Brownian motion. By Itô's formula, for a twice continuously differentiable real-valued function $F(x_1,\cdots,x_d)$ on $\mathbb{R}^d$,
\begin{align*}
	F(B_t^1,\cdots,B_t^d)&=F(B_0)+\sum_{j=1}^d\int_0^t\frac{\partial F}{\partial x_j}(B_s^1,\cdots,B_s^d)\,\d B_s^j + \frac{1}{2}\sum_{j=1}^d\int_0^t\frac{\partial^2 F}{\partial x_j^2}(B_s^1,\cdots,B_s^d)\,\d s\\
	\Rightarrow\quad F(B_t) &= F(B_0)+\int_0^t \nabla F(B_s)\cdot \d B_s + \frac{1}{2}\int_0^t \Delta F(B_s)\,\d s.
\end{align*}
Likewise, for a twice continuously differentiable real-valued function $G(t,x_1,\cdots,x_d)$ on $\mathbb{R}_+\times\mathbb{R}^d$, we have
\begin{align*}
	G(t,B_t)=G(0,B_0)+\int_0^t\nabla_x G(s,B_s)\cdot \d B_s +  \int_0^t\left(\frac{\partial G}{\partial t} + \frac{1}{2}\Delta_x G\right)(s,B_s)\,\d s.
\end{align*} 

\paragraph{} Now we introduce a time-changed Brownian motion representation of continuous local martingales. Before presenting the general conclusion, we first prove some technical results.
\paragraph{Lemma 5.12.\label{lemma:5.12}} Let $M=(M_t)_{t\geq 0}$ be a continuous local martingale. Almost surely, we have $M_a=M_b$ for all $0\leq a<b$ such that $\langle M,M\rangle_b=\langle M,M\rangle_a$.
\begin{proof}
Fix $0\leq a<b$. Consider the continuous local martingale $N_t=M_t-M_{t\wedge a}$, whose quadratic variation is given by $\langle N,N\rangle_t=\langle M,M\rangle_t-\langle M,M\rangle_{t\wedge a}$. We choose the sequence of stopping times $\tau_n=\inf\{t\geq 0:\langle N,N\rangle_t\geq 1/n\}$. Since $\langle N^{\tau_n},N^{\tau_n}\rangle\leq 1/n$, we have $N^{\tau_n}\in\mathbb{H}^2$, and
\begin{align*}
	\E\left[N_{t\wedge\tau_n}^2\right] = \E\left[\langle N,N\rangle_{t\wedge\tau_n}\right]\leq\frac{1}{n},\quad\forall t\in[a,b].
\end{align*}
On the event $A_{a,b}:=\left\{\langle M,M\rangle_b=\langle M,M\rangle_a\right\}\subset\{\tau_n\geq b\}$, we have
\begin{align*}
	\E\left[N_b^2\mathds{1}_{A_{a,b}}\right] = \E\left[N_{b\wedge\tau_n}^2\mathds{1}_{A_{a,b}}\right]\leq\E\left[N_{b\wedge\tau_n}^2\right]\leq\frac{1}{n} \quad\overset{n\to\infty}{\Rightarrow}\quad N_b=0\ \ a.s.\ on\ A_{a,b}.
\end{align*}
We set $E_{a,b}=\left\{\langle M,M\rangle_b=\langle M,M\rangle_a=M_b\neq M_a\right\}=A_{[a,b]}\cap\{M_b\neq M_a\}$, which satisfies $\P(E_{a,b})=0$. Take
\begin{align*}
	E=\bigcup_{a,b\in\mathbb{Q},\,0\leq a<b}E_{a,b}\quad\Rightarrow\quad \P(E)=0.
\end{align*}
On event $\Omega\backslash E$, whenever $\langle M,M\rangle_b=\langle M,M\rangle_a$, one can choose $\mathbb{Q}\ni a_n\downarrow a$ and $\mathbb{Q}\ni b_n\uparrow a$. Then $\langle M,M\rangle_{b_n}=\langle M,M\rangle_{a_n}$ for all $n\in\mathbb{N}$, and $M_{a_n}=M_{b_n}$. By sample-continuity of $M$, we obtain $M_a=M_b$.
\end{proof}

\paragraph{Theorem 5.13\label{thm:5.13}} (Dambis-Dubins-Schwarz). Let $M=(M_t)_{t\geq 0}$ be a continuous local martingale such that $M_0=0$ and $\langle M,M\rangle_\infty=\infty\ a.s.$. Then there exists a Brownian motion $\beta=(\beta_s)_{s\geq 0}$ such that
\begin{align*}
	a.s.\ \forall t\geq 0,\quad M_t = \beta_{\langle M,M\rangle_t}.
\end{align*}
\begin{proof}
For every $s\geq 0$, choose the stopping time $\tau_s=\inf\{t\geq 0:\langle M,M\rangle_t\geq s\}$. Since $\langle M,M\rangle_\infty=\infty\ a.s.$, we have $\tau_s<\infty\ a.s.$, and we reset $\tau_s(\omega)=0$ on the event $E=\{\langle M,M\rangle_\infty<\infty\}$. By completeness of the filtration $(\mathscr{F}_t)_{t\geq 0}$, the variable $\tau_s$ remains a stopping time. By construction, for every $\omega\in\Omega$, the function $s\mapsto\tau_s(\omega)$ is increasing. On the event $\Omega\backslash E$, we have
\begin{align*}
	\lim_{r\upuparrows s}\tau_r = \inf\bigcap_{r<s}\left\{t\geq 0:\langle M,M\rangle_t\geq r\right\} = \inf\{t\geq 0:\langle M,M\rangle_t\geq s\},\\
	\lim_{r\downdownarrows s}\tau_r = \inf\bigcup_{r>s}\left\{t\geq 0:\langle M,M\rangle_t\geq r\right\} = \inf\{t\geq 0:\langle M,M\rangle_t > s\}.
\end{align*}
Hence $s\mapsto\tau_s(\omega)$ is left-continuous, and has right limit $\tau_{s+}=\inf\{t\geq 0:\langle M,M\rangle_t>s\}$ at $s\geq 0$.

For every $s\geq 0$, we set $\beta_s=M_{\tau_s}$. By \hyperref[prop:3.12]{Proposition 3.12}, $\beta=(\beta_t)_{t\geq 0}$ is an adapted process with respect to the time-changed filtration $\mathscr{G}_s=\mathscr{F}_{\tau_s}$ for every $s\geq 0$, and $\mathscr{G}_\infty=\mathscr{F}_\infty$. The completeness of $(\mathscr{G}_t)_{t\geq 0}$ also follows from $(\mathscr{F}_t)_{t\geq 0}$. Moreover, the sample path $s\mapsto\beta_s$ is càglàd, with the right limit at $s$ given by $\beta_{s+}=M_{\tau_{s+}}$ on the event $\Omega\backslash E$. Note that $\langle M,M\rangle_{\tau_s}=\langle M,M\rangle_{\tau_{s+}}=s$ for every $s\geq 0$. By \hyperref[lemma:5.12]{Lemma 5.12}, we have almost surely $M_{\tau_{s+}}=M_{\tau_s}$ for all $s\geq 0$. Therefore, the sample path of $\beta=(\beta_s)_{s\geq 0}$ is almost surely continuous.

Now we verify that $(\beta_s)_{s>0}$ and $(\beta_s^2-s)_{s\geq 0}$ are martingales with respect to the filtration $(\mathscr{G}_s)_{s\geq 0}$. For every $n\in\mathbb{N}$, by \hyperref[thm:3.58]{Theorem 3.58}, the stopped processes $M^{\tau_n}$ and $(M^{\tau_n})^2-\langle M,M\rangle^{\tau_n}$ are uniformly integrable martingales, since $\langle M,M\rangle^{\tau_n}_\infty=\langle M,M\rangle_{\tau_n}=n<\infty$. By optional stopping theorem, for every $0\leq s<t\leq n$,
\begin{align*}
	&\E\left[\beta_t|\mathscr{G}_s\right] = \E\left[M^{\tau_n}_{\tau_t}|\mathscr{F}_{\tau_s}\right] = M_{\tau_s}=\beta_s,\\
	&\E\left[\beta_t^2-t|\mathscr{G}_s\right] = \E\left[(M_{\tau_t}^{\tau_n})^2-\langle M^{\tau_n},M^{\tau_n}\rangle_{\tau_s}|\mathscr{F}_{\tau_t}\right] = (M_{\tau_s}^{\tau_n})^2-\langle M,M\rangle_{\tau_s\wedge\tau_n} = \beta_s^2-s.
\end{align*}
The case $d=1$ of \hyperref[thm:5.11]{Theorem 5.11} implies that $\beta$ is a $(\mathscr{G}_t)$-Brownian motion.

On the other hand, by the very definition of $\tau_r$ and $\tau_{r+}$, for all $s\geq 0$, we have $\tau_{\langle M,M\rangle_s}\leq s\leq\tau_{\langle M,M\rangle_{s+}}$, and $\langle M,M\rangle_{\tau_{\langle M,M\rangle_s}}=\langle M,M\rangle_{\tau_{\langle M,M\rangle_{s+}}}=\langle M,M\rangle_s$. According to \hyperref[lemma:5.12]{Lemma 5.12}, almost surely, the equality $M_s=M_{\tau_{\langle M,M\rangle_s}}=\beta_{\langle M,M\rangle_s}$ holds for all $s\geq 0$, concluding the proof.
\end{proof}
\paragraph{Remark.} In this theorem, the Brownian motion $\beta=(\beta_t)_{t\geq 0}$ is no longer adapted with respect to the original filtration $(\mathscr{F}_t)_{t\geq 0}$, but with respect to the time-changed filtration $(\mathscr{G}_t)_{t\geq 0}$.

\paragraph{Corollary 5.14.\label{cor:5.14}} Let $M$ and $N$ be two continuous local martingales such that $M_0=N_0=0$. Assume the following conditions holds almost surely: (i) $\langle M,M\rangle_t = \langle N,N\rangle_t$ for all $t\geq 0$; (ii) $\langle M,N\rangle_t=0$ for all $t\geq 0$; (iii) $\langle M,M\rangle_\infty=\langle N,N\rangle_\infty=\infty$. Let $\beta$ and $\gamma$ be the Brownian motions such that $M_t=\beta_{\langle M,M\rangle_t}$ and $N_t=\gamma_{\langle N,N\rangle_t}$ for all $t\geq 0$ almost surely. Then $\beta$ and $\gamma$ are independent.
\begin{proof}
We choose the stopping times $\tau_s=\inf\{t\geq 0:\langle M,M\rangle_t\geq s\}=\inf\{t\geq 0:\langle N,N\rangle_t\geq s\}$ for all $s\geq 0$, so both $\beta_s=M_{\tau_s}$ and $\gamma_s=N_{\tau_s}$ are $(\mathscr{G}_t)$-Brownian motions, where $(\mathscr{G})_{t\geq 0}$ is the time-changed filtration $\mathscr{G}_t=\mathscr{F}_{\tau_t}$.

Since the continuous local martingales $M$ and $N$ are orthogonal, the process $MN$ is also a continuous local martingale. By \hyperref[prop:3.61]{Proposition 3.61 (vi)}, the stopped process $M^{\tau_n}N^{\tau_n}$ is a uniformly integrable martingale. By optional stopping theorem, for all $0\leq s<t\leq n$, we have
\begin{align*}
	\E\left[\beta_t\gamma_t|\mathscr{G}_s\right]=\E\left[M^{\tau_n}_{\tau_t}N^{\tau_n}_{\tau_t}|\mathscr{F}_{\tau_s}\right] = M^{\tau_n}_{\tau_s}N^{\tau_n}_{\tau_s} = \beta_s\gamma_s.
\end{align*}
Hence $(\beta_t\gamma_t)_{t\geq 0}$ is a $(\mathscr{G}_t)$-martingale, and again by \hyperref[prop:3.61]{Proposition 3.61 (vi)}, we have $\langle\beta,\gamma\rangle=0$. According to \hyperref[thm:5.11]{Theorem 5.11}, $(\beta,\gamma)$ is a two-dimensional $(\mathscr{G}_t)$-Brownian motion, hence $\beta$ and $\gamma$ are independent.
\end{proof}

Now we introduce a useful inequality connecting the local maxima of a continuous local martingale with its quadratic variation.

\paragraph{Theorem 5.15\label{thm:5.15}} (Burkholder-Davis-Gundy inequality). Given a continuous martingale $M$ with $M_0=0$, we define the local maxima $M_t^*=\sup_{0\leq s\leq t}\vert M_s\vert$ for all $t\geq 0$.
Then for every $p>0$, there exist a constant $C_p>0$ depending only on $p$ such that for every stopping time $\tau$,
\begin{align*}
	\E\left[(M_\tau^*)^p\right]\leq C_p\E\bigl[\langle M,M\rangle_\tau^{p/2}\bigr]
\end{align*}
\begin{proof}
By replacing a continuous local martingale $M$ with $M^\tau$, it suffices to deal with the case $\tau=\infty$. We can further assume that $M$ is bounded by replacing $M$ with $M^{\tau_n}$, where $\tau_n=\left\{t\geq 0:\vert M_t\vert = n\right\}$, and the result of $n\to\infty$ follows by monotone convergence theorem. When $p\geq 2$. Apply Itô's formula to $\vert x\vert^p$:
\begin{align*}
	\vert M_t\vert^p = \int_0^tp\vert M_s\vert^{p-1}\mathrm{sgn}(M_s)\,\d M_s + \frac{1}{2}\int_0^t p(p-1)\vert M_s\vert^{p-2}\,\d \langle M,M\rangle_s.
\end{align*}
Note that $M$ is a bounded. Then we have $M\in\mathbb{H}^2$, and the process $\left(\int_0^tp\vert M_s\vert^{p-1}\mathrm{sgn}(M_s)\,\d M_s\right)_{t\geq 0}$ is also a martingale in $\mathbb{H}^2$. Consequently, we have
\begin{align*}
	\E\left[\vert M_t\vert^p\right]&\leq\frac{p(p-1)}{2}\E\left[\int_0^t\vert M_s\vert^{p-2}\,\d \langle M,M\rangle_s\right]\leq\frac{p(p-1)}{2}\E\left[\vert M_t^*\vert^{p-2}\langle M,M\rangle_t\right]\\
	&\leq\frac{p(p-1)}{2}\E\left[\vert M_t^*\vert^p\right]^{\frac{p-2}{p}}\E\left[\langle M,M\rangle_t^{p/2}\right]^{2/p}
\end{align*}
On the other hand, the Doob's $L^p$-inequality [\hyperref[prop:3.40]{Proposition 3.40}] gives
\begin{align*}
	\E\left[\vert M_t^*\vert^p\right]\leq\left(\frac{p}{p-1}\right)^p\E\left[\vert M_t\vert^p\right]\quad\Rightarrow\quad \E\left[\vert M_t^*\vert^p\right]\leq\left(\left(\frac{p}{p-1}\right)^p\frac{p(p-1)}{2}\right)^{p/2}\E\left[\langle M,M\rangle_t^{p/2}\right]\tag{5.10}\label{eq:5.10}
\end{align*}

Now we deal with the case $p<2$. Since $M\in\mathbb{H}^2$, the process $M^2-\langle M,M\rangle$ is a uniformly integrable martingale. Hence $\E[(M_\tau)^2]=\E[\langle M,M\rangle_\tau]$ for every stopping time $\tau$. Given $x>0$, consider the stopping time $\tau_x=\inf\{t\geq 0: M_t^2\geq x\}$. If $\tau$ is a bounded stopping time, we have
\begin{align*}
	\P\left((M_\tau^*)^2\geq x\right) = \P(\tau_x\leq\tau) = \P\left((M_{\tau_x\wedge\tau})^2\geq x\right)\leq\frac{\E\left[(M_{\tau_x\wedge\tau})^2\right]}{x}=\frac{\E\left[\langle M,M\rangle_{\tau_x\wedge\tau}\right]}{x}\leq\frac{\E\left[\langle M,M\rangle_\tau\right]}{x}.
\end{align*}

Consider the stopping time $\sigma_x=\inf\{s\geq 0:\langle M,M\rangle_s\geq x\}$, so $\{\langle M,M\rangle_t\geq x\}\Leftrightarrow\{\sigma_x\leq t\}$. For every $t\geq 0$, we use the preceding bound with $\tau=\sigma_x\wedge t$:
\begin{align*}
	\P\left((M^*_t)^2\geq x\right)&\leq\P((M^*_{\sigma_x\wedge t})^2\geq x)+\P(\sigma_x\leq t)\\
	&\leq\frac{1}{x}\E\left[\langle M,M\rangle_{\sigma_x\wedge t}\right] + \P\left(\langle M,M\rangle_t\geq x\right) = \frac{1}{x}\E\left[\langle M,M\rangle_t\mathds{1}_{\{\langle M,M\rangle_t<x\}}\right] + 2\P\left(\langle M,M\rangle_t\geq x\right).
\end{align*}
Now we integrate both side with respect to $\frac{p}{2}x^{p/2-1}\,\d x$. For the left-hand side:
\begin{align*}
	\int_0^\infty \P\left((M^*_t)^2\geq x\right)\frac{p}{2}x^{p/2-1}\,\d x = \E\left[\int_0^{(M_t^*)^2}\frac{p}{2}x^{p/2-1}\,\d x\right] = \E\left[(M_t^*)^p\right].
\end{align*}
Similarly, $\int_0^\infty \P\left(\langle M,M\rangle_t\geq x\right)\frac{p}{2}x^{p/2-1}\,\d x=\E\left[\langle M,M\rangle_t^{p/2}\right]$. Furthermore,
\begin{align*}
	\int_0^\infty \E\left[\langle M,M\rangle_t\mathds{1}_{\{\langle M,M\rangle_t<x\}}\right]\frac{p}{2}x^{p/2-2}\,\d x = \E\left[\langle M,M\rangle_t\int_0^{\langle M,M\rangle_t}\frac{p}{2}x^{p/2-2}\,\d x\right]=\frac{p}{2-p}\E\left[\langle M,M\rangle^{p/2}_t\right].
\end{align*}
This gives the bound
\begin{align*}
	\E\left[(M_t^*)^p\right]\leq\frac{4-p}{2-p}\E\left[\langle M,M\rangle^{p/2}_t\right]\tag{5.11}\label{eq:5.11}
\end{align*}
Note both the bounds \hyperref[eq:5.10]{(5.10)} and \hyperref[eq:5.11]{(5.11)} holds for $t=\infty$ by monotone convergence theorem.
\end{proof}

\paragraph{Remark.} In fact, the complete Burkholder–Davis–Gundy inequality gives both upper and lower bounds: For every real $p > 0$,
there exist two constants $c_p, C_p > 0$ depending only on $p$ such that, for every continuous local martingale $M$ with $M_0=0$ and every stopping time $\tau$, one have
\begin{align*}
	c_p\E\bigl[\langle M,M\rangle_\tau^{p/2}\bigr]\leq \E\left[(M_\tau^*)^p\right]\leq C_p\E\bigl[\langle M,M\rangle_\tau^{p/2}\bigr].
\end{align*}
Furthermore, the case of $p=1$ gives a sufficient condition for a continuous local martingale $M$ with $M_0=0$ to be a uniformly integrable martingale: $\E[\langle M,M\rangle_\infty^{1/2}]<\infty$.

\newpage
\subsection{The Representation of Martingales as Stochastic	Integral}
On a probability space $(\Omega,\mathscr{F},P)$, we choose a filtration $(\mathscr{F}_t)_{t\geq 0}$ on $\Omega$ to be the completed canonical filtration of a Brownian motion. An interesting conclusion is that all martingales with respect to this filtration can be represented as stochastic integrals with respect to the Brownian motion. 

In this section, we fix a Brownian motion $B=(B_t)_{t\geq 0}$, and assume $(\mathscr{F}_t)_{t\geq 0}$ to be the completed canonical filtration of $B$. Before formally present this result, we first introduce some technical lemma.

\paragraph{Lemma 5.16.\label{lemma:5.16}} The vector space generated by the random variables of the form
\begin{align*}
	\exp\left(\i\sum_{j=1}^n\lambda_j(B_{t_j}-B_{t_{j-1}})\right),\quad \textit{where}\ \ 0=t_0<t_1<\cdots<t_n\ \ \textit{and}\ \ \lambda_1,\cdots,\lambda_n\in\mathbb{R}
\end{align*}
is dense in the space $L^2_\mathbb{C}(\Omega,\mathscr{F}_\infty,\P)$ of all square-integrable complex-valued $\mathscr{F}_\infty$-measurable random variables.
\begin{proof}
By elementary Hilbert theory, it suffices to show that $Z=0$ is the only variable that satisfying
\begin{align*}
	\E\left[Z\exp\left(\i\sum_{j=1}^n\lambda_j(B_{t_j}-B_{t_{j-1}})\right)\right]=0
\end{align*}
for all choices of $0=t_0<t_1<\cdots<t_n$ and $\lambda_1,\cdots,\lambda_n\in\mathbb{R}$. Define the complex measure $\mu$ on $\mathbb{R}^n$ by
\begin{align*}
	\mu(F)=\E\left[Z\mathds{1}_F(B_{t_1},B_{t_2}-B_{t_1},\cdots,B_{t_n}-B_{t_{n-1}})\right],\quad\forall F\in\mathscr{B}(\mathbb{R}^n).
\end{align*}
Then the Fourier transform of $\mu$ satisfies
\begin{align*}
	\int_{\mathbb{R}^n}\e^{\i\lambda^\top x}\,\mu(\d x) =	\E\left[Z\exp\left(\i\sum_{j=1}^n\lambda_j(B_{t_j}-B_{t_{j-1}})\right)\right]=0,\quad\forall\lambda\in\mathbb{R}^n.
\end{align*}
By Lévy's continuity theorem, we have $\mu=0$. Hence $\E[Z\mathds{1}_A]=0$ for all $A\in\sigma(B_{t_1},\cdots,B_{t_n})$. A monotone class argument shows that $\E[Z\mathds{1}_A]$ holds for all $A\in\sigma(B_t,t\geq 0)$, and further by completion, for all $A\in\mathscr{F}_\infty$. Hence $Z=0$ in $L^2_\mathbb{C}(\Omega,\mathscr{F}_\infty,\P)$.
\end{proof}

\paragraph{Theorem 5.17.\label{thm:5.17}} For every $Z\in L^2(\Omega,\mathscr{F}_\infty,\P)$, there exists a unique progressive process $H\in L^2(B)$ such that
\begin{align*}
	Z=\E[Z]+\int_0^\infty H_s\,\d B_s.\tag{5.12}\label{eq:5.12}
\end{align*}
Consequently, for every $L^2$-bounded martingale $M$ (resp. for every continuous local martingale $M$), there exists a unique process $H\in L^2(B)$ (resp. $H\in L^2_{\mathrm{loc}}(B)$) and a constant $C\in\mathbb{R}$ such that
\begin{align*}
	M_t = C+\int_0^t H_s\,\d B_s,\quad \forall t\geq 0.\label{eq:5.13}\tag{5.13}
\end{align*}
\begin{proof}
Consider the first assertion. If both $H$ and $H^\prime$ satisfy this \hyperref[eq:5.12]{(5.12)}, the second moment formula gives
\begin{align*}
	0=\E\left[\left(\int_0^\infty(H_s-H^\prime_s)\,\d B_s\right)^2\right]=\E\left[\int_0^\infty(H_s-H^\prime_s)^2\,\d s\right]\quad\Rightarrow\quad H=H^\prime\ \ \textit{in}\ \ L^2(B).
\end{align*}
This shows uniqueness. For existence, let $\mathscr{H}$ be the vector space of all $Z\in L^2(\Omega,\mathscr{F}_\infty,\P)$ for which there exists an associated $H\in L^2(B)$ satisfying \hyperref[eq:5.12]{(5.12)}. By \hyperref[prop:5.10]{Proposition 5.10}, for any step function $h=\sum_{j=1}^n\lambda_j\mathds{1}_{(t_{j-1},t_j]}$, where $0=t_0<t_1<\cdots<t_n$ and $\lambda_1,\cdots,\lambda_n\in\mathbb{R}$, we have
\begin{align*}
	\exp\biggl(\i\sum_{j=1}^n\lambda_j(B_{t_j}-B_{t_{j-1}})+\frac{1}{2}\sum_{j=1}^n\lambda_j^2(t_j-t_{j-1})\biggr)=\mathscr{E}(\i h\cdot B)_t = 1+\i\int_0^t\mathscr{E}(\i h\cdot B)_s h(s)\,\d B_s
\end{align*}
Since $\mathscr{E}(\i h\cdot B)_sh(s)$ is a bounded continuous local martingale, both its real and imaginary parts are in $L^2(B)$. This implies that both the real and imaginary parts of $\exp\left(\i\sum_{j=1}^n\lambda_j(B_{t_j}-B_{t_{j-1}})\right)$ are in $\mathscr{H}$. According to \hyperref[lemma:5.16]{Lemma 5.16}, $\mathscr{H}$ contains a dense subset of $L^2(\Omega,\mathscr{F}_\infty,\P)$.

To prove the first assertion, it remains to show that $\mathscr{H}$ is closed, which implies $\mathscr{H}=L^2(\Omega,\mathscr{F}_\infty,\P)$. We assume that $Z_n\in\mathscr{H}$ is a sequence of random variables that converges to $Z$ in $L^2$. Let $H^{(n)}\in L^2(B)$ be the associated  progressive process. Then
\begin{align*}
	\Vert H^{(n)}-H^{(m)}\Vert_{L^2(B)} &= \E\left[\int_0^\infty(H^{(n)}_s-H^{(m)}_s)^2\,\d s\right] = \E\left[\left(\int_0^\infty(H_s^{(n)}-H_s^{(m)})\,\d B_s\right)^2\right]\\
	&= \E\left[\left(Z_n-Z_m-\E[Z_n]+\E[Z_m]\right)^2\right] \leq \Vert Z_n-Z_m\Vert^2_2.
\end{align*}
Hence $H^{(n)}$ is a Cauchy sequence in $L^2(B)$, which converges to some $H\in L^2(B)$ by completeness of $L^2(B)$. Consequently, $Z_n=\E[Z_n]+\int_0^\infty H_s^{(n)}\,\d B_s$ converges to $Z=\E[Z]+\int_0^\infty H_s\,\d B_s$ in $L^2$.

We turn to the second assertion. The uniqueness argument is similar. If $M$ is a $L^2$-bounded martingale, then $M_t$ converges $a.s.$ and in $L^2$ to some $M_\infty\in L^2(\Omega,\mathscr{F}_\infty,\P)$. Since $(\int_0^t H_s\,\d B_s)_{s\geq 0}$ is bounded in $L^2$, it is a uniformly integrable martingale. We can find the process $H\in L^2(B)$ that satisfies \hyperref[eq:5.12]{(5.12)} for $M_\infty$. Then
\begin{align*}
	M_\infty = \E[M_\infty] + \int_0^\infty H_s\,\d B_s\quad\Rightarrow\quad M_t = \E[M_\infty|\mathscr{F}_t] = \E[M_\infty]+\int_0^t H_s\,\d B_s,\quad \forall t\geq 0.
\end{align*}

Finally, if $M$ is a continuous local martingale, we have $M_0=C\ a.s.$ for some constant $C\in\mathbb{R}$ because $\mathscr{F}_0$ is $\P$-trivial. We then choose stopping times $\tau_n=\left\{t\geq 0:\vert M_t\vert\geq n\right\}$ so that $M^{\tau_n}$ are bounded martingales. According to the preceding result, there exists $H^{(n)}\in L^2(B)$ such that
\begin{align*}
	M_{t\wedge\tau_n} = C + \int_0^t H^{(n)}_s\,\d B_s,\quad\forall t\geq 0.
\end{align*}
By uniqueness of the progressive process in this representation, we have $H^{(m)}=H^{(n)}\mathds{1}_{[0,\tau_m]}$ in $L^2(B)$ for all $n>m$. Consequently, we can find $H\in L^2_{\mathscr{loc}}(B)$ such that $H^{(n)}=H\mathds{1}_{[0,\tau_n]}$ in $L^2(B)$ for all $n\in\mathbb{N}$, and the representation formula \hyperref[eq:5.13]{(5.13)} follows by letting $n\to\infty$. The uniqueness argument is similar.
\end{proof}

\paragraph{Remark.} In this theorem, we do not require the $L^2$-bounded martingale $M$ to be sample-continuous. Next we discuss some consequence of this representation theorem.

\paragraph{Proposition 5.18.\label{prop:5.18}} The filtration $(\mathscr{F}_t)_{t\geq 0}$ is right-continuous.
\begin{proof}
Let $Z$ be a bounded $\mathscr{F}_{t+}$-measurable random variable. By \hyperref[thm:5.17]{Theorem 5.17}, there exists $H\in L^2(B)$ such that $Z=\E[Z]+\int_0^\infty H_s\,\d B_s$. Given $\epsilon>0$, $Z$ is $\mathscr{F}_{t+\epsilon}$ measurable. By continuity of $H\cdot B$, we have
\begin{align*}
	Z=\E[Z|\mathscr{F}_{t+\epsilon}]=\E[Z]+\int_0^{t+\epsilon}H_s\,\d B_s\overset{a.s.}{\rightarrow}\E[Z]+\int_0^{t}H_s\,\d B_s,\quad\epsilon\downdownarrows 0.
\end{align*}
As a result, $Z=\E[Z]+\int_0^t H_s\,\d B_s\ a.s.$. By completeness of the filtration $(\mathscr{F}_t)_{t\geq 0}$, $Z$ is also $\mathscr{F}_t$-measurable. Hence for all $A\in\mathscr{F}_{t+}$, the variable $Z=\mathds{1}_A$ is $\mathscr{F}_t$ measurable, and $A\in\mathscr{F}_t$. Therefore $\mathscr{F}_{t+}=\mathscr{F}_t$.
\end{proof}

\paragraph{Remark.} We also can define the left limit of $(\mathscr{F}_t)_{t\geq 0}$ at any $t>0$:
\begin{align*}
	\mathscr{F}_{t-}=\sigma\left(\bigcup_{0\leq s<t}\mathscr{F}_s\right).
\end{align*}

A similar argument to \hyperref[prop:5.18]{Proposition 5.18} implies that $\mathscr{F}_{t-}=\mathscr{F}_t$. Namely, the completed filtration $(\mathscr{F}_t)_{t\geq 0}$ generated by a Brownian motion $B$ is also left-continuous.

\paragraph{Proposition 5.19.\label{prop:5.19}} All martingales with respect to the filtration $(\mathscr{F}_t)_{t\geq 0}$ have an $a.s.$ modification with continuous sample paths.
\begin{proof}
Following \hyperref[thm:5.17]{Theorem 5.17}, an $L^2$-bounded martingale is continuous according to the representation formula \hyperref[eq:5.13]{(5.13)}. Now we consider a uniformly integrable martingale $M$. This suffice since we can replace a martingale $M$ by the stopped martingale $M^a$ for every $a\geq 0$.

Since $M$ is a uniformly integrable martingale, we have $M_t=\E[M_\infty|\mathscr{F}_t]$ for all $t\geq 0$. By \hyperref[thm:3.44]{Theorem 3.44} and \hyperref[prop:5.18]{Proposition 5.1}, $M$ has an $a.s.$ modification with càdlàg sample paths, which we still denote by $M$ for simplicity. Let $M_\infty^{(n)}$ be a sequence of bounded random variables that converges in $L^1$ to $M_\infty$, and introduce the martingales $M^{(n)}_t=\E[M^{(n)}_\infty|\mathscr{F}_t]$. These martingales are then bounded in $L^2$, hence sample-continuous. Furthermore, the Doob's maximal inequality \hyperref[prop:3.40]{Proposition 3.40 (i)} gives
\begin{align*}
	\P\left(\sup_{t\geq 0}\vert M^{(n)}_t-M_t\vert\geq\lambda\right)\leq\frac{3}{\lambda}\E\bigl[\vert M_\infty^{(n)}-M_\infty\vert\bigr],\quad\forall\lambda>0.
\end{align*}
We choose a subsequence $n_k$ such that
\begin{align*}
	\P\left(\sup_{t\geq 0}\vert M^{(n_k)}_t-M_t\vert > \frac{1}{2^k}\right)\leq \frac{1}{2^k},\ \forall k\in\mathbb{N}\quad\Rightarrow\quad \P\left(\sup_{t\geq 0}\vert M^{(n_k)}_t-M_t\vert >\frac{1}{2^k}\ for\ infinitely\ many\ k\right)=0.
\end{align*}
Here we use Borel-Cantelli lemma. Consequently, $\sup_{t\geq 0}\vert M^{(n_k)}_t-M_t\vert\to 0\ a.s.$, and the sample paths of $M$, being the uniform limit of a sequence of continuous functions, is continuous.
\end{proof}

\newpage
\subsection{Stochastic Differential Equations}
We start from a deterministic process $(y_t)_{t\geq 0}$, of which the dynamic is specified by the following ordinary differential equation (ODE):
\begin{align*}
	\d y_t = b(t,y_t)\,\d t\quad\Leftrightarrow\quad y_t = y_0 + \int_0^t b(s,y_s)\,\d s 
\end{align*}

To model a noisy system, we simply introduce a random perturbation of the term $\sigma\,\d B_t$, where $\sigma>0$ is a constant, and $B_t$ is a Brownian motion. This form implicitly assumes independence of perturbations affecting disjoint time intervals, and we get
\begin{align*}
	\d y_t = b(t,y_t)\,\d t + \sigma\,\d B_t\quad\Leftrightarrow\quad y_t = y_0 + \int_0^t b(s,y_s)\,ds + \sigma B_t.
\end{align*}

We generalize the above equation by allowing $\sigma$ depending on the time $t$ and the state $y_t$:
\begin{align*}
	\d y_t = b(t,y_t)\,\d t + \sigma(t,y_t)\,\d B_t\quad\Leftrightarrow\quad y_t = y_0 + \int_0^t b(s,y_s)\,ds + \int_0^t\sigma(s,y_s)\,\d B_s.
\end{align*}
This gives rise to the following definition of stochastic differential equation.

\paragraph{Definition 5.20\label{def:5.20}} (Stochastic Differential Equation, SDE). Let $p,q\in\mathbb{N}$. Let $\sigma=(\sigma_{ij})_{i\in[p],j\in[q]}:\mathbb{R}_+\times\mathbb{R}^q\to\mathbb{R}^{p\times q}$ and $b=(b_i)_{i\in[p]}:\mathbb{R}_+\times\mathbb{R}^q\to\mathbb{R}^p$ be locally bounded measurable functions. A \textit{solution of the stochastic equation} $E(\sigma,b)$, which is given by
\begin{align*}
	\d X_t = \sigma(t,X_t)\,\d B_t + b(t,X_t)\,\d t,
\end{align*}
consists of:
\begin{itemize}
	\item A filtered probability space $(\Omega,\mathscr{F},\P)$ and a complete filtration $(\mathscr{F})_{t\geq 0}$;
	\item A $q$-dimensional $(\mathscr{F}_t)$-Brownian motion $B=(B^1,\cdots,B^q)$ starting from $0$;
	\item An $(\mathscr{F}_t)$-adapted and sample-continuous process $X=(X^1,\cdots,X^p)$ taking values in $\mathbb{R}^p$, such that
	\begin{align*}
		X_t = X_0 + \int_0^t\sigma(s,X_s)\,\d B_s + \int_0^t b(s,X_s)\,\d s\quad\overset{def}{\Leftrightarrow}\quad X_t^i = X_0^i + \sum_{j=1}^q\int_0^t\sigma_{ij}(s,X_s)\,\d B_s^j + \int_0^t b_i(s,X_s)\,\d s.
	\end{align*}
\end{itemize}
If $X_0\sim\delta_x$ for any $x\in\mathbb{R}^p$, we say that $X$ is a solution of $E^x(\sigma,b)$.

\paragraph{} There are several notions of existence and uniqueness for stochastic differential equations.

\paragraph{Definition 5.21\label{def:5.21}} For the stochastic differential equation $E(\sigma,b)$, we say that there is
\begin{itemize}
	\item \textit{weak existence}, if for every $x\in\mathbb{R}^p$, there exists a solution of $E^x(\sigma,b)$;
	\item \textit{weak existence} and \textit{weak uniqueness}, if in addition, for every $x\in\mathbb{R}^p$, all solutions
	of $E^x(\sigma,b)$ have the same law;
	\item \textit{pathwise uniqueness}, if, whenever the filtered probability space $(\Omega,\mathscr{F},(\mathscr{F}_t)_{t\geq 0},\P)$ and the $(\mathscr{F}_t)$-Brownian motion $B$ are fixed, two solutions $X$ and $Y$ such that $X_0=Y_0\ a.s.$ are indistinguishable.
\end{itemize}
Furthermore, we say that a solution $X$ of $E(\sigma,b)$ is a \textit{strong solution} if $X$ is adapted with respect to the completed canonical filtration of $B$.

\paragraph{Remark.} We give a simple example of stochastic differential equation where weak existence and weak uniqueness hold, but pathwise uniqueness fails. Let $(\beta_t)_{t\geq 0}$ be a Brownian motion with $\beta_0=x\in\mathbb{R}$, and consider
\begin{align*}
	B_t = \int_0^t\mathrm{sgn}(\beta_s)\,\d\beta_s,\quad\textit{where}\ \ \mathrm{sgn}(x)=\mathds{1}_{(0,\infty)}(x)-\mathds{1}_{(-\infty,0]}(x).
\end{align*}
Then $(B_t)_{t\geq 0}$ is a continuous local martingale with quadratic variation $\langle B,B\rangle_t=t$, hence is a Brownian motion by \hyperref[thm:5.11]{Theorem 5.11}. Furthermore, by associativity of stochastic integrals, one have
\begin{align*}
	\int_0^t \mathrm{sgn}(\beta_s)\,\d B_s=\int_0^t\mathrm{sgn}(\beta_s)^2\,\d\beta_s = \beta_t-\beta_0,\quad\forall t\geq 0.
\end{align*}
Therefore, $(\beta_t)_{t\geq 0}$ solves the following stochastic differential equations:
\begin{align*}
	\d X_t = \mathrm{sgn}(X_t)\,\d B_s,\quad X_0=x.
\end{align*}
For this equation, weak existence holds, and again by \hyperref[thm:5.11]{Theorem 5.11} we know that any solution of this equation must be a Brownian motion. Hence weak uniqueness also holds. Nevertheless, pathwise uniqueness fails for this equation. For example, if we set $x=0$, then both $\beta$ and $-\beta$ solve the preceding SDE with the same Brownian motion $B$ starting from $0$.

\paragraph{} Now we study the properties of SDE $E(\sigma,b)$ where functions $\sigma$ and $b$ are continuous on $\mathbb{R}_+\times\mathbb{R}^q$ and Lipschitz in the variable $x$. Then there exists a constant $L$ such that for every $t\geq 0$ and $x,y\in\mathbb{R}^d$,
\begin{align*}
	\vert\sigma(t,x)-\sigma(t,y)\vert&\leq L\vert x-y\vert,\\
	\vert b(t,x)-b(t,y)\vert&\leq L\vert x-y\vert.
\end{align*}
Here we use $\vert\cdot\vert$ to denote the Euclidean norm of vectors and the Frobenius norm of matrices.
\paragraph{Lemma 5.22\label{lemma:5.22}} (Gronwall's lemma). Let $T>0$ and let $g:[0,T]\to\mathbb{R}_+$ be a bounded
measurable function. If there exist two constants $a\geq 0$ and $b\geq 0$ such that
\begin{align*}
	g(t)\leq a+b\int_0^t g(s)\,\d s,\quad\forall t\in[0,T],
\end{align*}
then we have $g(t)\leq a\e^{bt}$ for all $t\in[0,T]$.
\begin{proof}
A simple recursion on $g$ gives
\begin{align*}
	g(t)&\leq a + a(bt) + b^2\int_0^t\left(\int_0^{s_1} g(s_2)\,\d s_2\right)\,\d s_1\leq\cdots\\
	& \leq a + a(bt) + a\frac{(bt)^2}{2} +\cdots + a\frac{(bt)^n}{n!} + b^{n+1}\int_0^t\d s_1\int_0^{s_1}\d s_2\cdots\int_0^{s_{n+1}}\d s_{n+1} g(s_{n+1}).
\end{align*}
Since $g$ is bounded, we let $0\leq g(t)\leq M$ for all $t\in[0,T]$. Then
\begin{align*}
	a\sum_{k=0}^{n}\frac{(bt)^n}{n!}\leq\cdots\leq a\sum_{k=0}^{n}\frac{(bt)^n}{n!} + \frac{M(bt)^{n+1}}{(n+1)!}.
\end{align*}
Letting $t\to\infty$ produces the desired result.
\end{proof}

The following theorem gives the existence of a solution of SDE in the Lipschitz case.

\paragraph{Theorem 5.23.\label{thm:5.23}} Let functions $\sigma$ and $b$ be continuous on $\mathbb{R}_+\times\mathbb{R}^q$ and Lipschitz in the variable $x$. Then pathwise uniqueness holds for the SDE $E(\sigma,b)$. Furthermore, for every complete filtered $(\Omega,\mathscr{F},(\mathscr{F}_t)_{t\geq 0},\P)$, every $(\mathscr{F}_t)$-Brownian motion $B$ and every $x\in\mathbb{R}^p$, there exists a unique strong solution of $E^x(\sigma,b)$.
\begin{proof}
We prove the case $p=q=1$. The multi-dimensional case is similar. To tackle pathwise uniqueness, we fix the complete filtered probability space $(\Omega,\mathscr{F},(\mathscr{F}_t)_{t\geq 0},\P)$ and the $(\mathscr{F}_t)$-Brownian motion $B$ with $B_0=0$. Let $X$ and $Y$ be two solutions of $E(\sigma,b)$ with $X_0=Y_0\ a.s.$. We fix $M>0$, and set
\begin{align*}
	\tau=\inf\left\{t\geq 0:\vert X_t\vert\vee\vert Y_t\vert\geq M\right\}
\end{align*}
Then $t\mapsto\E[(X_{t\wedge\tau}-Y_{t\wedge\tau})^2]$ is a bounded measurable function. Moreover, for every $t\geq 0$,
\begin{align*}
	X_{t\wedge\tau} = X_0 + \int_0^{t\wedge\tau}\sigma(s,X_s)\,\d B_s+\int_0^{t\wedge\tau}b(s,X_s)\,\d s,
\end{align*}
and a similar formula holds for $Y_{t\wedge\tau}$. We fix $T>0$. For all $t\in[0,T]$, use the bound (\ref{eq:5.6}):
\begin{align*}
	\E\left[\left(X_{t\wedge\tau}-Y_{t\wedge\tau}\right)^2\right]&\leq 2\E\left[\left(\int_0^{t\wedge\tau}\left(\sigma(s,X_s)-\sigma(s,Y_s)\right)\,\d B_s\right)^2\right] + 2\E\left[\left(\int_0^{t\wedge\tau}\left(b(s,X_s)-b(s,Y_s)\right)\,\d s\right)^2\right]\\
	&\leq 2\E\left[\int_0^{t\wedge\tau}\left(\sigma(s,X_s)-\sigma(s,Y_s)\right)^2\d s\right] + 2\E\left[T\int_0^{t\wedge\tau}\left(b(s,X_s)-b(s,Y_s)\right)^2\,\d s\right]\\
	&\leq 2(1+T)L^2\E\left[\int_0^{t\wedge\tau}(X_s-Y_s)^2\,\d s\right]\\
	&\leq 2(1+T)L^2\E\left[\int_0^t(X_{s\wedge\tau}-Y_{s\wedge\tau})^2\,\d s\right].
\end{align*}
By \hyperref[lemma:5.22]{Lemma 5.22}, we have $\E\left[(X_{t\wedge\tau}-Y_{t\wedge\tau})^2\right]=0$, and $X_{t\wedge\tau}=Y_{t\wedge\tau}\ a.s.$ for all $t\in[0,T]$. Let $M\to\infty$ and $T\to\infty$, we then have $X_t=Y_t\ a.s.$ for all $t\geq 0$. The indistinguishability of $X$ and $Y$ then follows from sample-continuity and a density argument.

Next we construct a solution of $E^x(\sigma,b)$ using Picard's approximation. Define by induction:
\begin{align*}
	&X_t^0=x,\quad X_t^1=x+\int_0^t\sigma(s,x)\,\d B_s + \int_0^t b(s,x)\,\d s,\\
	&X_t^n=x+\int_0^t\sigma(s,X_s^{n-1})\,\d B_s + \int_0^t b(s,X_s^{n-1})\,\d s,\quad n\in\mathbb{N}.
\end{align*}
Clearly, $X^n$ is sample-continuous and adapted to the completed canonical filtration of $B$. We fix $T>0$, and find a strong solution on $[0,T]$. Define
\begin{align*}
	g_n(t)=\E\left[\sup_{s\in[0,t]}\vert X_s^n-X_s^{n-1}\vert^2\right],\quad\forall t\in[0,T].
\end{align*}

Since $\sigma(\cdot,x)$ is continuous, the process $\bigl(\int_0^t\sigma(s,x)\,\d B_s\bigr)_{t\geq 0}$ is a continuous local martingale with finite quadratic variation, hence is a martingale by \hyperref[cor:3.59]{Corollary 3.59}. Consequently, we can use Doob's $L^2$-inequality [\hyperref[prop:3.40]{Proposition 3.40 (ii)}] and boundedness of functions $\sigma(\cdot,x)$ and $b(\cdot,x)$ on $[0,T]$ to find some constant $C_T>0$ depending only on $T$ such that $g_1(t)\leq C_T$ for all $t\in[0,T]$.

Now we bound $g_n$ by induction. For any $n\in\mathbb{N}$, one have
\begin{align*}
	X_t^{n+1}-X_t^n=\int_0^t\left(\sigma(s,X_s^n)-\sigma(s,X_s^{n-1})\right)\d B_s + \int_0^t\left(b(s,X_s^n)-b(s,X_s^{n-1})\right)\d s.
\end{align*}

We then use the Burkholder-Davis-Gundy inequality [\hyperref[thm:5.15]{Theorem 5.15}]:
\begin{align*}
	&\E\left[\sup_{s\in[0,t]}(X_s^{n+1}-X_s^n)^2\right]\\
	&\leq 2\E\left[\sup_{s\in[0,t]}\left\vert\int_0^s\left(\sigma(r,X_r^n)-\sigma(r,X_r^{n-1})\right)\d B_r\right\vert^2\right] + 2\E\left[\sup_{s\in[0,t]}\left\vert\int_0^s\left(b(r,X_r^n)-b(r,X_r^{n-1})\right)\d r\right\vert^2\right]\\
	&\leq 2 C_2\E\left[\int_0^t\left(\sigma(r,X_r^n)-\sigma(r,X_r^{n-1})\right)^2\d r\right] + 2\E\left[T\int_0^t \left(b(r,X_r^n)-b(r,X_r^{n-1})\right)^2\d r\right]\\
	&\leq 2(C_2+T)L^2\E\left[\int_0^t(X_r^n-X_r^{n-1})^2\,\d r\right]
\end{align*}
Consequently, we have 
\begin{align*}
	g_{n+1}(t)\leq 2(C_2+T)L^2\int_0^t g_n(s)\,\d s,\quad\forall t\in[0,T].
\end{align*}
Since $g_1(t)\leq C_T$ for all $t\in[0,T]$, an induction gives
\begin{align*}
	g_n(t)\leq C_T\frac{(2(C_2+T)L^2t)^{n-1}}{(n-1)!},\quad \forall t\in[0,T].
\end{align*}
Hence we have
\begin{align*}
	\sum_{n=1}^\infty g_n(T)^{1/2}<\infty \quad\Rightarrow\quad \sum_{n=1}^\infty\sup_{t\in[0,T]}\left\vert X_t^n-X_t^{n-1}\right\vert <\infty\ \ a.s..
\end{align*}
By Weierstrass M-test, the sequence of processes $(X^n_t,t\in[0,T])_{n=1}^\infty$ converges $a.s.$ uniformly to a limiting process $(X_t,t\in[0,T])$, which also has continuous sample paths on $[0,T]$ and is adapted to the completed canonical filtration of $B$. Furthermore, using Lipschitz property of $\sigma(t,\cdot)$ and $b(t,\cdot)$ and dominated convergence theorem for stochastic integrals [\hyperref[thm:5.7]{Theorem 5.7}], the following convergences hold in probability:
\begin{align*}
	\lim_{n\to\infty}\left(\int_0^t\sigma(s,X_s)\,\d B_s-\int_0^t\sigma(s,X_s^n)\,\d B_s\right)&=0,\\
	\lim_{n\to\infty}\left(\int_0^t b(s,X_s)\,\d s-\int_0^t b(s,X_s^n)\,\d s\right)&=0,
\end{align*}
where we use $\sum_{n=1}^\infty\sup_{s\in[0,t]}\left\vert X_s^n-X_s^{n-1}\right\vert$ to dominate the stochastic parts. By passing these limits to the definition of $X_t^n$, we conclude that $X_t$ is a strong solution of $E^x(\sigma,b)$ on $[0,T]$. Let $T\to\infty$, we obtain a process $X=(X_t)_{t\geq 0}$ solving $E^x(\sigma,b)$, and the uniqueness of this strong solution follows from pathwise uniqueness.
\end{proof}

\paragraph{Theorem 5.24.\label{thm:5.24}} Equip both spaces $C(\mathbb{R}_+,\mathbb{R}^p)$ and $C(\mathbb{R}_+,\mathbb{R}^q)$ with the Borel $\sigma$-algebra of the compact convergence topology, and complete this $\sigma$-algebra on $C(\mathbb{R}_+,\mathbb{R}^p)$ by $W$-negligible sets, where $W$ is the Wiener measure. Under the assumptions of the preceding theorem, there exists a measurable mapping $F_x:C(\mathbb{R}_+,\mathbb{R}^q)\to C(\mathbb{R}_+,\mathbb{R}^p)$ such that
\begin{itemize}
	\item[(i)] for every $t\geq 0$, the mapping $\bfw\mapsto F_x(\bfw)_t$ coincides $W$-$a.s.$ with a measurable function of $(\bfw(r))_{0\leq r\leq t}$;
	\item[(ii)] for every $\bfw\in C(\mathbb{R}_+,\mathbb{R}^q)$, the mapping $x\mapsto F_x(\bfw)$ is continuous;
	\item[(iii)] for every $t\geq 0$, and for every choice of the complete filtered probability space $(\Omega,\mathscr{F},(\mathscr{F}_t)_{t\geq 0},\P)$ and of the $(\mathscr{F}_t)$-Brownian motion $B$ with $B_0=0$, the process $X_t=F_x(B)_t$ is the unique solution of $E^x(\sigma,b)$; furthermore, if $U$ is an $\mathscr{F}_0$-measurable $\mathbb{R}^p$-valued random variable, the process $F_U(B)_t$ is the unique solution of $E(\sigma,b)$ with $X_0=U$.
\end{itemize}
\begin{proof}
\textit{Step I:} For brevity, we only consider the case $p=q=1$. We let $\mathscr{G}_t$ be the $\sigma$-algebra generated by projection mappings $\{\pi_s:0\leq s\leq t\}$ and all $W$-negligible sets in $C(\mathbb{R}_+,\mathbb{R})$, so $(\mathscr{G}_t)_{t\geq 0}$ is a complete filtration. By \hyperref[thm:5.23]{Theorem 5.23}, with the filtered probability space $(C(\mathbb{R}_+,\mathbb{R}),\mathscr{G}_\infty,(\mathscr{G}_t)_{t\geq 0},W)$ fixed, and with the Brownian motion $B_t(\bfw)=\bfw(t)$ fixed as the canonical process, there exists a unique (up to indistinguishability) and strong solution $X^x=(X_t^x)_{t\geq 0}$ of $E^x(\sigma,b)$ for every $x\in\mathbb{R}$. \vspace{0.1cm}

\textit{Step II:} Let $d$ be a metric on $C(\mathbb{R}^+,\mathbb{R})$ that induces the compact convergence topology. We fix $q\geq 2$, and prove that there exists a constant $C_q^*>0$ depending only on $q$, such that for all $x,y\in\mathbb{R}$, 
\begin{align*}
	\E\left[d(X^x,X^y)^q\right]\leq C_p^*\vert x-y\vert^q.\tag{5.14}\label{eq:5.14}
\end{align*}
Then by Kolmogorov's lemma [\hyperref[thm:4.6]{Theorem 4.6}] applied to the process $(X^x)_{x\in\mathbb{R}}$ taking values in $C(\mathbb{R}_+,\mathbb{R})$, we find a modification $(\widetilde{X}^x)_{x\in\mathbb{R}}$ of $(X^x)_{x\in\mathbb{R}}$ with continuous sample paths. Define $F_x(\bfw)=\widetilde{X}^x(\bfw)=(\widetilde{X}^x_t(\bfw))_{t\geq 0}$. Then $F_x:(C(\mathbb{R}_+,\mathbb{R}),\mathscr{G}_\infty)\to (C(\mathbb{R}_+,\mathbb{R}),\mathscr{G}_\infty)$ is a measurable mapping with property (ii).

To this end, we define the stopping time
\begin{align*}
	\tau_n=\inf\left\{t\geq 0:\vert X_t^x\vert\vee\vert X_t^y\vert\geq n\right\},\quad n=1,2,\cdots.
\end{align*} 
We fix some $T\geq 1$. For every $t\in[0,T]$, we apply Jensen's inequality, Burkholder-Davis-Gundy inequality [\hyperref[thm:5.15]{Theorem 5.15}], Hölder's inequality and Lipschitz property as follows:
\begin{align*}
	&\E\left[\sup_{s\in[0,t]}\left\vert X_{s\wedge\tau_n}^y-X_{s\wedge\tau_n}^y\right\vert^q\right]\\
	&\leq 3^{q-1}\left(\vert x-y\vert^q+ \E\left[\sup_{s\in[0,t]}\left\vert\int_0^{s\wedge\tau_n}\left(\sigma(r,X_r^x)-\sigma(r,X_r^y)\right)\d B_r\right\vert^q +\sup_{s\in[0,t]}\left\vert\int_0^{s\wedge\tau_n}\left(b(r,X_r^x)-b(r,X_r^y)\right)\d r\right\vert^q\right]\right)\\
	&\leq 3^{q-1}\left(\vert x-y\vert^q+ C_q\E\left[\left(\int_0^{t\wedge\tau_n}\left(\sigma(r,X_r^x)-\sigma(r,X_r^y)\right)^2\d r\right)^{q/2}\right] + \E\left[\left(\int_0^{t\wedge\tau_n}\left\vert b(r,X_r^x)-b(r,X_r^y)\right\vert\d r\right)^q\right]\right)\\	
	&\leq 3^{q-1}\biggl(\vert x-y\vert^q+ C_qt^{\frac{q}{2}-1}\E\left[\int_0^t\left\vert\sigma(r\wedge\tau_n,X_{r\wedge\tau_n}^x)-\sigma(r\wedge\tau_n,X_{r\wedge\tau_n}^y)\right\vert^q\d r\right]\\
	&\qquad\qquad + t^{q-1}\E\left[\int_0^t\left\vert b(r\wedge\tau_n,X_{r\wedge\tau_n}^x)-b(r\wedge\tau_n,X_{r\wedge\tau_n}^y)\right\vert^q\d r\right]\biggr)\\
	&\leq 3^{q-1}\left(\vert x-y\vert^q + K^qT^{\frac{q}{2}-1}\left(C_q+T^{q/2}\right)\E\left[\int_0^t\left\vert X_{r\wedge\tau_n}^x-X_{r\wedge\tau_n}^y\right\vert^q\d r\right]\right)
\end{align*}
We let $C_q^\prime=3^{q-1}K^q(1+C_q)$. Then we obtain the following estimate by using Gronwall's lemma [\hyperref[lemma:5.22]{Lemma 5.22}] on the bounded function $t\mapsto\E\left[\sup_{s\in[0,t]}\left\vert X_{s\wedge\tau_n}^y-X_{s\wedge\tau_n}^y\right\vert^q\right]$:
\begin{align*}
	\E\biggl[\sup_{s\in[0,t]}\left\vert X_{s\wedge\tau_n}^y-X_{s\wedge\tau_n}^y\right\vert^q\biggr]\leq C_q^\prime\vert x-y\vert^q\exp\left(C_q^\prime T^{q-1}t\right),\quad\forall t\in[0,T].
\end{align*}
A monotone convergence argument follows by letting $n\to\infty$:
\begin{align*}
	\E\biggl[\sup_{s\in[0,t]}\left\vert X_s^y-X_s^y\right\vert^q\biggr]\leq C_q^\prime\vert x-y\vert^q\exp\left(C_q^\prime t^q\right),\quad\forall t\geq 0.
\end{align*}
We define the following metric $d$ on $C(\mathbb{R}_+,\mathbb{R})$, which induces the uniform topology:
\begin{align*}
	d(\bfw,\bfw^\prime) = \sum_{k=1}^\infty\alpha_k\biggl(\sup_{s\in[0,k]}\vert\bfw(s)-\bfw^\prime(s)\vert\wedge 1\biggr),
\end{align*}
where $\alpha_k>0$ is a real sequence such that $\sum_{k=1}^\infty\alpha_k$ converges. Choose $(\alpha_k)$ such that $\sum_{k=1}^\infty\alpha_k\exp(C_q^\prime k^q)<\infty$. Again by Hölder's inequality, one have
\begin{align*}
	\E\left[d(X_s^x,X_s^y)^q\right]\leq\left(\sum_{k=1}^\infty\alpha_k\right)^{q-1}\sum_{k=1}^\infty\alpha_k\E\left[\sup_{s\in[0,k]}\left\vert X_s^x-X_s^y\right\vert^q\right]\leq C_q^*\vert x-y\vert^q,
\end{align*}
where $C_q^*=C_q^\prime\left(\sum_{k=1}^\infty\alpha_k\right)^{q-1}\left(\sum_{k=1}^\infty\alpha_k\exp(C_q^\prime k^q)\right)$. This complete the proof of (\ref{eq:6.15}). For the assertion (i), we point out that for any $t\geq 0$, the mapping $\bfw\mapsto F_x(\bfw)_t=\widetilde{X}_t^x\overset{a.s.}{=}X_t^x$ is $\mathscr{G}_t$-measurable. The result then follows from Doob-Dynkin theorem [\hyperref[thm:2.22]{Theorem 2.22}].
\vspace{0.1cm}

\textit{Step III:} We prove the first part of assertion (iii). Fix a complete filtered probability space $(\Omega,\mathscr{F},(\mathscr{F}_t)_{t\geq 0},\P)$ and an $(\mathscr{F}_t)$-Brownian motion $B$. Clearly, the process $F_x(B)=(F_x(B)_t)_{t\geq 0}$ has continuous paths, and is also adapted since $F_x(B)_t$ coincide $a.s.$ with a measurable function of $(B_s)_{s\in[0,t]}$ by (i), and since the filtration $(\mathscr{F}_t)_{t\geq 0}$ is complete. Then it remains to show that $F_x(B)$ solves $E^x(\sigma,b)$.

By construction of $F_x$ and the fact that $X^x=\widetilde{X}^x$ $W$-$a.s.$, for all $t\geq 0$, we have
\begin{align*}
	F_x(\bfw)_t = x+\int_0^t\sigma(s,F_x(\bfw)_s)\,\d\bfw(s) + \int_0^t b(s,F_x(\bfw)_s)\,\d s,\quad for\ W\textit{-}a.s.\ \bfw\in C(\mathbb{R}_+,\mathbb{R}).
\end{align*}
By \hyperref[prop:5.8]{Proposition 5.8}, we have the following approximation:
\begin{align*}
	\int_0^t\sigma(s,F_x(\bfw)_s)\,\d\bfw(s) = \lim_{n\to\infty}\sum_{j=1}^{2^n}\sigma\left(\frac{(j-1)t}{2^n},F_x\left(\bfw\right)_{\frac{(j-1)t}{2^n}}\right)\left(\bfw\left(\frac{jt}{2^n}\right)-\bfw\left(\frac{(j-1)t}{2^n}\right)\right)\tag{5.15}\label{eq:5.15}
\end{align*}
in probability $W(\d\bfw)$. Since $W$ is the law of $B$, by \hyperref[prop:5.8]{Proposition 5.8}, we have
\begin{align*}
	F_x(B)_s &= x + \lim_{n\to\infty}\sum_{j=1}^{2^n}\sigma\left(\frac{(j-1)t}{2^n},F_x(B)_{\frac{(j-1)t}{2^n}}\right)\left(B_{\frac{jt}{2^n}}-B_{\frac{(j-1)t}{2^n}}\right) +  \int_0^t b(s,F_x(B)_s)\,\d s\\
	&= x + \int_0^t\sigma(s,F_x(B)_s)\,\d B_s + \int_0^t b(s,F_x(B)_s)\,\d s,\quad a.s.,
\end{align*}
where the $a.s.$ convergence follows by passing the convergence in probability to an appropriate subsequence. Therefore, $F_x(B)$ is the desired solution of $E^x(\sigma,b)$.\vspace{0.1cm}

\textit{Step IV:} We prove the first part of assertion (iii). The mapping $x\mapsto F_x(B)_t(\omega)$ is continuous for any fixed $\omega\in\Omega$, and the mapping $\omega\mapsto F_x(B)_t(\omega)$ is $\mathscr{F}_t$-measurable for any fixed $\omega\in\Omega$. Then $(x,\omega)\mapsto F_x(B)_t(\omega)$ is $\mathscr{B}(\mathbb{R})\otimes\mathscr{F}_t$-measurable according to a similar procedure in \hyperref[prop:3.9]{Proposition 3.9}. If $U$ is a $\mathscr{F}_0$-measurable random variable, then $F_U(B)_t$ is a composition of $\omega\mapsto(U(\omega),\omega)$ and $(x,\omega)\to F_x(B)_t(\omega)$, hence is $\mathscr{F}_t$-measurable.

Let $H(x,\bfw)_t=F_x(\bfw)_t-x-\int_0^t b(s,F_x(\bfw)_s)\,\d s$. We use the convergence result (\ref{eq:5.15}) in probability $W$. Since $B\sim W$, and $U$ is a $\mathscr{F}_0$-measurable variable, which is independent of $B$, we have that
\begin{align*}
	H(U,B)_t=\sum_{j=1}^{2^n}\sigma\left(\frac{(j-1)t}{2^n},F_U(B)_{\frac{(j-1)t}{2^n}}\right)\left(B_{\frac{jt}{2^n}}-B_{\frac{(j-1)t}{2^n}}\right),
\end{align*}
where the series converges in probability, and the limit is the stochastic integral $\int_0^t\sigma(s,F_U(B)_s)\,\d B_s$. Hence
\begin{align*}
	F_U(B)_t - U - \int_0^t b(s,F_U(B))\,\d s = \int_0^t\sigma(s,F_U(B)_s)\,\d B_s.
\end{align*}
Consequently, $F_U(B)=(F_U(B))_{t\geq 0}$ solves the SDE $E(\sigma,b)$ with initial value $U$.
\end{proof}

\subsection{Girsanov's Theorem and Cameron–Martin Formula}
In Section 5.2, we show that the class of continuous semimartingales is invariant under composition with $C^2$-function. In this section, we study the effect on the class of continuous semimartingales of an absolutely continuous transformation of probability measures. We consider two probability measures $\bbP$ and $\bbQ$ on the same measurable space $(\Omega,\mathscr{F})$. To avoid confusion, we write $\E_\bbP$ and $\E_\bbQ$ for the expectation under $\bbP$ and $\bbQ$, respectively. Unless otherwise specified, our notions of semimartingales refer to the underlying probability measure $\bbP$. We will point it out explicitly when consider these notions under $\bbQ$.

Throughout this section, we assume $(\mathscr{F}_t)_{t\geq 0}$ is a complete and right continuous filtration. Most of the time we may assume that $\bbP$ and $\bbQ$ are mutually absolutely continuous, hence the filtration $(\mathscr{F}_t)_{t\geq 0}$, being complete with respect to $\bbP$, is also complete with respect to $\bbQ$.

\paragraph{Proposition 5.25.\label{prop:5.25}} Assume that $\bbQ$ is a probability measure on $(\Omega,\mathscr{F}_\infty)$ which is absolutely continuous with respect to $\bbP$ on $\mathscr{F}_t$ for every $t\geq 0$. Let $D_t$ be the Radon-Nikodym derivative of $\bbQ$ with respect to $\bbP$ on $\mathscr{F}_t$:
\begin{align*}
	D_t = \left.\frac{\d\bbQ}{\d\bbP}\right|_{\mathscr{F}_t},\quad t\in\mathbb{R}_+.
\end{align*}
Then $D=(D_t)_{t\geq 0}$ is a $\P$-martingale, and $D$ has a càdlàg modification as a consequence of \hyperref[thm:3.44]{Theorem 3.44}. Furthermore, the following two assertions are equivalent:
\begin{itemize}
	\item[(i)] the martingale $D$ is uniformly integrable;
	\item[(ii)] $\bbQ\ll\bbP$ on $\mathscr{F}_\infty$.
\end{itemize}
\begin{proof}
We fix $t>s\geq 0$. Then for all $A\in\mathscr{F}_s$, we have $\bbQ(A) = \E_\bbP[\mathds{1}_A D_s]$. For the martingale property,
\begin{align*}
	\E_\bbP\left[D_s\mathds{1}_A\right]=\E_\bbP\left[D_t\mathds{1}_A\right]=\bbQ(A),\quad\forall A\in\mathscr{F}_s\subset\mathscr{F}_t.
\end{align*}
Hence we have $D_s=\E[D_t|\mathscr{F}_s]$, and $D$ is a martingale. If (i) holds, let $Z$ be the $a.s.$ and $L^1$ limit of $(D_t)_{t\geq 0}$. Using a monotone class argument, we have $\E_\bbP[Z\mathds{1}_A]=\bbQ(A)$ for all $A\in\mathscr{F}_\infty$, hence $\bbQ\ll\bbP$ on $\mathscr{F}_\infty$. If (ii) holds, let $D_\infty$ be the Radon-Nikodym derivative of $\bbQ$ with respect to $\bbP$ on $\mathscr{F}_t$. Then we have $\E[D_\infty|\mathscr{F}_t]=D_t$ for all $t\geq 0$, implying uniform integrability.
\end{proof}

In the sequel, we assume that the martingale $D=(D_t)_{t\geq 0}$ has càdlàg sample paths.

\paragraph{Proposition 5.26.\label{prop:5.26}} Under the assumption of the preceding proposition, for every stopping time $\tau$, we have $\d\bbQ = D_\tau\d\bbP$ on $\mathscr{F}_\tau\cap\{\tau<\infty\}$. Furthermore, if $\bbQ\ll\bbP$ on $\mathscr{F}_\infty$, we have 
\begin{align*}
	D_\tau = \left.\frac{\d\bbQ}{\d\bbP}\right|_{\mathscr{F}_\tau}
\end{align*}
\begin{proof}
For the uniform integrable case where $\bbQ\ll\bbP$ on $\mathscr{F}_\infty$, by optional stopping theorem [\hyperref[thm:3.49]{Theorem 3.49}],
\begin{align*}
	\bbQ(A)=\E_\bbP[D_\infty\mathds{1}_A]=\E_\bbP\left[\E_\bbP[D_\infty|\mathscr{F}_\tau]\mathds{1}_A\right]=\E_\bbP\left[D_\tau\mathds{1}_A\right],\quad\forall A\in\mathscr{F}_\tau.
\end{align*}
Since $D_\tau$ is $\mathscr{F}_\tau$-measurable, the second assertion follows. For general case, we use the fact that the stopped martingale $(D_{s\wedge t})_{s\geq 0}$ is uniformly integrable for every $t\geq 0$. Then
\begin{align*}
	\bbQ(A\cap\{\tau\leq t\})=\E_\bbP\left[D_{\tau\wedge t}\mathds{1}_{A\cap\{\tau\leq t\}}\right]=\E_\bbP[D_\tau\mathds{1}_{A\cap\{\tau\leq t\}}],\quad\forall A\in\mathscr{F}_{\tau}.
\end{align*}
Letting $t$ tends to infinity concludes the proof. 
\end{proof}
\paragraph{Remark.} Let $X=(X_t)_{\geq 0}$ be an adaptive process with càdlàg sample paths. If $XD$ is a $\P$-local martingale, then $X$ is a $\bbQ$-local martingale. We give a clarification below.

Let $\tau$ be a stopping time. Then for any $s\geq 0$ and any $A\in\mathscr{F}_s$, we have $A\cap\{\tau>s\}\in\mathscr{F}_s\cap\mathscr{F}_\tau=\mathscr{F}_{\tau\wedge s}$.  Let $t>s\geq 0$. If the stopped process $(XD)^\tau$ is a $\bbP$-martingale, then
\begin{align*}
	\E_\bbQ[X_{t\wedge\tau}\mathds{1}_A]&=\E_\bbQ[X_{t\wedge\tau}\mathds{1}_{A\cap\{\tau\leq s\}}]+\E_\bbQ[X_{t\wedge\tau}\mathds{1}_{A\cap\{\tau>s\}}]\\
	&=\E_\bbQ[X_{s\wedge\tau}\mathds{1}_{A\cap\{\tau\leq s\}}]+\E_\bbP[X_{t\wedge\tau}D_{t\wedge\tau}\mathds{1}_{A\cap\{\tau>s\}}]\\
	&=\E_\bbQ[X_{s\wedge\tau}\mathds{1}_{A\cap\{\tau\leq s\}}]+\E_\bbP[X_{s\wedge\tau}D_{s\wedge\tau}\mathds{1}_{A\cap\{\tau>s\}}]=\E_\bbQ[X_{s\wedge\tau}\mathds{1}_A].
\end{align*}
Hence $X^\tau$ is a $\bbQ$-martingale. In addition, a sequence of stopping times increasing $\bbP$-$a.s.$ to $\infty$ also increases $\bbQ$-$a.s.$ to $\infty$. Consequently, if $XD$ is a $\P$-local martingale, then $X$ is a $\bbQ$-local martingale.

\paragraph{Proposition 5.27.\label{prop:5.27}} Under the preceding assumption, the martingale $D$ is $\bbQ$-$a.s.$ strictly positive, i.e.
\begin{align*}
	\inf_{t\geq 0}D_t >0,\quad\ \bbQ\textit{-}a.s..
\end{align*}
\begin{proof}
For every $n\in\mathbb{N}$, define the stopping time $\tau_n=\inf\{t\geq 0:D_t<1/n\}$. Then the event $\{\tau_n<\infty\}$ is $\mathscr{F}_{\tau_n}$-measurable, and $D_{\tau_n}\leq 1/n$ on $\{\tau_n<\infty\}$ by right-continuity of $D$. Hence
\begin{align*}
	\bbQ(\tau_n<\infty)=\E_\bbP\left[D_{\tau_n}\mathds{1}_{\{\tau_n<\infty\}}\right]\leq\frac{1}{n}\quad\Rightarrow\quad\bbQ\left(\bigcap_{n=1}^\infty\{\tau_n<\infty\}\right)=0.
\end{align*}
Then $\mathbb{Q}$-$a.s.$ there exists $n\in\mathbb{N}$ such that $\tau_n=\infty$. This complete the proof.
\end{proof}
\paragraph{Remark.} If we further assume that $\bbP$ and $\bbQ$ are mutually absolutely continuous, then the martingale $D$ is also $\bbP$-$a.s.$ strictly positive.

\paragraph{Theorem 5.28\label{thm:5.28}} (Girsanov). Suppose the assumption of the preceding proposition holds, and assume that the martingale $D=(D_t)_{t\geq 0}$ is sample-continuous. If $M=(M_t)_{t\geq 0}$ is a continuous $\P$-local martingale, then
\begin{align*}
	\widetilde{M} = M - D^{-1}\cdot \langle M,D\rangle
\end{align*}
is a continuous $\bbQ$-local martingale. Furthermore, if $N$ is another continuous $\P$-local martingale, then
\begin{align*}
	\langle M,N\rangle = \langle\widetilde{M},N\rangle = \langle\widetilde{M},\widetilde{N}\rangle.
\end{align*}
\begin{proof}
By \hyperref[prop:5.27]{Proposition 5.27}, the process $D^{-1}\cdot\langle M,D\rangle$ is $\P$-$a.s.$ of finite variation, and the process $\widetilde{M}$ is a $\P$-semimartingale. According to the integration by parts formula, we have
\begin{align*}
	(\widetilde{M}D)_t &= \widetilde{M}_0D_0 + \int_0^t \widetilde{M}_s\,\d D_s +\int_0^t D_s\,\d\widetilde{M}_s + \langle\widetilde{M},D\rangle_t\\
	&=\widetilde{M}_0D_0 + \int_0^t \widetilde{M}_s\,\d D_s +\int_0^tD_s\,\d M_s - \langle M,D\rangle_t + \langle\widetilde{M},D\rangle_t\\
	&=\widetilde{M}_0D_0 + \int_0^t \widetilde{M}_s\,\d D_s +\int_0^t D_s\,\d M_s.
\end{align*}
Consequently, the process $\widetilde{M}D$ is a continuous $\bbP$-local martingale. By the Remark under \hyperref[prop:5.26]{Proposition 5.26}, the process $\widetilde{M}$ is a continuous $\bbQ$-local martingale. The last assertion holds because the bracket of a finite variation process and a semimartingale vanishes.
\end{proof}

\paragraph{Proposition 5.29.\label{prop:5.29}} If $D$ is a continuous local martingale taking strictly positive values. There exists a unique continuous local martingale $L$ such that
\begin{align*}
D_t=\mathscr{E}(L)_t = \exp\left(L_t-\frac{1}{2}\langle L,L\rangle_t\right).
\end{align*}
Moreover, $L$ is given by the formula
\begin{align*}
	L_t=\log D_0+\int_0^t D_s^{-1}\,\d D_s.\tag{5.16}\label{eq:5.16}
\end{align*}
\begin{proof} 
We first prove uniqueness: If both $L$ and $L^\prime$ has the desired property, then $L-L^\prime=\frac{1}{2}\langle L^\prime,L^\prime\rangle - \frac{1}{2}\langle L,L\rangle$ is a continuous martingale of finite variation, hence is constantly zero [\hyperref[prop:3.55]{Proposition 3.55}]. To show the second assertion, use Itô's formula to the process $\log D$:
\begin{align*}
	\log D_t = \log D_0 + \int_0^t D_s^{-1}\,\d D_s - \frac{1}{2}\int_0^t D_s^{-2}\,\d\langle D,D\rangle_s = L_t-\frac{1}{2}\langle L,L\rangle_t,
\end{align*}
where $L$ is given in (\ref{eq:5.16}).
\end{proof}

\paragraph{Theorem 5.30\label{thm:5.30}} (Girsanov). Suppose the assumption of the preceding proposition holds, and assume that the martingale $D=(D_t)_{t\geq 0}$ is sample-continuous. If $M=(M_t)_{t\geq 0}$ is a continuous $\P$-local martingale, then
\begin{align*}
	\widetilde{M} = M - D^{-1}\cdot \langle M,D\rangle = M - \langle M,L\rangle.
\end{align*}
is a continuous $\bbQ$-local martingale, where $L$ is given in (\ref{eq:5.16}). Moreover, $D^{-1}=\mathscr{E}(-\widetilde{L})$.
\begin{proof}
The first identity immediately follows from associativity of stochastic integral. For the second assertion, we use the identity $\widetilde{L}=L-\langle L,L\rangle$:
\begin{align*}
	\mathscr{E}(-\widetilde{L})_t=\exp\left(-\widetilde{L}_t-\frac{1}{2}\langle \widetilde{L},\widetilde{L}\rangle_t\right)=\exp\left(-L_t+\frac{1}{2}\langle L,L\rangle_t\right)=\mathscr{E}(L)_t^{-1}.
\end{align*}
This proves the second assertion.
\end{proof}
\paragraph{Remark I.} According to \hyperref[thm:5.30]{Theorem 5.30}, if $P$ and $Q$ are mutually absolutely continuous on $\mathscr{F}_\infty$, then the role of $P$ and $Q$ can be exchanged by replacing $L$ with $-\widetilde{L}$.

\paragraph{Remark II.} If $M=B$ is an $(\mathscr{F}_t)$-Brownian motion under $P$, then $\widetilde{B}=B-\langle B,L\rangle$ is a $\bbQ$-continuous local martingale, and $\langle\widetilde{B},\widetilde{B}\rangle_t=\langle B,B\rangle_t=t$. By Lévy's characterization of Brownian motions [\hyperref[thm:5.11]{Theorem 5.11}], $\widetilde{B}$ is an $(\mathscr{F}_t)$-Brownian motion under $\bbQ$.

\paragraph{Theorem 5.31\label{thm:5.31}} Let $L$ be a continuous local martingale such that $L_0=0$. Consider the following properties:
\begin{itemize}
	\item[(i)] (Novikov's criterion). $\E\left[\exp\left(\frac{1}{2}\langle L,L\rangle_\infty\right)\right]<\infty$.
	\item[(ii)] (Kazamaki's criterion). $L$ is a uniformly integrable martingale, and $\E\left[\exp\left(\frac{1}{2}L_\infty\right)\right]<\infty$.
	\item[(iii)] $\mathscr{E}(L)$ is a uniformly integrable martingale.
\end{itemize}
Then (i) $\Rightarrow$ (ii) $\Rightarrow$ (iii).
\begin{proof}
(i) $\Rightarrow$ (ii): The process $\mathscr{E}(L)$ is a nonnegative continuous local martingale, hence is a supermartingale by \hyperref[prop:3.54]{Theorem 3.54 (i)}. By Fatou's lemma, $$\E[\mathscr{E}(L)_\infty]\leq\lim_{t\to\infty}\E[\mathscr{E}(L)_t]\leq\E[\mathscr{E}(L)_0]=1.$$ 

By property (i), we have $\E[\langle L,L\rangle_\infty]<\infty$, and $L$ is a continuous martingale that is bounded in $L^2$ by \hyperref[thm:3.48]{Theorem 3.48}. By Cauchy-Schwarz inequality,
\begin{align*}
	\E\left[\exp\left(\frac{1}{2}L_\infty\right)\right] = \E\left[\mathscr{E}(L)_\infty^{1/2}\e^{\frac{1}{4}\langle L,L\rangle_\infty}\right]\leq\sqrt{\E\left[\mathscr{E}(L)_\infty\right]}\sqrt{\E\left[\e^{\frac{1}{2}\langle L,L\rangle_\infty}\right]}\leq \sqrt{\E\left[\e^{\frac{1}{2}\langle L,L\rangle_\infty}\right]}<\infty.
\end{align*}

(ii) $\Rightarrow$ (iii): If $L$ is a uniformly integrable martingale, by optional stopping theorem, for any stopping time $\tau$, one have $L_\tau=\E\left[L_\infty|\mathscr{F}_\tau\right]$. By Jensen's inequality,
\begin{align*}
	\E\left[\exp\left(\frac{1}{2}L_\tau\right)\right]\leq\E\left[\exp\left(\frac{1}{2}L_\infty\right)|\mathscr{F}_\tau\right].
\end{align*}
Since $\exp\left(\frac{1}{2}L_\infty\right)$ is integrable, the collection of random variables $\exp\left(\frac{1}{2}L_\tau\right)$ is uniformly integrable, where $\tau$ runs over all stopping times. On the other hand, set $Z^{(a)}_t=\exp\left(\frac{aL_t}{1+a}\right)$. Then for all $0<a<1$,
\begin{align*}
	\mathscr{E}(aL)_t = \left(\mathscr{E}(L)_t\right)^{a^2}(Z_t^{(a)})^{1-a^2}.
\end{align*}
By Hölder's inequality, for any measurable set $\Gamma\in\mathscr{F}$ and any stopping time $\tau$, one have
\begin{align*}
	\E\left[\mathds{1}_\Gamma\mathscr{E}(aL)_\tau\right] \leq \E\left[\mathscr{E}(L)_\tau\right]^{a^2}\E\left[\mathds{1}_\Gamma Z^{(a)}_\tau\right]^{1-a^2}\leq\E\left[\mathds{1}_\Gamma Z^{(a)}_\tau\right]^{1-a^2}\leq\E\left[\mathds{1}_\Gamma\exp\left(\frac{1}{2}L_\tau\right)\right]^{2a(1-a)},
\end{align*}
where we also use Jensen's inequality and the fact $\frac{1+a}{2a}>1$ in the last inequality. Consequently, the collection of random variables $\mathscr{E}(aL)_\tau$ is uniformly integrable, where $\tau$ runs over all stopping times. Let $\tau_n\wedge\infty$ be a sequence of stopping times reducing $\mathscr{E}(aL)$. Then for all $t>s\geq 0$, by uniform integrability,
\begin{align*}
	\E[\mathscr{E}(aL)_t|\mathscr{F}_s]=\lim_{n\to\infty}\E\left[\mathscr{E}(aL)_{t\wedge\tau_n}|\mathscr{F}_s\right]=\lim_{n\to\infty}\mathscr{E}(aL)_{s\wedge\tau_n}=\mathscr{E}(aL)_s.
\end{align*}
Hence $\mathscr{E}(aL)$ is a uniformly integrable martingale, and
\begin{align*}
	1=\E[\mathscr{E}(aL)_\infty]\leq \E\left[\mathscr{E}(L)_\infty\right]^{a^2}\E\left[Z^{(a)}_\infty\right]^{1-a^2}\leq\E\left[\mathscr{E}(L)_\infty\right]^{a^2}\E\left[\exp\left(\frac{1}{2}L_\infty\right)\right]^{2a(1-a)}\quad\Rightarrow\quad \E\left[\mathscr{E}(L)_\infty\right]=1.
\end{align*}
Again, by Fatou's lemma, $\E[\mathscr{E}(L)_\infty|\mathscr{F}_t]\leq\mathscr{E}(L)_t$. On the other hand, we have $\E[\mathscr{E}(L)_\infty]=\E[\mathscr{E}(L)_t]=\E[\mathscr{E}(L)_0]=1$. Hence $\E[\mathscr{E}(L)_\infty|\mathscr{F}_t]=\mathscr{E}(L)_t$, and $\mathscr{E}(L)$ is a uniformly integrable martingale.
\end{proof}

\paragraph{Remark.} Let $L$ be a continuous $\P$-local martingale satisfying property (ii). To apply Girsanov's theorem, we let $\bbQ$ be the probability measure with density $\mathscr{E}(L)_\infty$ with respect to $\bbP$. According to \hyperref[prop:5.25]{Proposition 5.25}, the Radon-Nikodym derivative is $\frac{\d\bbQ}{\d\bbP}|_{\mathscr{F}_t}=D_t=\mathscr{E}(L)_t$.

\paragraph{Girsanov transformation and SDE.} Let $\beta=(\beta_t)_{t\geq 0}$ be a $p$-dimensional $(\mathscr{F}_t)$-Brownian motion under $\bbP$. Consider the following continuous local martingale:
\begin{align*}
	L_t = \int_0^t b(s,\beta_s)\,\d\beta_s.
\end{align*}
If there exists $g\in L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$ such that $\vert b(t,x)\vert\leq g(t)$ for all $(t,x)\in\mathbb{R}_+\times\mathbb{R}^p$, the Novikov's criterion is satisfied. Then the associated exponential martingale is given by
\begin{align*}
	D_t=\mathscr{E}(L)_t = \exp\left(\int_0^t b(s,\beta_s)\,\d\beta_s-\frac{1}{2}\int_0^t\vert b(s,\beta_s)\vert^2\,\d s\right).
\end{align*}
We set $\d\bbQ = D_\infty\d\bbP$, which is a probability distribution. By Remark II under \hyperref[thm:5.30]{Theorem 5.30}, the following process $B$ is an $(\mathscr{F}_t)$-Brownian motion under $\bbQ$:
\begin{align*}
	B_t = \beta_t - \int_0^t b(s,\beta_s)\,\d s
\end{align*}
Consequently, $X=\beta$ solves the following SDE under probability measure $\bbQ$:
\begin{align*}
	\d X_t = \d B_t + b(t,X_t)\,\d t
\end{align*}
Here we only assume that $b:\mathbb{R}_+\times\mathbb{R}^p\to\mathbb{R}^p$ is dominated by a function $g\in L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$.

\paragraph{Proposition 5.32.\label{prop:5.32}} Consider the following two SDEs admitting unique strong solution on $\mathbb{R}_+$:
\begin{align*}
	\begin{cases}
		\d X_t = \mu(t,X_t)\,\d t + \sigma(t,X_t)\,\d B_t,\\
		\d Y_t = (\mu+\nu)(t,Y_t)\,\d t + \sigma(t,Y_t)\,\d B_t,
	\end{cases}
\end{align*}
where $B_t$ is a $p$-dimensional $(\mathscr{F}_t)$-Brownian motion under $\bbP$, and $\sigma:\mathbb{R}_+\times\mathbb{R}^p\to\mathbb{R}^{p\times p}$ is almost everywhere invertible. Furthermore, the \textit{Novikov's condition} is satisfied:
\begin{align*}
	\E\left[\exp\left(\frac{1}{2}\int_0^\infty\vert\sigma(s,Y_s)^{-1}\nu(s,Y_s)\vert^2\,\d s\right)\right]<\infty.
\end{align*}
Then, if $X_0\overset{d}{=}Y_0$, the following identity holds for all bounded functional $\Phi:C(\mathbb{R}_+)\to\mathbb{R}$:
\begin{align*}
	\E\left[\Phi(X)\right]=\E\left[\Phi(Y)\exp\left(-\int_0^\infty\sigma(s,Y_s)^{-1}\nu(s,Y_s)\,\d s-\frac{1}{2}\int_0^\infty\vert\sigma(s,Y_s)^{-1}\nu(s,Y_s)\vert^2\,\d B_s\right)\right].
\end{align*}
\begin{proof}
Define the following continuous $\P$-local martingale:
\begin{align*}
	L_t = -\int_0^t\sigma(s,Y_s)^{-1}\nu(s,Y_s)\,\d B_s.
\end{align*}
Since the Novikov's criterion is satisfied, we can use the exponential martingale:
\begin{align*}
	\mathscr{E}(L)_t = \exp\left(-\int_0^t\sigma(s,Y_s)^{-1}\nu(s,Y_s)\,\d s-\frac{1}{2}\int_0^t\vert\sigma(s,Y_s)^{-1}\nu(s,Y_s)\vert^2\,\d B_s\right).
\end{align*}
Define $\d\bbQ = \mathscr{E}(L)_\infty\,\d\bbP$, and the $(\mathscr{F}_t)$-Brownian motion under $\bbP$:
\begin{align*}
\widetilde{B}_t = B_t - \langle B,L\rangle_t = B_t + \int_0^t\sigma(s,Y_s)^{-1}\nu(s,Y_s)\,\d s.
\end{align*}
Then we have $\d Y_t = \mu(t,Y_t)\,\d t + \sigma(t,Y_t)\,\d\widetilde{B}_t$. Consequently, $Y_t$ solves the first SDE under probability measure $\bbQ$ and $(\mathscr{F}_t,\bbQ)$-Brownian motion $\widetilde{B}$. By uniqueness of the strong solution, if $X_0\overset{d}{=}Y_0$, then $X$ and $Y$ has the same law under $\bbP$ and $\bbQ$, respectively. The final result follows by $\E_\bbP\left[\Phi(X)\right]=\E_\bbQ\left[\Phi(Y)\right]$.
\end{proof}

We specialize the previous discussion to the case $b(t,x)=g(t)$, where $g\in L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$. We set
\begin{align*}
	h(t) = \int_0^tg(s)\,\d s,\quad\forall t\in\mathbb{R}_+.
\end{align*}
The set $\mathscr{H}$ of all functions $h$ that can be written in this form is called the \textit{Cameron-Martin space}, and we write the derivative of $h\in\mathscr{H}$ in sense of distribution by $\dot{h}=g\in L^2(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+),m)$. As a special case of our previous discussion, given a Brownian motion $B$ under $\bbP$, we construct the probability measure
\begin{align*}
	\d\bbQ = D_\infty\,\d\bbP = \exp\left(\int_0^\infty g(t)\,\d B_s-\frac{1}{2}\int_0^t\vert g(t)\vert^2\,\d s\right)\d\bbP.
\end{align*}
Then the process $\widetilde{B}_t=B_t-h(t)$ is a Brownian motion under $\bbQ$. Consequently, for every nonnegative measurable function on $C(\bbR_+,\bbR)$, one have
\begin{align*}
	\E_\bbP\left[D_\infty\Phi((B_t)_{t\geq 0})\right]=\E_\bbQ\left[\Phi((B_t)_{t\geq 0})\right]=\E_\bbQ\left[\Phi\left((\widetilde{B}_t+h(t))_{t\geq 0}\right)\right]=\E_\bbP\left[\Phi((B_t+h(t))_{t\geq 0})\right]
\end{align*}
We rewrite this formula to the following form.

\paragraph{Theorem 5.33\label{thm:5.33}} (Cameron-Martin formula). Let $W$ be the Wiener measure on $C(\bbR_+,\bbR)$, and let $h$ be a function in the Cameron-Martin space $\scr{H}$. Then for every nonnegative measurable function $\Phi$ on $C(\bbR_+,\bbR)$,
\begin{align*}
	\int W(\d\bfw)\Phi(\bfw+h) = \int W(\d\bfw)\exp\left(\int_0^\infty\dot{h}(s)\d\bfw(s)-\frac{1}{2}\int_0^\infty\vert\dot{h}(s)\vert^2\,\d s\right)\Phi(\bfw).
\end{align*}
\paragraph{Remark.} The integral $\int_0^\infty\dot{h}(s)\d\bfw(s)$ is viewed as the Wiener integral, where $\bfw\sim W$.

\paragraph{Application: Law of hitting times for Brownian motion with drift.} Let $B$ be a real Brownian motion with $B_0=0$, and define the hitting time $\tau_a=\inf\{t\geq 0:B_t=a\}$ for every $a>0$. Now given a constant $\mu\in\mathbb{R}$, we are interested in the law of the stopping time
\begin{align*}
	\upsilon_a=\inf\{t\geq 0:B_t+\mu t=a\}.
\end{align*}
Clearly, if $\mu=0$, we have $\upsilon_a=\tau_a$, and the desired law is given by \hyperref[cor:4.25]{Corollary 4.25}. For the general case, we fix $t>0$, and use Cameron-Martin formula to the following function:
\begin{align*}
	h(s)=\mu(s\wedge t),\quad \dot{h}(s)=\mu\mathds{1}_{\{s\leq t\}},\qquad\Phi(\bfw)=\mathds{1}_{\{\max_{s\in[0,t]}\bfw(s)\geq a\}},\ \bfw\in C(\mathbb{R}_+,\mathbb{R}).
\end{align*}
Then we have
\begin{align*}
	\P\left(\upsilon_a\leq t\right)=\E\left[\Phi(B+h)\right]&=\E\left[\exp\left(\mu B_t - \frac{\mu^2}{2}t\right)\mathds{1}_{\{\tau_a\leq t\}}\right].
\end{align*}
By optional stopping theorem [\hyperref[thm:3.48]{Theorem 3.48}], we have
\begin{align*}
	\exp\left(\mu B_{t\wedge\tau_a} - \frac{\mu^2}{2}(t\wedge\tau_a)\right)=\E\left[\exp\left(\mu B_t - \frac{\mu^2}{2}t\right)|\mathscr{F}_{t\wedge\tau_a}\right].
\end{align*}
Consequently,
\begin{align*}
	\P\left(\upsilon_a\leq t\right)
	&=\E\left[\exp\left(\mu B_{t\wedge\tau_a} - \frac{\mu^2}{2}(t\wedge\tau_a)\right)\mathds{1}_{\{\tau_a\leq t\}}\right]=\E\left[\exp\left(\mu a - \frac{\mu^2}{2}\tau_a\right)\mathds{1}_{\{\tau_a\leq t\}}\right]\\
	&=\int_0^t\frac{a}{\sqrt{2\pi s^3}}\e^{-\frac{1}{2s}(\mu s-a)^2}\d s.
\end{align*}
Therefore, $\upsilon_a$ has a density supported on $\mathbb{R}_+$: $\rho_{\upsilon_a}(t)=\displaystyle\frac{a}{\sqrt{2\pi t^3}}\e^{-\frac{1}{2t}(\mu t-a)^2},\ t>0$.

\newpage
\section{Markov Processes}
\subsection{Transition Semigroups and Feller Semigroups}
\paragraph{Definition 6.1\label{def:6.1}} (Markovian transition kernels). Let $(E,\mathscr{E})$ be a measurable space. A \textit{Markovian transition kernel} (or \textit{transition kernel}, for short) on $E$ is a mapping $Q:E\times\mathscr{E}\to[0,1]$ satisfying the following properties:
\begin{itemize}
	\item[(i)] For every $x\in E$, the mapping $\mathscr{E}\ni A\mapsto Q(x,A)$ is a probability measure on $(E,\mathscr{E})$;
	\item[(ii)] For every $A\in\mathscr{E}$, the mapping $E\ni x\mapsto Q(x,A)$ is $\mathscr{E}$-measurable.
\end{itemize} 
\paragraph{Remark.} (i) If $E$ is a finite or countable set equipped with the $\sigma$-algebra $\mathscr{E}$ of all its subsets, we can  then characterize a transition kernel $Q$ by the matrix $(Q(x,\{y\}))_{x,y\in E}$.

(ii) Let $B(E)$ be the vector space of all bounded measurable real functions on $E$, and we define the norm $\Vert f\Vert=\sup_{x\in E}\vert f(x)\vert$ for all $f\in B(E)$. Given a function $f\in B(E)$, we define
\begin{align*}
	Qf:E\to\mathbb{R},\quad Qf(x)=\int_E Q(x,\d y)f(y).
\end{align*}

For every $A\in\mathscr{E}$, we have $Q\mathds{1}_A(x)=Q(x,A)$, hence the function $Q\mathds{1}_A$ is measurable. A simple function approximation argument shows that $Qf$ is measurable for all $f\in B(E)$. Furthermore,
\begin{align*}
	\Vert Qf\Vert = \sup_{x\in E}\left(\int_E Q(x,\d y)f(y)\right)\leq \Vert f\Vert\sup_{x\in E}\left(\int_E Q(x,\d y)\right) = \Vert f\Vert.
\end{align*}

Clearly, $B(E)$ is complete under the norm $\Vert\cdot\Vert$. From this perspective, we can view $Q$ as a bounded linear operator on the Banach space $B(E)$ such that $\Vert Q\Vert\leq 1$, which is called a \textit{contraction} on $B(E)$.

\paragraph{Definition 6.2} (Transition semigroups). A collection $(Q_t)_{t\geq 0}$ of transition kernels on $E$ is said to be a \textit{transition semigroup} on $E$ if the following properties hold:
\begin{itemize}
	\item[(i)] $Q_0(x,\cdot)=\delta_x$ for every $x\in E$.
	\item[(ii)] (Chapman-Kolmogorov identity). For every $s,t\geq 0$ and every $A\in\mathscr{E}$,
	\begin{align*}
		Q_{t+s}(x,A)=\int_E Q_t(x,\d y)Q_s(y,A).
	\end{align*}
    \item[(iii)] For every $A\in\mathscr{E}$, the mapping $(t,x)\mapsto Q_t(x,A)$ is measurable with respect to $\mathscr{B}(\mathbb{R}_+)\times\mathscr{E}$.
\end{itemize}
\paragraph{Remark.} If we view $(Q_t)_{t\geq 0}$ as bounded linear operators on $B(E)$, the Chapman-Kolmogorov identity implies that $Q_{t+s}=Q_tQ_s$ for all $s,t\geq 0$. This give rise to the associative property: $(Q_rQ_s)Q_t=Q_r(Q_sQ_t)$ for all $r,s,t\geq 0$. Hence $(Q_t)_{t\geq 0}$ is a semigroup of contractions on $B(E)$.

\paragraph{Definition 6.3\label{def:6.3}} (Resolvent). Let $\lambda>0$. The \textit{$\lambda$-resolvent} of the transition semigroup $(Q_t)_{t\geq 0}$ is the linear operator $R_\lambda:B(E)\to B(E)$ defined by
\begin{align*}
	R_\lambda f(x) = \int_0^\infty\e^{-\lambda t}Q_tf(x)\,\d t,\quad \forall f\in B(E),\ x\in E.
\end{align*}
\paragraph{Remark.} The resolvent has the following properties:
\begin{itemize}
	\item[(i)] $R_\lambda:B(E)\to B(E)$ is a positive and bounded linear operator. Note that $R_\lambda f\geq 0$ for all $f\geq 0$, and
	\begin{align*}
		 \sup_{x\in E}\left(\int_0^\infty\e^{-\lambda t}Q_tf(x)\,\d t\right) \leq \int_0^\infty\e^{-\lambda t}\Vert Q_t f\Vert\,\d t \int_0^\infty\e^{-\lambda t}\Vert f\Vert\,\d t\quad\Rightarrow\quad \Vert R_\lambda f\Vert\leq \frac{1}{\lambda}\Vert f\Vert.
	\end{align*}
	\item[(ii)] (Resolvent equation). For all $\lambda,\mu>0$, we have $R_\lambda-R_\mu + (\lambda-\mu)R_\lambda R_\mu = 0$:
	\begin{align*}
		R_\lambda(R_\mu f)(x) &= \int_0^\infty\e^{-\lambda s}\left(\int_E Q_s(x,\d y)\int_0^\infty \e^{-\mu t}Q_tf(y)\,\d t\right)\d s\\
		&=\int_0^\infty\e^{-\lambda s}\left(\int_0^\infty\e^{-\mu t} Q_{s+t} f(x)\,\d t\right)\d s \overset{r=s+t}{=} \int_0^\infty\e^{-(\lambda-\mu)s}\left(\int_s^\infty\e^{-\mu r} Q_{r} f(x)\,\d r\right)\d s\\
		&= \int_0^\infty Q_rf(x)\e^{-\mu r}\left(\int_0^r\e^{-(\lambda-\mu)s}\,\d s\right)\d r = \int_0^\infty \left(\frac{\e^{-\mu r}-\e^{-\lambda r}}{\lambda-\mu}\right)Q_rf(x)\,\d r.
	\end{align*}
	
	Consequently, if we fix $f\in B(E)$, it holds
	\begin{align*}
		\Vert(R_\lambda-R_\mu)f\Vert=\vert\lambda-\mu\vert\left\Vert R_\lambda R_\mu f\right\Vert\leq\frac{\vert\lambda-\mu\vert\left\Vert f\right\Vert}{\lambda\mu},\quad\forall \lambda,\mu\in(0,+\infty).
	\end{align*}
	Hence $\lambda\to R_\lambda f$ is a continuous mapping from $(0,+\infty)$ into $B(E)$.
	\item[(iii)] For every $\lambda>0$ and every $n\in\mathbb{N}$, we have
	\begin{align*}
		R_\lambda^nf(x) = \int_0^\infty\frac{s^{n-1}}{(n-1)!}\e^{-\lambda s}Q_sf(x)\,\d s.
	\end{align*}
     Clearly, this equality holds for $n=1$. Hence we can prove the general case by induction:
     \begin{align*}
     	R_\lambda^{n+1}f(x) = R_\lambda(R_\lambda^nf)(x)&= \int_0^\infty\e^{-\lambda r}\left(\int_0^\infty\frac{t^{n-1}}{(n-1)!}\e^{-\lambda t}Q_{r+t}f(x)\,\d t\right)\d r\\
     	&=\int_0^\infty\left(\int_r^\infty\frac{(s-r)^{n-1}}{(n-1)!}\e^{-\lambda s}Q_sf(x)\,\d t\right)\d r\\
     	&=\int_0^\infty\int_0^s\frac{(s-r)^{n-1}}{(n-1)!}\e^{-\lambda s}Q_sf(x)\,\d r\,\d s = \int_0^\infty\frac{s^n}{n!}\e^{-\lambda s}Q_sf(x)\,\d s.
     \end{align*}
\end{itemize}

\paragraph{Preliminary: LCCB space.} From now on, we deal with a special topological space $E$, which is \textit{Locally Compact, Hausdorff, and has a Countable Basis $\mathscr{B}$ (LCCB)}. 

Since $E$ is locally compact, for each $x\in X$ there exists an open neighborhood $U_x$ with compact closure. Consequently, one can find a basis set $B_x\in\mathscr{B}$ such that $x\in B_x\subset U_x$, and $\overline{B}_x$ is compact. We choose $\mathscr{B}_K\subset\mathscr{B}$ to be the collection of all basis sets with compact closure. Then $B_x\in\mathscr{B}_K$ for all $x\in X$, and $E=\bigcup_{B\in\mathscr{B}_K}\overline{B}$ is a countable union of compact sets. Therefore, $E$ is a \uwave{$\sigma$-compact} topological space.


By $\sigma$-compactness of $E$, we choose an increasing sequence $(C_n)_{n=1}^\infty$ of compact subsets increasing to $E$. Then one can construct an increasing sequence $(K_n)_{n=1}^\infty$ of compact subspace of $E$ such that
\begin{align*}
	K_1\subset K_2^\circ\subset K_2 \subset K_3^\circ\subset\cdots\subset K_n\subset K_{n+1}^\circ\subset K_{n+1}\subset\cdots,\quad E=\bigcup_{n=1}^\infty K_n.\tag{*}\label{compactseq}
\end{align*}

We start by choosing a neighborhood $U_x$ with compact closure for each $x\in E$ and setting $K_0=\emptyset$. If $K_{n-1}$ is constructed, then $K_{n-1}\cup C_n$ is compact, and there exists $x_1,\cdots,x_k$ such that $K_{n-1}\cup C_n\subset U_{x_1}\cup\cdots\cup U_{x_k}$. We construct $K_n=\overline{U}_{x_1}\cup\cdots\cup\overline{U}_{x_k}$, which is also compact. Then we have $K_{n-1}\subset K_n^\circ$, and $\bigcup_{n=1}^\infty K_n\supset\bigcup_{n=1}^\infty (K_n)^\circ\supset\bigcup_{n=1}^\infty C_n=E$. Clearly, for any compact subset $K$ of $E$, $\{(K_n)^\circ\}_{n=1}^\infty$ is an open cover of $K$, hence there exists $K_n$ such that $K\subset K_n$.


\paragraph{Preliminary: Continuous functions vanishing at infinity.} Let $E$ be a locally compact Hausdorff space. A continuous function $f:E\to\mathbb{R}$ is said to be \textit{vanishing at infinity}, if for all $\epsilon>0$,
\begin{align*}
	\exists\ compact\ K\subset E\ such\ that\ \vert f(x)\vert<\epsilon,\ \forall x\in E\backslash K\quad\Leftrightarrow\quad \{x\in E:\vert f(x)\vert\geq\epsilon\}\ is\ compact.
\end{align*}
In addition, If $(K_n)_{n=1}^\infty$ is a sequence of compact sets specified in (\ref{compactseq}), then we have
\begin{align*}
	\lim_{n\to\infty}\sup_{x\in E\backslash K_n}\vert f(x)\vert = 0.
\end{align*}

We denote by $C_0(E)$ the vector space of all continuous real-valued functions on $E$ vanishing at infinity. This is a closed subspace of $B(E)$. Consequently, $C_0(E)$ is a Banach space given the supremum norm.

In addition, since $E$ is a locally compact Hausdorff space, it admits the Alexandroff compactification $E^*=E\cup\{\infty\}$, whose topology consists of all open sets in $E$ and all sets of the form $(E\backslash K)\cup\{\infty\}$, where $K$ is a compact subset of $E$. Consequently, for every $f\in C_0(E)$, we can extend it to a function of $C_b(E^*)$ by setting $f(\infty)=0$. Conversely, for every $f\in C_b(E^*)$, we have $f|_E-f(\infty)\in C_0(E)$.

\paragraph{Riesz-Markov Theorem.} If $E$ is a locally compact Hausdorff space and $L:C_0(E)\to\mathbb{R}$ is a bounded linear functional, then there exists a unique regular (finite signed) measure $\mu$ such that $Lf=\int f\,\d \mu$ for all $f\in C_0(E)$. Furthermore, $\Vert L\Vert=\vert\mu\vert(E)$.

\paragraph{Using $C_0(E)$ to separate points of a LCCB space $E$.} If $X$ is a second-countable normal space, we choose a basis $\mathscr{B}=\{B_n\}_{n=1}^\infty$ of $X$. We define $I_c:=\{(m,n)\in\mathbb{N}^2:\overline{B}_m\subsetneq B_n\}$, and for each $(m,n)\in I_c$, by Urysohn's lemma, we can find a continuous function $f_{m,n}:X\to[0,1]$ such that $f_{m,n}(\overline{B}_m)=\{1\}$ and $f_{m,n}(X\backslash B_n)=\{0\}$. Since for every pair of distinct points $x_1,x_2\in X$, there exists disjoint neighborhoods $B_{n_1}\ni x_1$ and $B_{n_2}\ni x_2$. Therefore, $\mathcal{F}=\{f_{m,n}:(m,n)\in I_c\}\subset C(X)$ is a countable collection of functions separating points of $X$, i.e. for all $x_1,x_2\in E$ with $x_1\neq x_2$, there exists $f\in\mathcal{F}$ such that $f(x_1)\neq f(x_2)$.

Now we consider a LCCB space $E$. If $E^*=E\cup\{\infty\}$ is the Alexandroff compactification of $E$, then $U_n:=E^*\backslash K_n$ is a countable local base of $\infty$, where $\{K_n\}_{n=1}^\infty$ is specified in (\ref{compactseq}), because for any neighborhood $V$ of $\infty$ in $E^*$, $\{K_n^\circ\}_{n=1}^\infty$ is an open cover of the compact set $E^*\backslash K$. Consequently, we can construct a countable collection $\mathcal{F}\in C(E^*)$ of functions separating points $E^*$, and we may set these functions to $0$ at $\infty$. By restricting these function on $E$, we obtain a countable collection of functions in $C_0(E)$ separating points of $E$. We again use this conclusion when finding the càdlàg version of a Feller process.

\paragraph{Vague convergence.} We equip a metrizable space $E$ with its Borel $\sigma$-algebra $\mathscr{E}$. Let $\mu_n$ be a sequence of probability measures on $(E,\mathscr{E})$. If there exists a probability measure $\mu$ such that
\begin{align*}
	\int_E f\,\d \mu_n\to\int_E f\,\d \mu,\quad\forall f\in C_c(E),
\end{align*}
then $\mu_n$ is said to \textit{vaguely converges to} $\mu$. Clearly, weak convergence implies vague convergence.

In a LCCB space $E$, we can prove that weak convergence is equivalent vague convergence. Let $\mu_n$ be a sequence of probability measures converging vaguely to $\mu$, and fix $\epsilon>0$. For any $g\in C_b(E)$ with $\Vert g\Vert\leq M$, we let $\phi\in C_c(E)$ be a function supported on $K$ such that $\int_E\phi\,\d \mu>1-\frac{\epsilon}{3M}$. By vague convergence, there exists $N_1$ such that $\int_E\phi\,\d \mu_n>1-\frac{\epsilon}{3M}$ for all $n\geq N$. Since $g\phi\in C_c(E)$, we also choose $N_2$ such that $\left\vert\int_E g\phi\,\d \mu_n - \int_E g\phi\,\d \mu\right\vert<\epsilon/3$ for all $n\geq N_2$. Then
\begin{align*}
	\left\vert\int_E g\,\d \mu_n - \int_E g\,\d \mu\right\vert &= \left\vert\int_E g(1-\phi)\,\d \mu_n\right\vert + \left\vert\int_E g\phi\,\d \mu_n - \int_E g\phi\,\d \mu\right\vert + \left\vert\int_E g(1-\phi)\,\d \mu\right\vert\\
	&\leq 2M\left\vert\int_E(1-\phi)\,\d \mu\right\vert + \left\vert\int_E g\phi\,\d \mu_n - \int_E g\phi\,\d \mu\right\vert <\epsilon,\quad\forall n\geq\max\{N_1,N_2\}.
\end{align*}
Since $\epsilon$ is arbitrarily small, we have $\int_E g\,\d \mu_n\to\int_E g\,\d \mu$, and the weak convergence is clear.

\paragraph{}

\paragraph{Definition 6.4\label{def:6.4}} (Feller semigroups). A transition semigroup $(Q_t)_{t\geq 0}$ is said to be a \textit{Feller semigroup} if
\begin{itemize}
	\item[(i)] For every $t\geq 0$, we have $Q_tC_0(E)\subset C_0(E)$.
	\item[(ii)] For every $f\in C_0(E)$ and every $x\in E$, $\lim_{t\downdownarrows 0}Q_tf(x)= f(x)$.
\end{itemize}
\paragraph{Remark.} Consider the $\lambda$-resolvent. For any sequence $x_n\in E$, by dominated convergence theorem,
\begin{align*}
	\lim_{n\to\infty}R_\lambda f(x_n) = \lim_{n\to\infty}\int\e^{-\lambda s}Q_sf(x_n)\,\d s=\int\e^{-\lambda s}\lim_{n\to\infty}Q_sf(x_n)\,\d s,\quad\forall f\in C_0(E).
\end{align*}

In the last equality, we use the fact $\sup_{t\geq 0}\left\vert\e^{-\lambda s}Q_{s}f(x_n)\right\vert\leq\Vert f\Vert$ for all $n\in\mathbb{N}$. Consequently, if $(Q_t)_{t\geq 0}$ is a Feller semigroup, we also have $R_\lambda C_0(E)\subset C_0(E)$ for all $\lambda>0$.

\paragraph{Proposition 6.5.\label{prop:6.5}} Let $(Q_t)_{t\geq 0}$ be a Feller semigroup, and let $R_\lambda$ be its $\lambda$-resolvent, where $\lambda>0$. Define $\mathfrak{R}=\left\{R_\lambda f:f\in C_0(E)\right\}$. Then $\mathfrak{R}$ does not depend on the choice of $\lambda$, and $\mathfrak{R}$ is a dense subspace of $C_0(E)$.
\begin{proof}
For any $\mu\neq\lambda$, the resolvent equation gives $R_\lambda f=R_\mu g$, where $g=f-(\lambda-\mu)R_\lambda f\in C_0(E)$. Hence $\mathfrak{R}$ does not depend on the choice of $\lambda$. For the second assertion, note that
\begin{align*}
	\lim_{\lambda\to\infty}\lambda R_\lambda f(x) = \lim_{\lambda\to\infty}\lambda\int_0^\infty\e^{-\lambda s} Q_sf(x)\,\d s = \lim_{\lambda\to\infty}\int_0^\infty \e^{-t}Q_{t/\lambda}f(x)\,\d t = \int_0^\infty\e^{-t}f(x)\,\d t = f(x),
\end{align*}
where the last equality holds by dominated convergence, since $\sup_{s\geq 0}\left\vert\e^{-s}Q_{s/\lambda}f(x)\right\vert\leq\Vert f\Vert$ for all $\lambda>0$. Furthermore, for all $\lambda,\mu>0$ and all $x\in E$, we have
\begin{align*}
	(\lambda R_\lambda-\mu R_\mu) f(x) &= \int_0^\infty\left(\lambda\e^{-\lambda s}-\mu\e^{-\mu s}\right) Q_sf(x)\,\d s = \int_0^\infty\e^{-t}\left(Q_{t/\lambda}-Q_{t/\mu}\right)f(x)\,\d t\\
	&=\int_0^\infty\e^{-t}\left(\frac{t}{\mu}-\frac{t}{\lambda}\right)Q_{t/\mu}Q_{t/\lambda}f(x)\,\d t\leq\left\vert\frac{1}{\lambda}-\frac{1}{\mu}\right\vert\Vert f\Vert.
\end{align*}
Hence $\Vert(\lambda R_\lambda-\mu R_\mu)f\Vert\to 0$ as $\lambda,\mu\to\infty$, and $\lambda R_\lambda f\in\mathfrak{R}$ converges in $C_0(E)$ as $\lambda\to\infty$ by completeness, and the pointwise limit $f$ must be the limit $C_0(E)$. Consequently, $\mathfrak{R}$ is dense in $C_0(E)$.
\end{proof}

\paragraph{Remark.} In the proof, we also conclude that $\lim_{\lambda\to\infty}\Vert\lambda R_\lambda f-f\Vert =0$ for all $f\in C_0(E)$.

\paragraph{Proposition 6.6} (Strong continuity). Let $(Q_t)_{t\geq 0}$ be a Feller semigroup, and fix $f\in C_0(E)$. Then $\lim_{t\downdownarrows 0}\Vert Q_t f-f\Vert = 0$. Consequently, the mapping $t\mapsto Q_t f$ is uniformly continuous from $(0,\infty)$ into $C_0(E)$.
\begin{proof}
By Fubini's theorem, since $(s,y)\mapsto\e^{-\lambda s}Q_sf(y)$ is dominated by $\e^{-\lambda s}\Vert f\Vert$, it holds
\begin{align*}
	Q_tR_\lambda f(x) = \e^{\lambda t}\int_t^\infty \e^{-\lambda s}Q_sf(x)\,\d s \leq\e^{\lambda t}R_\lambda f -\int_0^t\e^{\lambda(t-s)}Q_sf(x)\,\d s
\end{align*}
Consequently, we have
\begin{align*}
	\Vert Q_tR_\lambda f-R_\lambda f\Vert = \left(\e^{\lambda t}-1\right)\Vert R_\lambda f\Vert + t\e^{\lambda t}\Vert f\Vert\to 0\quad \textit{as}\ \ t\downdownarrows 0.
\end{align*}
Therefore we have $\lim_{t\downdownarrows 0}\Vert Q_t f-f\Vert = 0$ for all $f\in\mathfrak{R}$. The continuity of $Q_t-\mathrm{Id}$ and a standard density argument extend this conclusion to all $f\in C_0(E)$. For the second assertion, note that for all $t>s\geq 0$,
\begin{align*}
	\Vert Q_tf-Q_sf\Vert = \Vert Q_s(Q_{t-s}f - f)\Vert = \Vert Q_{t-s}f - f\Vert \to 0\quad\textit{as}\ \ t-s\downdownarrows 0.
\end{align*}
Since this convergence is uniform for all $s\in(0,\infty)$, the mapping $t\mapsto Q_tf$ is uniformly continuous.
\end{proof}

\paragraph{Definition 6.7\label{def:6.7}} (Infinitesimal generator). Let $(Q_t)_{t\geq 0}$ be a Feller semigroup. Define the space $\mathfrak{D}(L)$ by
\begin{align*}
	\mathfrak{D}(L)=\left\{f\in C_0(E):\frac{Q_t f-f}{t}\ \textit{converges in}\ C_0(E)\ \textit{when}\ t\downdownarrows 0\right\}.
\end{align*}
Then $\mathfrak{D}(L)$ is a subspace of the vector space $C_0(E)$. Define the linear operator $L:\mathfrak{D}(L)\to C_0(E)$ as follows:
\begin{align*}
	Lf=\lim_{t\downdownarrows 0}\frac{Q_t f-f}{t},\quad\forall f\in C_0(E).
\end{align*}
The operator $L:\mathfrak{D}(L)\to C_0(E)$ is called the $\textit{infinitesimal generator}$ (or \textit{generator}, for short) of $(Q_t)_{t\geq 0}$.

\paragraph{Proposition 6.8.\label{prop:6.8}} Let $L$ be the generator of Feller semigroup $(Q_t)_{t\geq 0}$.
\begin{itemize}
	\item[(i)] For every $f\in\mathfrak{D}(L)$ and every $s>0$, $Q_sf\in\mathfrak{D}(L)$, and $L(Q_sf)=Q_s(Lf)$. Furthermore,
	\begin{align*}
		Q_tf = f+\int_0^t Q_sLf\,\d s = f+\int_0^t LQ_sf\,\d s\quad\Leftrightarrow\quad  \frac{\d}{\d t}Q_tf=LQ_tf.\tag{6.1}\label{eq:6.1}
	\end{align*}
	\item[(ii)] $\mathfrak{D}(L)=\mathfrak{R}$, and $R_\lambda$ is the inverse of $\lambda\id-L$ for all $\lambda>0$, namely, $(\lambda\id-L)R_\lambda=R_\lambda(\lambda\id-L)=\id$.
	\item[(iii)] The semigroup $(Q_t)_{t\geq 0}$ is determined by the generator $L$:
	\begin{align*}
		Q_t = \e^{tL} := \lim_{\lambda\to\infty}\e^{-t\lambda}\sum_{k=0}^\infty\frac{t^k\lambda^{2k}}{k!}(\lambda\id - L)^{-k}
	\end{align*}
\end{itemize}
\begin{proof}
(i) For all $s\geq 0$, $Q_s$ is a bounded linear operator on $C_0(E)$, and $Q_sf\in C_0(E)$ for all $f\in D(E)\subset C_0(E)$. Semigroup property and continuity of $Q_s$ implies
\begin{align*}
	\frac{Q_t(Q_sf)-Q_sf}{t} = Q_s\left(\frac{Q_tf-f}{t}\right)\quad\Rightarrow\quad Q_sf\in \mathfrak{D}(L),\ \ L(Q_s f) = Q_s(L f).
\end{align*}
Similarly, we have $h^{-1}\left(Q_{t+h}f-Q_f\right)=Q_t(Lf)$ when $h\downdownarrows 0$. Moreover,
\begin{align*}
	\lim_{h\downdownarrows 0}\left\Vert\frac{Q_t f - Q_{t-h}f}{h} - Q_t(Lf)\right\Vert&=\lim_{h\downdownarrows 0}\left\Vert Q_{t-h}\left(\frac{Q_h f-f}{h}\right)-Q_t(Lf)\right\Vert\\
	&\leq\lim_{h\downdownarrows 0}\left\Vert Q_{t-h}\left(\frac{Q_h f-f}{h}-Lf\right)\right\Vert+\lim_{h\downdownarrows 0}\left\Vert(Q_{t-h}-Q_t)(Lf)\right\Vert\\
	&\leq\lim_{h\downdownarrows 0}\left\Vert\frac{Q_h f-f}{h}-Lf\right\Vert+\lim_{h\downdownarrows 0}\left\Vert Lf-Q_h(Lf)\right\Vert = 0.
\end{align*}
Consequently, for every $x\in E$, the mapping $t\mapsto Q_tf(x)$ is differentiable. By fundamental theorem of calculus,
\begin{align*}
	Q_tf-f = \int_0^t Q_s(Lf)\,\d s = \int_0^t L(Q_sf)\,\d s,\quad\forall t\in\mathbb{R}_+.
\end{align*}
(ii) If $f\in\mathfrak{D}(L)$, we use \hyperref[eq:6.1]{(6.1)} and Fubini's theorem:
\begin{align*}
R_\lambda(\lambda\id - L)f &= \int_0^\infty \lambda\e^{-\lambda s}Q_sf\,\d s-\int_0^\infty \e^{-\lambda s}Q_s(Lf)\,\d s\\
&=\int_0^\infty \lambda\e^{-\lambda s}\left(f+\int_0^s L(Q_tf)\,\d t\right)\,\d s -\int_0^\infty \e^{-\lambda s}L(Q_sf)\,\d s\\
&= f+\int_0^\infty\left(\int_t^\infty\lambda\e^{-\lambda s}L(Q_t f)\,\d s\right)\,\d t-\int_0^\infty \e^{-\lambda s}L(Q_sf)\,\d s = f.
\end{align*}
Apparently, we have $\mathfrak{D}(L)\subset\mathfrak{R}$. On the other hand, for $g\in C_0(E)$, by dominated convergence theorem,
\begin{align*}
	\lim_{h\downdownarrows 0}\frac{Q_hR_\lambda g-R_\lambda g}{h} &= \lim_{h\downdownarrows 0}\frac{1}{h}\int_0^\infty \e^{-\lambda t}\left(Q_{t+h}-Q_t\right)g\,\d t\\
	&=\lim_{h\downdownarrows 0}\frac{1}{h}\left(\left(1-\e^{-\lambda h}\right)\int_0^\infty \e^{-\lambda t}Q_{t+h} g\,\d t-\int_0^h\e^{-\lambda t}Q_tg\,\d t\right) = \lambda R_\lambda g-g.
\end{align*}
Hence $R_\lambda g\in\mathfrak{D}(L)$, and $\mathfrak{R}=\mathfrak{D}(L)$. Moreover, the first assertion in (i) implies
\begin{align*}
	LR_\lambda g = \lambda R_\lambda g - g\quad\Rightarrow\quad (\lambda\id-L)R_\lambda g = g.
\end{align*}
(iii) Fix $f\in C_0(E)$. For every $\lambda>0$, if $(\lambda\id - L)g=f$ for some $g\in\mathfrak{D}(L)$, then $g=R_\lambda(\lambda\id - L)g=R_\lambda f$. Hence the resolvent $R_\lambda$ is uniquely determined by inverting the operator $\lambda\id-L$, which is defined on $C_0(E)$.

For every $\lambda>0$, we define $A_\lambda:=\lambda R_\lambda L = \lambda^2R_\lambda - \lambda\id$. Then $A_\lambda:\mathfrak{D}(L)\to C_0(E)$ is a bounded linear operator, and $\Vert A_\lambda\Vert\leq 2\lambda$. Then we define the following series, which converges in norm:
\begin{align*}
	\e^{tA_\lambda}=\sum_{k=0}^\infty\frac{(tA_\lambda)^k}{k!}\quad\Rightarrow\quad \left\Vert\e^{tA_\lambda}\right\Vert=\left\Vert\e^{t\lambda^2R_\lambda-\lambda\id}\right\Vert\leq\sum_{k=0}^\infty\e^{-t\lambda}\frac{\Vert t\lambda^2R_\lambda\Vert^k}{k!}\leq\sum_{k=0}^\infty\e^{-t\lambda}\frac{(t\lambda)^k}{k!}= 1.
\end{align*}

By commutativity principle (i.e. $\e^{T+S}=\e^T\e^S$ for commutative bounded linear operators $ST=TS$), the collection $(\e^{tA_\lambda})_{t\geq 0}$ is a semigroup of contractions. Moreover, since $A_\lambda A_\mu=A_\mu A_\lambda = \frac{\mu-\lambda}{\mu\lambda}L(R_\lambda-R_\mu)L$,
\begin{align*}
	\e^{tA_\lambda}-\e^{tA_\mu}=\int_0^t\frac{\d}{\d s}\e^{tA_\mu+s(A_\lambda-A_\mu)}\,\d s=\int_0^t\e^{(t-s)A_\mu}\e^{sA_\lambda}(A_\lambda-A_\mu)\,\d s.
\end{align*}
Hence for all $f\in\mathfrak{D}(L)$, since $A_\lambda f\to Lf$ as $\lambda\to\infty$,
\begin{align*}
	\left\Vert\left(\e^{tA_\lambda}-\e^{tA_\mu}\right)f\right\Vert\leq\left\Vert (A_\lambda-A_\mu)f\right\Vert\quad\Rightarrow\quad \e^{tL}f:=\lim_{\lambda\to\infty}\e^{tA_\lambda}f\ \textit{exists in}\ C_0(E).
\end{align*}
Clearly, the mapping $\e^{tL}:\mathfrak{D}(L)\to C_0(E)$ is a contraction. Since $\mathfrak{D}(L)$ is dense in $C_0(E)$, we can extend the definition of $\e^{tL}$ from $\mathfrak{D}(L)$ to $C_0(E)$. The following shows that $(e^{tL})_{t\geq 0}$ is a semigroup: $\forall f\in\mathfrak{D}(L)$,
\begin{align*}
	\left\Vert\e^{(t+s)L}f-\e^{tL}\e^{sL}f\right\Vert &= \left\Vert(\e^{(t+s)L}-\e^{(t+s)A_\lambda})f\right\Vert + \left\Vert\e^{tA_\lambda}(\e^{sA_\lambda}-\e^{sL})f\right\Vert + \left\Vert(\e^{tA_\lambda}-\e^{tL})\e^{sL}f\right\Vert\\
	&\leq\left\Vert(\e^{(t+s)L}-\e^{(t+s)A_\lambda})f\right\Vert + \left\Vert(\e^{sA_\lambda}-\e^{sL})f\right\Vert + \left\Vert(\e^{tA_\lambda}-\e^{tL})\e^{sL}f\right\Vert\to 0\quad\textit{as}\ \lambda\to\infty.
\end{align*}
We also note strong continuity so that $(\e^{tL})_{t\geq 0}$ is a Feller semigroup: $\forall f\in\mathfrak{D}(L)$,
\begin{align*}
\Vert \e^{tL}f-f\Vert \leq \Vert(\e^{tL}-\e^{tA_\lambda})f\Vert + \Vert\e^{tA_\lambda}f-f\Vert\leq t\Vert Lf-A_\lambda f\Vert + \Vert\e^{tA_\lambda}f-f\Vert \to 0\quad\textit{as}\ t\downdownarrows 0.
\end{align*}
Furthermore, the generator of $(\e^{tL})_{t\geq 0}$ is $L$:
\begin{align*}
	\e^{tA_\lambda}f - f = \int_0^tA_\lambda\e^{sA_\lambda}f\,\d s\quad\Rightarrow\quad\e^{tL}f-f=\int_0^t L\e^{sL}f\,\d s\quad\Rightarrow\quad \lim_{t\downdownarrows 0}\frac{\e^{tL}f-f}{t} = L.
\end{align*}
Since the resolvent $R_\lambda f=\int_0^\infty \e^{-\lambda t}Q_tf\,\d t$ is the Laplacian transform, it has a one-to-one correspondence with the transition semigroup $(Q_t)_{t\geq 0}$. Hence we can recover $Q_t=\e^{tL}$ uniquely from the generator $L$.
\end{proof}
\paragraph{Remark.} Note that the domain of operator $(\lambda\id-L)R_\lambda$ is $C_0(E)$.

\newpage
\subsection{Markov Processes and Feller Processes}
Now we fix a probability space $(\Omega,\mathscr{F},\P)$ and a filtration $(\mathscr{F}_t)_{t\geq 0}$.

\paragraph{Definition 6.9\label{def:6.9}} (Markov processes and Feller processes). Let $(Q_t)_{t\geq 0}$ be a transition semigroup on $E$. A \textit{Markov process} with respect to the filtration $(\mathscr{F}_t)_{t\geq 0}$ with transition semigroup $(Q_t)_{t\geq 0}$ is an adapted process $X=(X_t)_{t\geq 0}$ with values in $E$ such that for all $f\in E$ and all $s,t\geq 0$,
\begin{align*}
	\E\left[f(X_{s+t})|\mathscr{F}_s\right] = Q_tf(X_s).\tag{6.2}\label{eq:6.2}
\end{align*}

Without specifying a filtration, we implicitly mean that the definition holds for the canonical filtration $\mathscr{F}_t^X=\sigma(X_s,0\leq s\leq t)$. Clearly, a Markov process $X$ with respect to any filtration $(\mathscr{F}_t)_{t\geq 0}$ is also a Markov process with respect to the canonical filtration $(\mathscr{F}_t^X)_{t\geq 0}$.

A Markov process with values in $E$ is called a \textit{Feller process} if its transition semigroup is Feller.

\paragraph{Remark.} Particularly, for all $A\in\mathscr{E}$, one can take $f=\mathds{1}_A$ in \hyperref[eq:6.1]{(6.1)} to obtain
\begin{align*}
	\P(X_{s+t}\in A|\mathscr{F}_s)=Q_t(X_s,A).
\end{align*}
It is seen that the conditional distribution of $X_{s+t}$ given the history $\mathscr{F}_s$ only depends on the current state $X_s$. Furthermore, given the law $\gamma$ of $X_0$, we have for all $0<t_1<\cdots<t_n$ and all $A_0,A_1,\cdots,A_n\in\mathscr{E}$ that
\begin{align*}
	\P&(X_0\in A_0,X_{t_1}\in A_1,X_{t_2}\in A_2,\cdots,X_{t_n}\in A_n) \\
	&= \int_{A_0}\gamma(\d x_0)\int_{A_1} Q_{t_1}(x_0,\d x_1)\int_{A_1} Q_{t_2-t_1}(x_1,\d x_2)\cdots\int_{A_n}Q_{t_n-t_{n-1}}(x_{n-1},\d x_n).\tag{6.3}\label{eq:6.3}
\end{align*}
More generally, if $f_0,f_1,\cdots,f_n\in B(E)$, we have
\begin{align*}
	\E&\left[f_0(X_0)f_1(X_{t_1})\cdots f_n(X_{t_n})\right]\\
	&= \int_{E}\gamma(\d x_0)f_0(x_0)\int_{E} Q_{t_1}(x_0,\d x_1)f_1(x_1)\int_{E} Q_{t_2-t_1}(x_1,\d x_2)f_2(x_2)\cdots\int_{E}Q_{t_n-t_{n-1}}(x_{n-1},\d x_n)f_n(x_n).
\end{align*}

Now we discuss the existence of a Markov process with transition semigroup $(Q_t)_{t\geq 0}$. According to \hyperref[eq:6.2]{(6.2)}, with the initial $\gamma$ given, we obtain a pre-measure $P_{t_1,\cdots,t_n}^\gamma=\P(X_0\in\cdot,X_{t_1}\in\cdot,\cdots,X_{t_n}\in\cdot)$ on all measurable rectangles $\mathscr{E}^n$, which can be uniquely extended to a measure on the product space $(E^n,\mathscr{E}^{\otimes n})$. In addition, the collection of all finite marginals $\{P^\gamma_{t_1,\cdots,t_n}:n\in\mathbb{N},\ 0<t_1<\cdots<t_n\}$ satisfies the compatibility condition given in \hyperref[prop:4.11]{Proposition 4.11}, according to the Chapman-Kolmogorov identity.

We further assume that $E$ is a Polish space. Then according to \hyperref[cor:4.15]{Corollary 4.15}, which is a consequence of the \hyperref[thm:4.13]{Daniell-Kolmogorov extension theorem}, the compatible family $\{P^\gamma_{t_1,\cdots,t_n}:n\in\mathbb{N},\ 0<t_1<\cdots<t_n\}$ of probability measures has a unique extension $P^\gamma$ on the canonical space $(E^{\mathbb{R}_+},\mathscr{E}^{\otimes\mathbb{R}_+})$. Consequently, the canonical process $\{\pi_t\}_{t\geq 0}$ on $(E^{\mathbb{R}_+},\mathscr{E}^{\otimes\mathbb{R}_+})$ is a Markov process under $P^\gamma$ with transition semigroup $(Q_t)_{t\geq 0}$ with respect to the canonical filtration, and the law of $\pi_0$ is given by $\gamma$.

To summarize, if $E$ is a Polish space, we can construct a $E$-valued Markov process $(X_t)_{t\geq 0}$ with transition semigroup $(Q_t)_{t\geq 0}$ under any given initial distribution $\gamma$.

\paragraph{Alternative definition of Feller Semigroup.} Let $(Q_t)_{t\geq 0}$ be a transition group, and for every $x\in E$, let $(X_t^x)_{t\geq 0}$ be a Markov process with semigroup $(Q_t)_{t\geq 0}$ starting from $X_0\sim\delta_x$. Then $(X_t^x)_{t\geq 0,x\in E}$ is a \textit{Markov family}, and the law of every process $(X_t^x)_{t\geq 0}$ is given by $P^x:=P^{\delta_x}$.

Clearly, if $(Q_t)_{t\geq 0}$ is a Feller semigroup, then every process $(X_t^x)_{t\geq 0}$ is a Feller process. In fact, we can characterize a Feller semigroup by the following properties of the law of Markov families $(X_t^x)_{t\geq 0,x\in E}$. Following our discussion of \textit{vague convergence}, one can easily show that
\begin{align*}
\forall x\in E\ \ and\ \ \forall t\geq 0,\ \ X_t^y\overset{d}{\to}X_t^x\ \ as\ \ y\to x\quad&\Leftrightarrow\quad Q_tC_0(E)\subset C_0(E);\\
\forall x\in E,\ \ X_t^x\overset{\P}{\to} x\ \ as\ \ t\downarrow 0\quad&\Leftrightarrow\quad Q_tf(x)\to f(x),\ \ \forall f\in C_0(E).
\end{align*}

\paragraph{} Now we study the sample path property of Feller processes. Recall that we assume $E$ to be a LCCB space.

\paragraph{Proposition 6.10.\label{prop:6.10}} If $(X_t)_{t\geq 0}$ is a Markov process with transition semigroup $(Q_t)_{t\geq 0}$, and $h\in B(E)$ is a nonnegative function, then the process $(\e^{-\lambda t}R_\lambda h(X_t))_{t\geq 0}$ is a supermartingale.
\begin{proof}
	Clearly $\e^{-\lambda t}R_\lambda h(X_t)$ is bounded, hence in $L^1$. For every $s,t\geq 0$, we have
	\begin{align*}
		Q_sR_\lambda h = \int_0^\infty\e^{-\lambda t}Q_{t+s}h\,\d t = \e^{\lambda s}\int_0^\infty\e^{-\lambda(s+t)}Q_{t+s}h\,\d t\leq \e^{\lambda s}R_\lambda h.
	\end{align*}
	By \hyperref[eq:6.2]{(6.2)}, the following inequality holds for $(X_t)_{t\geq 0}$:
	\begin{align*}
		\E[\e^{-\lambda(t+s)}R_\lambda h(X_{t+s})|\mathscr{F}_t]=e^{-\lambda(t+s)}Q_sR_\lambda h(X_t)\leq \e^{\lambda t} R_\lambda h(X_t)
	\end{align*}
	Therefore $(\e^{-\lambda t}R_\lambda h(X_t))_{t\geq 0}$ is a martingale.
\end{proof}

\paragraph{} We first consider the case where $E$ is a compact space.

\paragraph{Lemma 6.11.\label{lemma:6.11}} Let $E$ be a compact Hausdorff space and have countable basis. Let $(X_t)_{t\geq 0}$ be a Feller process with transition semigroup $(Q_t)_{t\geq 0}$, with respect to the filtration $(\mathscr{F}_t)_{t\geq 0}$. Then the process $(X_t)_{t\geq 0}$ has a càdlàg modification.
\begin{proof}
Let $\mathscr{H}\in C(E)$ be a countable collection of functions separating points of $E$. We first show that a sequence $x_n\in E$ converges if $h(x_n)$ converges for all $h\in\mathscr{H}$. By compactness of $E$, every sequence of points of $E$ has at least one limit point, and a sequence converges if and only if the limit point is unique. If $x,y\in E$ are both limit points of $x_n$, then $h(x)=\lim_{n\to\infty} h(x_n)=h(y)$ for all $h\in\mathscr{H}$, and $x=y$ by definition of $\mathscr{H}$. Consequently, $x_n$ converges a unique limit point.

We take a sequence $f_n\in C_0^+(E)$ separating the points of $E$, and take \begin{align*}
	\mathscr{H}=\left\{R_pf_n:p\in\mathbb{N},n\in\mathbb{N}\right\}.
\end{align*}
This is also a countable subset of $C(E)$ that separates the points of $E$, since $\Vert pR_pf-f\Vert\to 0$ as $p\to\infty$.

Let $D$ be a countable dense subset of $\mathbb{R}_+$. By \hyperref[prop:6.10]{Proposition 6.10}, if $h\in\mathscr{H}$, there exists $p\in\mathbb{N}$ such that $(\e^{-pt}h(X_t))_{t\geq 0}$ is a supermartingale. By \hyperref[thm:3.43]{Theorem 3.43 (i)}, the left limit $\lim_{D\ni s\downarrow t}h(X_s)$ [resp. the right limit $\lim_{D\ni s\uparrow t}h(X_s)$] exists for all $t\in\mathbb{R}_{++}$ [resp. $t\in\mathbb{R}_+$] except on an event $N_h$ of probability zero. We take $N=\bigcup_{h\in\mathscr{H}}N_h$, hence $(h(X_t))_{t\in D}$ has side limits on $\Omega\backslash N$. Then we define 
\begin{align*}
	\begin{cases}
		\widetilde{X}_t(\omega)=\lim_{D\ni s\downarrow t}X_s(\omega),\quad
		&\omega\in\Omega\backslash N\\
		\widetilde{X}_t(\omega)=x_0,\quad &\omega\in N
	\end{cases}
\end{align*}
where $x_0\in E$ is a fixed point. Clearly this is a càdlàg process. 

It remains to show that $(\widetilde{X}_t)_{t\geq 0}$ is a modification of $(X_t)_{t\geq 0}$. For any $t\geq 0$, take $D\ni t_n\downarrow t$. Then for all $f,g\in C(E)$, we have $$\E[f(X_t)g(\widetilde{X}_t)]=\lim_{n\to\infty}\E[f(X_t)g(X_{t_n})]=\lim_{n\to\infty}\E[f(X_t)Q_{t_n-t}g(X_t)]=\E[f(X_t)g(X_t)].$$
By functional monotone class theorem, $\E[\varphi(X_t,\widetilde{X}_t)]=\E[\varphi(X_t,X_t)]$ for all bounded Borel function on $E\times E$. We take $\varphi(x,y)=\mathds{1}_{\{x=y\}}$, which gives $\P(X_t=\widetilde{X}_t)=1$. Hence $(\widetilde{X}_t)_{t\geq 0}$ is a modification of $(X_t)_{t\geq 0}$.
\end{proof}

\paragraph{Theorem 6.12\label{thm:6.12}} (Regularity of sample paths). Let $E$ be a LCCB space. Let $(X_t)_{t\geq 0}$ be a Feller process with transition semigroup $(Q_t)_{t\geq 0}$, with respect to the filtration $(\mathscr{F}_t)_{t\geq 0}$. Set $\widetilde{\mathscr{F}}_\infty=\mathscr{F}_\infty$, and denote by $\mathscr{N}$ the class of all zero probability sets in $\widetilde{\mathscr{F}}_\infty$. Define the modified filtration: $\widetilde{\mathscr{F}}_t=\sigma\left(\mathscr{F}_{t+}\cup\sigma(\mathscr{N})\right),\ \forall t\geq 0$.

The process $(X_t)_{t\geq 0}$ has a càdlàg modification $(\widetilde{X}_t)_{t\geq 0}$, such that $(\widetilde{X}_t)_{t\geq 0}$ is a Feller process with respect to the modified filtration $(\widetilde{\mathscr{F}}_t)_{t\geq 0}$.
\begin{proof}
Let $E^*=E\cup\{\infty\}$ be the Alexandroff compactification of $E$. According to \hyperref[lemma:6.11]{Lemma 6.11}, and note that $N_h\in\mathscr{F}_\infty$ for all $h\in\mathscr{H}$, we can find a càdlàg modification $(\widetilde{X}_t)_{t\geq 0}$ taking values in $E^*$ and adapted to the filtration $(\widetilde{\mathscr{F}_t})_{t\geq 0}$. We also point out that the filtration $(\widetilde{\mathscr{F}_t})_{t\geq 0}$ is right continuous, so the stopping time we are about to define makes sense.

It is necessary to show $(\widetilde{X}_t)_{t\geq 0}$ is also càdlàg as a process taking values in $E$. We take a strictly positive $g\in C_0(E)$, then the function $h=R_1g\in C_0(E)$ is also strictly positive, and $\bigl(\e^{-t}h(\widetilde{X}_t)\bigr)_{t\geq 0}$ is a nonnegative and càdlàg supermartingale with respect to the filtration $(\widetilde{\mathscr{F}}_t)_{t\geq 0}$. We set
\begin{align*}
	\tau_n=\inf\left\{t\geq 0:\e^{-t}h(\widetilde{X}_t)<\frac{1}{n}\right\},\quad and\quad \tau=\lim_{t\to\infty}\tau_n,
\end{align*}
which are stopping times with respect to the filtration $(\widetilde{\mathscr{F}}_t)_{t\geq 0}$ by \hyperref[prop:3.13]{Proposition 3.13}. By optional sampling theorem [\hyperref[thm:3.48]{Theorem 3.48}], we have
\begin{align*}
	\E\left[\e^{-t}h(\widetilde{X}_t)\right]\leq\E\left[\e^{-\tau_n\wedge t}h(\widetilde{X}_{\tau_n\wedge t})\right] \quad\Rightarrow\quad \E\left[\mathds{1}_{\{\tau_n\leq t\}}\e^{-t}h(\widetilde{X}_t)\right]\leq \E\left[\mathds{1}_{\{\tau_n\leq t\}}\e^{-\tau_n}h(\widetilde{X}_{\tau_n})\right]\leq\frac{1}{n}.
\end{align*}
Letting $n$ increase to $\infty$, we have
\begin{align*}
	\E\left[\mathds{1}_{\{\tau\leq t\}}\e^{-t}h(\widetilde{X}_t)\right]\leq 0.
\end{align*}

Since $\P(\widetilde{X}_t=\infty)=0$, and since $h$ is strictly positive, we have $\tau>t\ a.s.$. Note that $t>0$ is arbitrary. Then $\tau_n\to\infty\ a.s.$, and $\inf_{s\in[0,t]}\e^{-s}h(\widetilde{X}_s)>0\ a.s.$ for all $t>0$. As a result, almost surely, we have $\widetilde{X}_{s-}\neq\infty$ and $\widetilde{X}_s\neq\infty$ for all $s>0$. This extends càdlàg property to $E$.

Finally we verify that $(\widetilde{X}_t)_{t\geq 0}$ is a Markov process with semigroup $(Q_t)_{t\geq 0}$ with respect to the filtration $(\widetilde{\mathscr{F}}_t)_{t\geq 0}$. It suffices to prove that, for all $s\geq 0,t>0$ and $A\in\widetilde{\mathscr{F}}_s$, $f\in C_0(E)$, we have
\begin{align*}
	\E\left[\mathds{1}_Af(\widetilde{X}_{s+t})\right]=\E\left[\mathds{1}_AQ_tf(\widetilde{X}_s)\right].\tag{6.4}\label{eq:6.4}
\end{align*}
We may assume $A\in\mathscr{F}_{s+}$ since it $a.s.$ equals to some $\mathscr{F}_{s+}$-set. Taking $D\ni s_n\downarrow s$, we have
\begin{align*}
	\E\left[\mathds{1}_Af(X_{s+t})\right]=\E\left[\E\left[\mathds{1}_Af(X_{s+t})|\mathscr{F}_{s_n}\right]\right]=\E\left[\mathds{1}_AQ_{t+s-s_n}f(X_{s_n})\right].\label{eq:6.5}\tag{6.5}
\end{align*}
Since $Q_{t+s-s_n}f$ converges uniformly to $Q_tf$, and $X_{s_n}\overset{a.s.}{=}\widetilde{X}_{s_n}\overset{a.s.}{\to}\widetilde{X}_s\overset{a.s.}{=}X_s$, setting $n\to\infty$ in (\ref{eq:6.5}) gives $\E[\mathds{1}_Af(X_{s+t})]=\E[\mathds{1}_AQ_t(X_s)]$. As $(\widetilde{X}_t)_{t\geq 0}$ is a modification of $(X_t)_{t\geq 0}$, this is equivalent to (\ref{eq:6.4}).
\end{proof}

\paragraph{Remark.} In fact, we prove the existence of a Feller process with càdlàg sample paths in this theorem. Given a locally compact Polish space $E$, a starting law $\gamma$ and a Feller semigroup $(Q_t)_{t\geq 0}$, we can construct a canonical process $(X_t^\gamma)$ on the space $E^{\mathbb{R}_+}$, which is a càdlàg Feller process with semigroup $(Q_t)_{t\geq 0}$, according to the Remark under \hyperref[def:6.9]{Definition 6.9} and \hyperref[thm:6.12]{Theorem 6.12}. Meanwhile, this construction, using Kolmogorov extension, gives a probability measure $P^\gamma$ on the measurable space $(\mathbb{D}(E),\mathscr{D})$, where $\mathbb{D}(E)$ is the space of càdlàg paths $\mathbb{R}_+\to E$, and $\mathscr{D}$ is the $\sigma$-algebra on $\mathbb{D}(E)$ generated by all coordinate maps.

\paragraph{Notations.} In later discussions, we often make use of this càdlàg property of Feller processes, which, as is indicated by this theorem, is not a harmful assumption. For every $x\in E$, we use $P^x$ to denote the probability measure on $\mathbb{D}(E)$ which is the law of Feller process $(X_t^x)_{t\geq 0}$ starting from $X_0\sim\delta_x$. Moreover, we use $\E_x$ to denote the expectation taken with respect to $P^x$.

If $\Phi:\mathbb{D}(E)\to\mathbb{R}_+$ is a measurable map, the mapping $x\mapsto\E_x[\Phi]$ is also measurable. To see this, it suffices to consider the case $\Phi=\mathds{1}_A$, where $A\in\mathscr{D}$. If $A$ depends on only finitely many coordinate maps:
\begin{align*}
	A=\left\{f\in\mathbb{D}(E):f(t_1)\in B_1,\cdots,f(t_p)\in B_{t_p}\right\},\quad\textit{where}\ \ 0\leq t_1<\cdots<t_p,\ \ \textit{and}\ \ B_1,\cdots,B_p\in\mathscr{E},\tag{6.6}\label{eq:6.6}
\end{align*}
then the mapping $x\mapsto\E_x[\mathds{1}_A]$ has an explicit form:
\begin{align*}
	\E_x[\mathds{1}_A]= \int_{B_1}Q_{t_1}(x,\d y_1)\int_{B_2}Q_{t_2-t_1}(y_1,\d y_2)\cdots \int_{B_p}Q_{t_p-t_{p-1}}(y_{p-1},\d y_p),
\end{align*}
which is measurable. The remaining case then follows from a $\pi$-$\lambda$ theorem argument.

In addition, for any initial law, the expectation $\E_\gamma$ taken with respect to $P^\gamma$ is given by
\begin{align*}
	\E_\gamma[\mathds{1}_A]=\int\gamma(\d x)\int_{B_1}Q_{t_1}(x,\d y_1)\int_{B_2}Q_{t_2-t_1}(y_1,\d y_2)\cdots \int_{B_p}Q_{t_p-t_{p-1}}(y_{p-1},\d y_p)=\int_E\E_x[\mathds{1}_A]\gamma(\d x).
\end{align*}
Using an argument of $\pi$-$\lambda$ system and a simple function approximation, we obtain
\begin{align*}
	\E_\gamma[\Phi]=\int_E\E_x[\Phi]\gamma(\d x),\quad\forall\ \textit{measurable\ mapping}\ \Phi:(\mathbb{D}(E),\mathscr{D})\to(\mathbb{R}_+,\mathscr{B}(\mathbb{R}_+)).
\end{align*}

\paragraph{Theorem 6.13\label{thm:6.13}} (Simple Markov property). Let $(X_t)_{t\geq 0}$ be a Markov process with semigroup $(Q_t)_{t\geq 0}$ with respect to the filtration $(\mathscr{F}_t)_{t\geq 0}$. Assume that process $(X_t)_{t\geq 0}$ has càdlàg sample paths. Let $s\geq 0$, and let $\Phi:\mathbb{D}(E)\to\mathbb{R}_+$ be a measurable map. Then
\begin{align*}
	\E\left[\Phi((X_{s+t})_{t\geq 0})|\mathscr{F}_s\right]=\E_{X_s}[\Phi]\quad a.s..
\end{align*}
\begin{proof}
Following our preceding discussion, $\E_{X_s}[\Phi]$ is a composition of $X_s$ and the mapping $x\mapsto \E_x[\Phi]$. It suffices to consider the case $\Phi=\mathds{1}_A$, where $A$ is given in (\ref{eq:6.6}). For $\varphi_1,\cdots,\varphi_p\in B(E)$, we have
\begin{align*}
	&\E\left[\varphi_1(X_{s+t_1})\cdots\varphi_p(X_{s+t_p})|\mathscr{F}_s\right] \\
	&= \E\left[\varphi_1(X_{s+t_1})\cdots\varphi_{p-1}(X_{s+t_{p-1}})\E\left[\varphi_p(X_{s+t_p})|\mathscr{F}_{s+t_{p-1}}\right]|\mathscr{F}_s\right]\\
	&= \E\left[\varphi_1(X_{s+t_1})\cdots\varphi_{p-1}(X_{s+t_{p-1}}) Q_{t_p-t_{p-1}}\varphi_p(X_{s+t_{p-1}})|\mathscr{F}_s\right]\\
	&=\E\left[\varphi_1(X_{s+t_1})\cdots\varphi_{p-1}(X_{s+t_{p-1}}) \int_E Q_{t_p-t_{p-1}}(X_{s+t_{p-1}},\d y_p)\varphi_p(y_p)\,\d y_p|\mathscr{F}_s\right]\\
	&=\cdots = \int_E Q_{t_1}(X_s,\d y_1)\varphi_1(y_1)\int_E Q_{t_2-t_1}(y_1,\d y_2)\varphi_2(y_2)\cdots\int_E Q_{t_p-t_{p-1}}(y_{p-1},\d y_p)\varphi_p(y_p).
\end{align*}
Taking $\varphi_j=\mathds{1}_{B_j}$ immediately gives the desired conclusion according to (\ref{eq:6.6}).
\end{proof}

\paragraph{Theorem 6.14\label{thm:6.14}} (Strong Markov property). Let $(X_t)_{t\geq 0}$ be a Feller process with semigroup $(Q_t)_{t\geq 0}$ with respect to the filtration $(\mathscr{F}_t)_{t\geq 0}$. Assume that process $(X_t)_{t\geq 0}$ has càdlàg sample paths. Let $\tau$ be a stopping time of the filtration $(\mathscr{F}_{t+})_{t\geq 0}$, and let $\Phi:\mathbb{D}(E)\to\mathbb{R}_+$ be a measurable map. Then
\begin{align*}
	\E\left[\mathds{1}_{\{\tau<\infty\}}\Phi((X_{\tau+t})_{t\geq 0})|\mathscr{F}_\tau\right]=\mathds{1}_{\{\tau<\infty\}}\E_{X_\tau}[\Phi]\quad a.s..
\end{align*}
\begin{proof}
The right hand side of the last display is a measurable mapping, since the mapping  $\{\tau<\infty\}\ni\omega\mapsto X_\tau(\omega)$ is $\mathscr{F}_\tau$-measurable by \hyperref[prop:3.12]{Proposition 3.12}, and the mapping $x\mapsto\E_x[\Phi]$ is measurable. To show the desired conclusion, it suffices to show
\begin{align*}
	\E\left[\mathds{1}_{A\cap\{\tau<\infty\}}\Phi((X_{\tau+t})_{t\geq 0})\right]=\E\left[\mathds{1}_{A\cap\{\tau<\infty\}}\E_{X_\tau}[\Phi]\right]\quad a.s..
\end{align*}

As is discussed above, we can restrict our discussion to the special case $\Phi(f)=\varphi_1(f(t_1))\cdots\varphi_p(f(t_p))$, where $0\leq t_1<t_2<\cdots<t_p$, $\varphi_1,\cdots,\varphi_p\in B(E)$. 

Consider the case $p=1$. Let $t>0$ and $\varphi\in B(E)$ fixed. By Riesz-Markov representation, we may assume that $\varphi\in C_0(E)$. Denote by $[\tau]_n$ the estimate $\max\{k2^{-n}:k2^{-n}>\tau, k\in\mathbb{N}\}$.  Then
\begin{align*}
	\E\left[\mathds{1}_{A\cap\{\tau<\infty\}}\varphi(X_{\tau+t})\right]&=\lim_{n\to\infty}\E\left[\mathds{1}_{A\cap\{\tau<\infty\}}\varphi(X_{[\tau]_n+t})\right]\\
	&=\lim_{n\to\infty}\E\left[\sum_{k=1}^\infty\mathds{1}_{A\cap\{(k-1)2^{-n}\leq\tau<k2^{-n}\}}\varphi(X_{k2^{-n}+t})\right]\\
	&=\lim_{n\to\infty}\E\left[\sum_{k=1}^\infty\mathds{1}_{A\cap\{(k-1)2^{-n}\leq\tau<k2^{-n}\}}Q_t\varphi(X_{k2^{-n}})\right]\\
	&=\lim_{n\to\infty}\E\left[\mathds{1}_{A\cap\{\tau<\infty\}}Q_t\varphi(X_{[\tau]_n})\right]=\E\left[\mathds{1}_{A\cap\{\tau<\infty\}}Q_t\varphi(X_{\tau})\right],
\end{align*}
where first and last equalities uses right-continuity of sample paths and the fact that $\varphi,Q_t\varphi\in C_0(E)$, and the third equality uses \hyperref[prop:3.10]{Proposition 3.10 (ii)}, which implies $A\cap\{(k-1)2^{-n}\leq\tau<k2^{-n}\}\in\mathscr{F}_{k2^{-n}}$.

For the general case $p\in\mathbb{N}$, we have
\begin{align*}
	&\E\left[\mathds{1}_{A\cap\{\tau<\infty\}}\varphi_1(X_{\tau+t_1})\cdots\varphi_p(X_{\tau+t_p})\right]\\
	&= \E\left[\mathds{1}_{A\cap\{\tau<\infty\}}\varphi_1(X_{\tau+t_1})\cdots\varphi_{p-1}(X_{\tau+t_{p-1}})\E\left[\mathds{1}_{\{\tau<\infty\}}\varphi_p(X_{\tau+t_p})|\mathscr{F}_{\tau+t_{p-1}}\right]\right]\\
	&=\E\left[\mathds{1}_{A\cap\{\tau<\infty\}}\varphi_1(X_{\tau+t_1})\cdots\varphi_{p-1}(X_{\tau+t_{p-1}})\int_E Q_{t_p-t_{p-1}}(X_{\tau+t_{p-1}},\d y_p)\varphi_p(y_p)\right]\\
	&=\cdots = \E\left[\mathds{1}_{A\cap\{\tau<\infty\}}\int_E Q_{t_1}(X_\tau,\d y_1)\varphi_1(y_1)\int_E Q_{t_2-t_1}(y_1,\d y_2)\varphi_2(y_2)\cdots\int_E Q_{t_p-t_{p-1}}(y_{p-1},\d y_p)\varphi_p(y_p)\right].
\end{align*}
This concludes the proof of strong Markov property.
\end{proof}

\paragraph{Remark.} In this theorem, we assume that $\tau$ is a stopping time of the filtration $(\mathscr{F}_{t+})_{t\geq 0}$. This is a more general assumption than a stopping time of the filtration $(\mathscr{F}_t)_{t\geq 0}$.

\newpage
\subsection{The Generator and Feynman-Kac Formula}
In this section, we are going to discuss some properties of generators. We first study the Brownian process, whose generator has a simple form.

\paragraph{Example 6.15.\label{example:6.15}} A Brownian motion $B=(B_t)_{t\geq 0}$ is a real-valued Markov process with transition group
\begin{align*}
	Q_0(x,A)=\mathds{1}_A(x),\ \ Q_t(x,A)=\int_A\frac{1}{\sqrt{2\pi t}}\e^{-\frac{(y-x)^2}{2t}}\,\d y,\quad \forall t> 0,\ x\in\mathbb{R},\ A\in\mathscr{B}(\mathbb{R}).
\end{align*}

\paragraph{We first verify that $(Q_t)_{t\geq 0}$ is a Feller semigroup.} We fix $f\in C_0(\mathbb{R})$, and there exists $M>0$ such that $\Vert f\Vert\leq M$. For any $x_0\in\mathbb{R}$, by Lebesgue dominated convergence theorem,
\begin{align*}
	\lim_{x\to x_0}Q_t f(x) = \lim_{x\to x_0}\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi t}}\e^{-\frac{(y-x)^2}{2t}}f(y)\,\d y = \int_{-\infty}^\infty\frac{1}{\sqrt{2\pi t}}\e^{-\frac{(y-x_0)^2}{2t}}f(y)\,\d y = Q_tf(x_0).
\end{align*}

Given any $\epsilon>0$, we choose $K>0$ such that $\vert f(y)\vert<\epsilon/2$ for all $\vert y\vert>K$, and choose $\alpha>0$ such that
\begin{align*}
	\int_\alpha^{\infty}\frac{1}{\sqrt{2\pi t}}\e^{\frac{-z^2}{2t}}\,\d z<\frac{\epsilon}{2M}\quad\Leftrightarrow\quad \int_{-\infty}^{-\alpha}\frac{1}{\sqrt{2\pi t}}\e^{\frac{-z^2}{2t}}\,\d z<\frac{\epsilon}{2}.
\end{align*}
Then for all $x>K+\alpha$, we have $\vert f(z+x)\vert<\epsilon/2$ for all $z>-\alpha$, and
\begin{align*}
	\vert Q_tf(x)\vert = \left\vert\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi t}}\e^{-\frac{(y-x)^2}{2t}}f(y)\,\d y\right\vert\leq M\int_{-\infty}^{-\alpha}\frac{1}{\sqrt{2\pi t}}\e^{\frac{-z^2}{2t}}\,\d z + \int_{-\alpha}^\infty\frac{1}{\sqrt{2\pi t}}\e^{\frac{-z^2}{2t}}\vert f(z+x)\vert\,\d z<\alpha.
\end{align*}
Hence we conclude that $Q_tf\in C_0(\mathbb{R})$. To show continuity of $(Q_t)_{t\geq 0}$, we fix $\eta>0$. Then we choose $\delta>0$ such that $\vert f(y)-f(x_0)\vert<\eta/2$ for all $\vert y-x_0\vert\leq\delta$:
\begin{align*}
	\left\vert\int_{\vert y-x_0\vert\leq\delta}\frac{1}{\sqrt{2\pi t}}\e^{-\frac{(y-x_0)^2}{2t}}(f(y)-f(x_0))\,\d y\right\vert<\frac{\eta}{2}\left\vert\int_{\vert y-x_0\vert\leq\delta}\frac{1}{\sqrt{2\pi t}}\e^{-\frac{(y-x_0)^2}{2t}}\,\d y\right\vert\leq\frac{\eta}{2},\ \ \forall t>0;
\end{align*}
and choose $t_0>0$ such that
\begin{align*}
	\int_{\vert z\vert\geq\delta}\frac{1}{\sqrt{2\pi t_0}}\e^{-\frac{z^2}{2t_0}}\,\d z < \frac{\eta}{4M}\quad\Rightarrow\quad \left\vert\int_{\vert y-x_0\vert\geq\delta}\frac{1}{\sqrt{2\pi t}}\e^{-\frac{(y-x_0)^2}{2t}}(f(y)-f(x_0))\,\d y\right\vert<\frac{\eta}{2},\ \ \forall t\in(0,t_0).
\end{align*}
Consequently, we have $\left\vert(Q_t f- f)(x_0)\right\vert<\eta$ for all $t\in(0,t_0)$, and $Q_tf(x_0)\to f(x_0)$ as $t\downdownarrows 0$.

\paragraph{Resolvent and generator.} For $\lambda>0$, the resolvent is
\begin{align*}
	R_\lambda f(x) = \int_0^\infty\e^{-\lambda t}Q_t f(x)\,\d t &= \int_{-\infty}^\infty\underbrace{\left(\int_0^\infty\frac{1}{\sqrt{2\pi t}}\e^{-\lambda t-\frac{(y-x)^2}{2t}}\,\d t\right)}_{r_\lambda(y-x)}f(y)\,\d y\\
	&=\int_{-\infty}^\infty \frac{1}{\vert y-x\vert}\E\left[\tau_y\e^{-\lambda\tau_y}\right]f(y)\,\d y,
\end{align*}
where the hitting time $\tau_y=\inf\{t\geq 0:B_t=\vert y-x\vert\}$ has density $\frac{\vert y-x\vert}{\sqrt{2\pi t^3}}\e^{-\frac{(y-x)^2}{2t}}\,\d t$. By \hyperref[prop:4.27]{Proposition 4.27},
\begin{align*}
	r_\lambda(y-x)=\frac{1}{\vert y-x\vert}\frac{\d}{\d\lambda}\E[\e^{-\lambda\tau_y}] = \frac{1}{\sqrt{2\lambda}}\e^{-\vert y-x\vert\sqrt{2\lambda}}.
\end{align*}
This gives the formula of $R_\lambda$:
\begin{align*}
	R_\lambda f(x)=\frac{1}{\sqrt{2\lambda}}\int_{-\infty}^\infty\e^{-\vert y-x\vert\sqrt{2\lambda}}f(y)\,\d y.
\end{align*}

Now we find the generator $L$ of $(Q_t)_{t\geq 0}$. If $h\in\mathfrak{D}(L)$, there exists $f\in C_0(\mathbb{R})$ such that $R_\lambda f=h$. Taking $\lambda=1/2$, and $\mathrm{sgn}(z)=\mathds{1}_{\{z\geq 0\}}-\mathds{1}_{\{z<0\}}$, we have
\begin{align*}
	h(x)=\int_{-\infty}^\infty \e^{-\vert y-x\vert}f(y)\,\d y\quad\Rightarrow\quad h^\prime(x)=\int_{-\infty}^\infty\mathrm{sgn}(y-x)\e^{-\vert y-x\vert}f(y)\,\d y.
\end{align*}
Furthermore, $h^\prime$ is differentiable: for all $x\in\mathbb{R}$,
\begin{align*}
	h^\prime(x+\delta)-h^\prime(x)&=\int_{-\infty}^\infty\mathrm{sgn}(y-x-\delta)\e^{-\vert y-x-\delta\vert}f(y)\,\d y-\int_{-\infty}^\infty\mathrm{sgn}(y-x)\e^{-\vert y-x\vert}f(y)\,\d y\\
	&=\int_{\mathbb{R}\backslash[x,x+\delta]}\mathrm{sgn}(y-x)\left(\e^{-\vert y-x-\delta\vert}-\e^{-\vert y-x\vert}\right)f(y)\,\d y-\int_x^{x+\delta}\left(\e^{y-x-\delta}+\e^{x-y}\right)f(y)\,\d y,
\end{align*}
Hence we have
\begin{align*}
	\lim_{\delta\downdownarrows 0}\frac{h^\prime(x+\delta)-h^\prime(x)}{\delta}=h(x)-2f(x).
\end{align*}
A similar argument also holds for the left limit, hence $h^{\prime\prime}=h-2f$. By \hyperref[prop:6.8]{Proposition 6.8}, we have
\begin{align*}
	\left(\frac{1}{2}\id - L\right)h=\left(\frac{1}{2}\id - L\right)R_{1/2}f=f\ \Rightarrow\  Lh=\frac{1}{2}h^{\prime\prime},\quad \textit{where}\ \ h\in \mathfrak{D}(L)\subset\left\{h\in C^2(\mathbb{R}): g,g^{\prime\prime}\in C_0(\mathbb{R})\right\}.
\end{align*}

In fact, we can show that $\mathfrak{D}(L)=\left\{g\in C^2(\mathbb{R}): g,g^{\prime\prime}\in C_0(\mathbb{R})\right\}$. If $g$ is a twice continuously differentiable function with $g,g^{\prime\prime}\in C_0(\mathbb{R})$, we take $f=\frac{1}{2}(g-g^{\prime\prime})\in C_0(\mathbb{R})$. Then $h=R_{1/2}f\in\mathfrak{D}(L)$, and the preceding argument gives $h^{\prime\prime}=h-2f$. This yields $(h-g)^{\prime\prime}=h-g\ \Rightarrow\ (h-g)(x)=C_1\e^x+C_2\e^{-x}$, where $C_1,C_2\in\mathbb{R}$. Since $h-g\in C_0(\mathbb{R})$, we have $g=h\in\mathfrak{D}(L)$.

\paragraph{Proposition 6.16.\label{prop:6.16}} Let $d\in\mathbb{N}$, and let $B=(B_t)_{t\geq 0}$ be a $d$-dimensional Brownian motion. The infinitesimal generator of $B$ is equal to $\frac{1}{2}\Delta$ on the space $C_0^2(\mathbb{R}^d)$.
\begin{proof}
For $f\in C_0(\mathbb{R}^d)$, we write
\begin{align*}
	Q_tf(x) = \frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^d}\e^{-\vert z\vert^2/2}f(x+z\sqrt{t})\,\d z,\tag{6.7}\label{eq:6.7}
\end{align*}
where $\vert z\vert^2=\sum_{j=1}^d z_j^2$. If $f\in C_0^2(\mathbb{R}^d)$, the chain rule gives
\begin{align*}
	\frac{\partial}{\partial u}f(x+zu)=z^\top\nabla f(x+zu),\quad \frac{\partial^2}{\partial u^2}f(x+zu)=z^\top\nabla^2f(x+zu)z,
\end{align*}
where $\nabla^2f=[\frac{\partial^2}{\partial x_i\partial x_j}f]_{i,j\in[n]}$ is Hessian matrix of $f$. By Taylor's formula, there exists $\theta(t,z)\in[0,1]$ such that
\begin{align*}
	Q_tf(x)&=f(x) + (2\pi)^{-d/2}\frac{t}{2}\int_{\mathbb{R}^d}\e^{-\vert z\vert^2/2}z^\top\nabla^2f(x+\theta(t,z)z\sqrt{t})z\,\d z\\
	&= f(x)+\frac{t}{2}\Delta f(x)+(2\pi)^{-d/2}\frac{t}{2}J(t,x),
\end{align*}
where
\begin{align*}
	J(t,x)=\int_{\mathbb{R}^d}\e^{-\vert z\vert^2/2}\left[\sum_{i,j=1}^d\left(\frac{\partial^2f}{\partial x_i\partial x_j}(x+\theta(t,z) z\sqrt{t})-\frac{\partial^2f}{\partial x_i\partial x_j}(x)\right)z_iz_j\right]\,\d z.
\end{align*}
We set the following function $F$, and use uniform continuity of second partial derivatives of $f$:
\begin{align*}
	F(x,z,t)=\max_{i,j\in[d]}\left\vert \frac{\partial^2f}{\partial x_i\partial x_j}(x+\theta(t,z) z\sqrt{t})-\frac{\partial^2f}{\partial x_i\partial x_j}(x)\right\vert\quad\Rightarrow\quad\limsup_{t\downdownarrows 0}F(x,z,t)=0.
\end{align*}
Moreover, for any $R>0$, one have
\begin{align*}
	\vert J(t,x)\vert\leq\int_{\vert z\vert\leq R}F(x,t,z)\,\e^{-\vert z\vert^2/2}\Vert z\Vert_1^2\,\d z+2\max_{i,j\in[d]}\left\Vert \frac{\partial^2 f}{\partial x_i\partial x_j}\right\Vert\int_{\vert z\vert>R}\e^{-\vert z\vert^2/2}\Vert z\Vert_1^2\,\d z.
\end{align*}
Since the first term converges to $0$ as $t\downdownarrows 0$ by dominated convergence theorem for any $R>0$, we have
\begin{align*}
	\lim_{t\downdownarrows 0}\left\Vert \frac{1}{t}(Q_t f-f)-\frac{1}{2}\Delta f\right\Vert\leq\max_{i,j\in[d]}\left\Vert \frac{\partial^2 f}{\partial x_i\partial x_j}\right\Vert\int_{\vert z\vert>R}\e^{-\vert z\vert^2/2}\Vert z\Vert_1^2\,\d z,\quad\forall R>0.
\end{align*}
Take $R\to\infty$. Then we have $Lf=\frac{1}{2}\Delta f$ for all $f\in C_0^2(\mathbb{R})$.
\end{proof}

\paragraph{Remark.} For the case $d\geq 2$, the space $C_0^2(\mathbb{R}^d)$ is not equal to $\mathfrak{D}(L)$. One can show that $\mathfrak{D}(L)$ is the subspace of $C_0(\mathbb{R}^d)$ of functions $f$ such that $\delta f$ taken in weak sense is in $C_0(\mathbb{R}^d)$. 

In addition, according to \hyperref[prop:6.8]{Proposition 6.8}, we have for all $\varphi\in C_0^2(\mathbb{R}^d)$ that
\begin{align*}
	\frac{\d}{\d t}Q_t\varphi = \frac{1}{2}Q_t\Delta \varphi=\frac{1}{2}\Delta Q_t\varphi
\end{align*}
Consequently, the function $u:\mathbb{R}_+\times\mathbb{R}^d\to\mathbb{R},\ (t,x)\mapsto Q_t\varphi(x)$ solves the following partial differential equation:
\begin{align*}
	\begin{cases}
		\frac{\partial u}{\partial t}(t,x) = \frac{1}{2}\Delta_x u(t,x),\ \ t>0\\
		u(0,x)=\varphi(x)
	\end{cases}
\end{align*}
This is called the \textit{heat diffusion equation}. Note that $(Q_t)_{t\geq 0}$ is defined in (\ref{eq:6.7}).

\paragraph{} The following theorem gives a characterization for the domain of the generator of a Feller semigroup $(Q_t)_{t\geq 0}$. For any $x\in E$, we can construct a Markov process $(X_t^x)_{t\geq 0}$ such that $\P(X_0=x)=1$ with semigroup $(Q_t)_{t\geq 0}$.

\paragraph{Theorem 6.17.\label{thm:6.17}} Let $h,g\in C_0(E)$. The following conditions are equivalent:
\begin{itemize}
	\item[(i)] $h\in\mathfrak{D}(L)$ and $Lh=g$;
	\item[(ii)] For every $x\in E$, the process
	\begin{align*}
		h(X_t^x) - \int_0^t g(X_s^x)\,\d s
	\end{align*}
	is a martingale with respect to the filtration $\mathscr{F}_t^x = \sigma(X_s^x,0\leq s\leq t)$.
\end{itemize}
\begin{proof}
	(i) $\Rightarrow$ (ii): Let $h\in\mathfrak{D}(L)$ and $Lh=g$. By \hyperref[eq:6.1]{(6.1)}, we have
	\begin{align*}
		Q_th = h+\int_0^t Q_sg\,\d s \quad\Rightarrow\quad \E\left[h(X_{t+s}^x)|\mathscr{F}_t^x\right]=Q_sh(X_t^x)=h(X_t^x)+\int_0^s Q_rg(X_t^x)\,\d r.\tag{6.8}\label{eq:6.8}
	\end{align*}
	Meanwhile, use the boundedness of $g$, we have
	\begin{align*}
		\E\left[\left.\int_t^{t+s}g(X_r^x)\,\d r\right|\mathscr{F}_t^x\right] = \int_t^{t+s}\E\left[g(X_r^x)|\mathscr{F}_t\right]=\int_t^{t+s}Q_{r-t}g(X_t^x)\,\d r = \int_0^s Q_rg(X_t^s)\,\d r.\tag{6.9}\label{eq:6.9}
	\end{align*}
	Combining (\ref{eq:6.8}) and (\ref{eq:6.9}), we have the martingale property:
	\begin{align*}
		\E\left[\left.h(X_{t+s}^x)-\int_0^{t+s}g(X_r^x)\,\d r\right|\mathscr{F}_t^x\right]=h(X_t^x) - \int_0^tg(X_r^x),\quad\forall t+s > t\geq 0.
	\end{align*}
	(ii) $\Rightarrow$ (i): For every $x\in E$ and every $t\geq 0$, we have
	\begin{align*}
		Q_th(x) - \int_0^t Q_rg(x)\,\d r=\E\left[h(X_t^x)-\int_0^tg(X_r^x)\,\d r\right] = h(x),
	\end{align*}
	where the first equality follows from Markov property, and the second from martingale property. Hence
	\begin{align*}
		\lim_{t\downdownarrows 0}\frac{Q_th-h}{t} - g =\lim_{t\downdownarrows 0}\frac{1}{t}\int_0^t (Q_rg - g)\,\d r = 0.
	\end{align*}
	Since $g\in C_0(E)$, we have $h\in\mathfrak{D}(L)$, and $Lh=g$.
\end{proof}

Now we introduce one case of Feynman-Kac formula.
\paragraph{Theorem 6.18.\label{thm:6.18}} (Feynman-Kac formula). Let $\upsilon\in C_0(E)$ be a nonnegative function. For every $t\geq 0$, define for every $\varphi\in B(E)$ and every $x\in E$ that
\begin{align*}
K_t\varphi(x)=\E\left[\varphi(X_t^x)\exp\left(-\int_0^t\upsilon(X_s^x)\,\d s\right)\right],
\end{align*}
where $(X_t^x)_{t\geq 0}$ is a càdlàg Feller process with semigroup $(Q_t)_{t\geq 0}$ starting from $X_0\sim\delta_x$.
\begin{itemize}
	\item[(i)] $(K_t)_{t\geq 0}$ is a semigroup of contractions on $B(E)$.
	\item[(ii)] If $\varphi\in\mathfrak{D}(L)$, then
	\begin{align*}
		\frac{\d}{\d t}K_t\varphi|_{t=0} = L\varphi - \upsilon\varphi.
	\end{align*}
\end{itemize}
\begin{proof}
(i) Fix $x\in E$, and let $(X_t)_{t\geq 0}$ be a càdlàg Feller process with semigroup $(Q_t)_{t\geq 0}$ starting from $X_0\sim\delta_x$. Clearly, $K_t$ is a contraction: $K_t\varphi(x)\leq\E\left[\varphi(X_t)\right]=Q_t\varphi(x)\leq\Vert\varphi\Vert$. Next, we let $\pi_t(f)=f(t)$ be the projection map from $\mathbb{D}(E)\to E$, and fix $s,t\geq 0$. By definition of $K_t$ and simple Markov property,
\begin{align*}
	K_s(K_t\varphi)(x) &= \E\left[(K_t\varphi)(X_s)\exp\left(-\int_0^s\upsilon(X_s)\,\d s\right)\right]\\
	&=\E\left[\E_{X_s}\biggl[(\varphi\circ \pi_t)\exp\left(-\int_0^t\upsilon\circ\pi_r\,\d r\right)\biggr]\exp\left(-\int_0^s\upsilon(X_r)\,\d r\right)\right]\\
	&=\E\left[\E\left[\left.\varphi(X_{s+t})\exp\left(-\int_0^t\upsilon(X_{s+r})\,\d r\right)\right|\mathscr{F}_s^X\right]\exp\left(-\int_0^s\upsilon(X_r)\,\d r\right)\right]\\
	&=\E\left[\varphi(X_{s+t})\exp\left(-\int_0^{s+t}\upsilon(X_{r})\,\d r\right)\right] = (K_{s+t}\varphi)(x).
\end{align*}

(ii) Since $\frac{\d}{\d s}\e^{-\int_s^tf(r)\,\d r}=f(s)\e^{-\int_s^tf(r)\,\d r}$, the fundamental theorem of calculus gives
\begin{align*}
	1-\e^{-\int_0^t\upsilon(X_r)\,\d r}=\int_0^t\upsilon(X_s)\e^{-\int_s^t\upsilon(X_r)\,\d r}\,\d s.
\end{align*}
By Fubini's theorem and simple Markov property, for every $\varphi\in B(E)$, we have
\begin{align*}
	K_t\varphi(x)&=\E\left[\varphi(X_t)\left(1-\int_0^t\upsilon(X_s)\exp\left(-\int_s^t\upsilon(X_r)\,\d r\right)\,\d s\right)\right]\\
	&= Q_t\varphi(x) - \int_0^t\E\left[\varphi(X_t)\upsilon(X_s)\exp\left(-\int_s^t\upsilon(X_r)\,\d r\right)\right]\,\d s\\
	&= Q_t\varphi(x) - \int_0^t\E\left[\upsilon(X_s)\,\E\left[\left.\varphi(X_t)\exp\left(-\int_s^t\upsilon(X_r)\,\d r\right)\right|\mathscr{F}_s^X\right]\right]\,\d s\\
	&= Q_t\varphi(x) - \int_0^t\E\left[\upsilon(X_s)K_{t-s}\varphi(X_s)\right]\,\d s\\
	&= Q_t\varphi(x) - \int_0^t Q_s(\upsilon\,K_{t-s}\varphi)(x)\,\d s.
\end{align*}
Hence $\lim_{t\downarrow 0}K_t\varphi=\varphi$. Furthermore,
\begin{align*}
	 \left\vert\frac{1}{t}\int_0^t Q_s(\upsilon\,K_{t-s}\varphi)(x)\,\d s - v(x)\varphi(x)\right\vert
	 &= \frac{1}{t}\left\vert\int_0^t Q_s(\upsilon\,K_{t-s}\varphi-\upsilon\varphi)(x)\,\d s\right\vert + \frac{1}{t}\left\vert\int_0^t(Q_s-\id)(\upsilon\varphi)(x)\,\d s\right\vert\\
	 &\leq\frac{1}{t}\int_0^t\left\Vert\upsilon K_{t-s}\varphi-\upsilon\varphi\right\Vert_\infty\d s + \frac{1}{t}\int_0^t\left\Vert(Q_s-\id)(\upsilon\varphi)\right\Vert_\infty\d s\\
	 &\leq\frac{1}{t}\int_0^t\left\Vert\upsilon\right\Vert_\infty\left\Vert K_s\varphi-\varphi\right\Vert_\infty\d s + \frac{1}{t}\int_0^t\left\Vert(Q_s-\id)(\upsilon\varphi)\right\Vert_\infty\d s
\end{align*}
Letting $t\downarrow 0$, the last display converges to zero. Consequently, we have
\begin{align*}
	\frac{\d}{\d t}K_t\varphi|_{t=0}=\frac{\d}{\d t}Q_t\varphi|_{t=0} - \frac{\d}{\d t}\int_0^t Q_s(\upsilon\,K_{t-s}\varphi)(x)\,\d s|_{t=0}=L\varphi-\upsilon\varphi,
\end{align*}
which complete the proof.
\end{proof}
\paragraph{Remark.} To find the derivative of $t\mapsto K_t\varphi$, we use the semigroup property:
\begin{align*}
	\frac{\d}{\d t}K_t\varphi = \frac{\d}{\d s}K_{s+t}\varphi|_{s=0} = \frac{\d}{\d s}K_s(K_t\varphi)|_{s=0} = LK_t\varphi - \upsilon K_t\varphi.
\end{align*}
Therefore, for any $f\in B(E)$, the function 
\begin{align*}
	u(t,x)=K_tf(x)=\E\left[f(X_t^x)\exp\left(-\int_0^t\upsilon(X_s^x)\,\d s\right)\right]
\end{align*} 
solves the following initial value problem (which is often a parabolic PDE):
\begin{align*}
\begin{cases}
	\displaystyle\frac{\partial u}{\partial t} = Lu-\upsilon u,\quad t>0\\ u(0,x)=f(x).
\end{cases}
\end{align*}
Here $(X_t^x)$ is a càdlàg Feller process with generator $L$ starting from $X_0\sim\delta_x$. Letting $\upsilon=0$ gives the \textit{Kolmogorov backward equation}, which coincides the form of (\ref{eq:6.1}).

\subsection{Diffusion Processes}
In this section, we discuss the solution of SDE $E(\sigma,b)$
\begin{align*}
	\d X_t = \sigma(X_t)\,\d B_t + b(X_t)\,\d t,\quad \sigma=(\sigma_{ij})_{i\in[p],j\in[q]},\ b=(b_i)_{i\in[p]}.\label{eq:6.10}\tag{6.10}
\end{align*} 
in form of a Markov process. The processes with continuous sample paths that are obtained as solutions
of SDE (\ref{eq:6.10}) are called \textit{diffusion processes}. The function $b:\mathbb{R}^p\to\mathbb{R}^p$ is called the \textit{drift coefficient}, and $\sigma:\mathbb{R}^p\to\mathbb{R}^{p\times q}$ is called the \textit{diffusion coefficient}.

We tackle with the homogeneous case where $\sigma(t,x)=\sigma(x)$ and $b(t,x)=b(x)$, and we maintain the Lipschitz assumption: there exists a constant $K>0$ such that for all $x,y\in\mathbb{R}^q$,
\begin{align*}
	\left\vert\sigma(x)-\sigma(y)\right\vert\leq K\vert x-y\vert,\quad \left\vert b(x)-b(y)\right\vert\leq K\vert x-y\vert.
\end{align*}
Here we use $\vert\cdot\vert$ to denote the Euclidean norm of vectors and the Frobenius norm of matrices.

\paragraph{Theorem 6.19.\label{thm:6.19}} Assume that $X=(X_t)_{t\geq 0}$ is a solution of SDE $E(\sigma,b)$ on a complete filtered probability space $(\Omega,\mathscr{F},(\mathscr{F}_t)_{t\geq 0},\P)$. Then $(X_t)_{t\geq 0}$ is a Markov process with respect to the filtration $(\mathscr{F}_t)_{t\geq 0}$, with semigroup $(Q_t)_{t\geq 0}$ defined by
\begin{align*}
	Q_tf(x) = \E\left[f(X_t^x)\right],
\end{align*}
where $X^x=(X_t^x)_{t\geq 0}$ is an arbitrary solution of $E^x(\sigma,b)$. Using the notation of \hyperref[thm:5.24]{Theorem 5.24}, we also write
\begin{align*}
	Q_tf(x) = \int f(F_x(\bfw))_t\,W(\d\bfw).\label{eq:6.11}\tag{6.11}
\end{align*}
\begin{proof}
We first prove that, for any $f\in B(\mathbb{R}^p)$ and any $s,t\geq 0$, we have
\begin{align*}
	\E\left[f(X_{s+t})|\mathscr{F}_s\right]=Q_tf(X_s),
\end{align*}
To deal the time shift $s$, we define filtration $(\mathscr{F}^\prime_t)_{t\geq 0}$ and processes $(X_t^\prime)_{t\geq 0}$, $(B^\prime_t)_{t\geq 0}$ as follows:
\begin{align*}
	\mathscr{F}^\prime_t = \mathscr{F}_{s+t},\quad X_t^\prime = X_{s+t},\quad B_t^\prime=B_{t+s}-B_s.
\end{align*}
Then $(\mathscr{F}^\prime_t)_{t\geq 0}$ is still complete, $X^\prime$ is adapted to $(\mathscr{F}^\prime_t)_{t\geq 0}$, and $B^\prime$ is a $q$-dimensional $(\mathscr{F}^\prime_t)$-Brownian motion. Furthermore, the approximation formula (\ref{eq:5.8}) for the integral of sample-continuous adapted processes gives
\begin{align*}
	X_t^\prime = X_{s+t} &= X_s + \int_s^{s+t}\sigma(X_r)\,\d B_r + \int_s^{s+t}b(X_r)\,\d r\\
	&= X_s+\int_0^t\sigma(X_r^\prime)\,\d B^\prime_r + \int_0^t b(X_r^\prime)\,\d r.
\end{align*}
Consequently, $X^\prime$ solves $E(\sigma,b)$ on the space $(\Omega,\mathscr{F},(\mathscr{F}_t^\prime)_{t\geq 0},\P)$ and with Brownian motion $B^\prime$, with $X_0^\prime=X_s$. By \hyperref[thm:5.24]{Theorem 5.24 (iii)}, we have $X^\prime=F_{X_s}(B^\prime)\ a.s.$, which implies
\begin{align*}
	\E\left[f(X_{s+t})|\mathscr{F}_s\right]=\E\left[f(X^\prime_t)|\mathscr{F}_s\right] &= \E\left[f(F_{X_s}(B^\prime)_t)|\mathscr{F}_s\right] \\
	&= \int f(F_{X_s}(\bfw)_t)\,W(\d\bfw) = Q_tf(X_s),
\end{align*}
where in the third equality we use the independence of $B^\prime$ and $\mathscr{F}_s$. 

Now it remains to verify that $(Q_t)_{t\geq 0}$ is a transition semigroup. Clearly, $Q_0$ is an identity map, and $(t,x)\mapsto Q_tf(x)$ is a continuous map, hence is measurable. Finally, one have
\begin{align*}
	Q_{s+t}f(x)=\E\left[X_{s+t}^x\right]=\E\left[\E\left[X_{s+t}^x|\mathscr{F}_s\right]\right]= \E\left[Q_tf(X_s^x)\right]=\int Q_s(x,\d y)Q_tf(y)
\end{align*}
which is the Chapman-Kolmogorov equation. This completes the proof.
\end{proof}

We then give an estimate for the second moment of a diffusion process.
\paragraph{Lemma 6.20.\label{lemma:6.20}} Fix $x\in\mathbb{R}^p$, and let $(X_t^x)_{t\geq 0}$ be a solution of the SDE $E^x(\sigma,b)$. Then there exists a constant $C_x>0$ depending only on $x$, such that for all $t\geq 0$,
\begin{align*}
	\E\left[\vert X_t^x - x\vert^2\right]\leq C_x\e^{C_x(t+t^2)}(t+t^2).
\end{align*}
\begin{proof}
By triangle inequality and Lipschitz property, for all $t\geq 0$, we have
\begin{align*}
	\vert\sigma(X_t^x)\vert^2\leq\left(\vert\sigma(x)\vert + \vert\sigma(X_t^x)-\sigma(x)\vert\right)^2\leq 2\vert\sigma(x)\vert^2 + 2K^2\vert X_t^x-x\vert^2.\label{eq:6.12}\tag{6.12}
\end{align*}
A similar formula also holds for $\vert b(X_t^x)\vert^2$. 

We define a stopping time $\tau=\inf\{t\geq 0:\vert X_t^x-x\vert>M\}$ for some $M>0$, and fix $T>0$. Then the function $t\mapsto\E\left[\vert X_{t\wedge\tau}^x - x\vert^2\right]$ is bounded on $[0,T]$. For any $t\in[0,T]$, we have
\begin{align*}
	\E\left[\vert X_{t\wedge\tau}^x - x\vert^2\right]&\leq  2\E\left[\left(\int_0^{t\wedge\tau}\sigma(X_s^x)\,\d B_s\right)^2\right] + 2\E\left[\left(\int_0^{t\wedge\tau} b(X_s^x)\,\d s\right)^2\right]\\
	&\leq 2\E\left[\int_0^{t\wedge\tau}\vert\sigma(X_s^x)\vert^2\,\d s\right] + 2\E\left[T\int_0^{t\wedge\tau}\vert b(X_s^x)\vert^2\,\d s\right]\\
	&\leq 4T(\vert\sigma(x)\vert^2 + T\vert b(x)\vert^2) + 4K^2(1+T)\int_0^{t\wedge\tau}\E\left[\vert X_s^x-x\vert^2\right]\d s\\
	&\leq 4T(\vert\sigma(x)\vert^2 + T\vert b(x)\vert^2) + 4K^2(1+T)\int_0^t\E\left[\vert X_{s\wedge\tau}^x-x\vert^2\right]\d s.
\end{align*}
where we use (\ref{eq:6.12}) in the third inequality. By Gronwall's lemma [\hyperref[lemma:5.22]{Lemma 5.22}], we have
\begin{align*}
	\E\left[\vert X_{t\wedge\tau}^x - x\vert^2\right]\leq 4T(\vert\sigma(x)\vert^2 + T\vert b(x)\vert^2)\e^{4K^2(1+T)t},\quad\forall t\in[0,T].
\end{align*}
Let $M\to\infty$, and use the monotone convergence theorem. Then for all $t\geq 0$, we have
\begin{align*}
	\E\left[\vert X_t^x - x\vert^2\right]\leq 4t\left(\vert\sigma(x)\vert^2 + t\vert b(x)\vert^2\right)\e^{4K^2(t+t^2)}.
\end{align*}
Setting $C_x=4\max\left\{\vert\sigma(x)\vert^2,\vert b(x)\vert^2,K^2\right\}$ concludes the proof.
\end{proof}

To move forward, we have the following conclusion.
\paragraph{Theorem 6.21.\label{thm:6.21}} The semigroup $(Q_t)_{t\geq 0}$ is a Feller semigroup, and its generator $L$ satisfies $\mathfrak{D}(L)\supset C_c^2(\mathbb{R}^p)$. In addition, for every $f\in C_c^2(\mathbb{R}^p)$, we have
\begin{align*}
	Lf = \frac{1}{2}\sum_{i,j=1}^p (\sigma\sigma^*)_{ij}\frac{\partial^2 f}{\partial x_i\partial x_j} + \sum_{i=1}^pb_i\frac{\partial f}{\partial x_i},
\end{align*}
where $\sigma^*$ is the matrix transpose of $\sigma$.
\begin{proof}
(i) We first fix $f\in C_0(\mathbb{R}^p)$, and verify that $Q_t f\in C_0(\mathbb{R}^p)$. Since $x\mapsto F_x(\bfw)$ is continuous for every $\bfw\in C(\mathbb{R}^+,\mathbb{R}^q)$, the continuity of $Q_tf$ follows from (\ref{eq:6.11}) and dominated convergence theorem. Let $X_t^x$ be a solution of $E^x(\sigma,b)$. By Markov's inequality and \hyperref[lemma:6.20]{Lemma 6.20}, there exists a constant $C_x>0$ such that
\begin{align*}
	\P(\vert X_t^x-x\vert>\lambda)&\leq\frac{1}{\lambda^2}\E\left[\vert X_t^x-x\vert^2\right]\leq \frac{C_x\e^{C_x(t+t^2)}(t+t^2)}{\lambda^2},\quad\forall t\geq 0.
\end{align*}
Then we have
\begin{align*}
	\left\vert Q_tf(x)\right\vert \leq\left\vert\E\left[f(X_t^x)\mathds{1}_{\{\vert X_t^x-x\vert\leq\lambda\}}\right]\right\vert + \left\Vert f\right\Vert\P(\vert X_t^x-x\vert>\lambda) \leq \sup_{y:\,\vert y-x\vert\leq\lambda}\vert f(y)\vert + \frac{C_x\e^{C_x(t+t^2)}\left\Vert f\right\Vert(t+t^2)}{\lambda^2}
\end{align*}

Since $f\in C_0(\mathbb{R}^p)$, the first term of the last display converges to $0$ as $x\to\infty$ for all $\lambda > 0$. Then we have $\limsup_{x\to\infty}\vert Q_t f(x)\vert\leq C\left\Vert f\right\Vert(t^2+t)/\lambda^2$ for all $\lambda>0$. This implies $Q_tf(x)\to 0$ as $x\to\infty$.

Now we fix $x\in\mathbb{R}^p$, and verify that $Q_tf(x)\to f(x)$ as $t\downarrow 0$. For any $\lambda>0$,
\begin{align*}
	\left\vert\E\left[f(X_t^x)\right] - f(x)\right\vert&\leq\sup_{y:\vert y-x\vert\leq\lambda}\vert f(x)-f(y)\vert + 2\left\Vert f\right\Vert\P\left(\vert X_t^x-x\vert > \lambda\right)\\
	&\leq \sup_{y:\vert y-x\vert\leq\lambda}\vert f(x)-f(y)\vert + \frac{2C_x\e^{C_x(t+t^2)}\Vert f\Vert(t+t^2)}{\lambda^2}\\ 
	&\to \sup_{y:\vert y-x\vert\leq\lambda}\vert f(x)-f(y)\vert,\quad as\ \ t\downarrow 0.
\end{align*}
Taking $\lambda\downarrow 0$ gives $Q_tf(x)\to f(x)$.\vspace{0.1cm}

(ii) To prove the second assertion, we use Itô's formula to $f(X_t^x)$, where $f\in C_c^2(\mathbb{R}^p)$. Recall that
\begin{align*}
	X_t^{x,i} &= x_i + \sum_{k=1}^q \int_0^t\sigma_{ik}(X_s^x)\,\d B_s^k + \int_0^t b_i(X_s^x)\,\d s,\\
	\langle X^{x,i},X^{x,j}\rangle_t &= \sum_{k=1}^q\int_0^t\sigma_{ik}(X_s^x)\sigma_{jk}(X_s^x)\,\d s = \int_0^t(\sigma\sigma^*)_{ij}(X_s^x)\,\d s.
\end{align*}
Using Itô's formula [\hyperref[thm:5.9]{Theorem 5.9}] and associativity of stochastic integrals, one have
\begin{align*}
	&f(X_t^x) = f(x)+\sum_{i=1}^p\int_0^t\frac{\partial f}{\partial x_i}(X_s^x)\,\d X_s^{x,i} + \frac{1}{2}\sum_{i,j=1}^p\int_0^t\frac{\partial^2 f}{\partial x_i\partial x_j}(X_s^x)\,\d\langle X^{x,i},X^{x,j}\rangle_s\\
	&= f(x) + \sum_{i=1}^p\sum_{k=1}^q\int_0^t\sigma_{ik}(X_s^x)\frac{\partial f}{\partial x_i}(X_s^x)\,\d B_s^k + \int_0^t\left[\sum_{i=1}^p b_i(X_s^x)\frac{\partial f}{\partial x_i}(X_s^x)+\frac{1}{2}\sum_{i,j=1}^p(\sigma\sigma^*)_{ij}(X_s^x)\frac{\partial^2 f}{\partial x_i\partial x_j}(X_s^x)\right]\d s.
\end{align*}
Define $\displaystyle g=\frac{1}{2}\sum_{i,j=1}^p(\sigma\sigma^*)_{ij}\frac{\partial^2 f}{\partial x_i\partial x_j}+\sum_{i=1}^p b_i\frac{\partial f}{\partial x_i}$. Then the process\vspace{-0.1cm}
\begin{align*}
	M_t = \sum_{i=1}^p\sum_{k=1}^q\int_0^t\sigma_{ik}(X_s^x)\frac{\partial f}{\partial x_i}(X_s^x)\,\d B_s^k = f(X_t^x)-f(x)-\int_0^t g(X_s^x)\,\d s
\end{align*}
is a continuous local martingale. Since $f\in C_c^2(\mathbb{R}^p)$, both $f$ and $g$ are compactly supported continuous functions, hence the process $(M_t)_{t\geq 0}$ is uniformly bounded. By \hyperref[prop:3.54]{Proposition 3.54 (ii)}, $M$ is a martingale. According to \hyperref[thm:6.17]{Theorem 6.17}, $f\in\mathfrak{D}(L)$, and $Lf=g$, which complete our proof.
\end{proof}
\paragraph{Remark.} Let $\Sigma=\sigma\sigma^*$ be the covariance matrix. We can formally write the generator $L$ of $E(\sigma,b)$ as
\begin{align*}
	L=\frac{1}{2}\Sigma\cdot\nabla^2 + b\cdot\nabla,
\end{align*}
where $\nabla^2$ is the Hessian matrix. Fix $f\in C_c^2(\mathbb{R}^p)$, and let $u(t,x)=Q_tf(x)$. Then $u:\mathbb{R}_+\times\mathbb{R}^p\to\mathbb{R}$ solves the following Kolmogorov backward equation:
\begin{align*}
	\begin{cases}
	\frac{\partial u}{\partial t}(t,x)=\frac{1}{2}\Sigma(x)\cdot\nabla_x^2 u(t,x)+b(x)\cdot \nabla_x u(t,x),\ t>0\\
	u(0,x)=f(x).
	\end{cases}
\end{align*}

\paragraph{Example 6.22\label{example:6.22}} (Ornstein-Uhlenbeck process). Let $\lambda>0$. An one-dimensional \textit{Ornstein-Uhlenbeck} process is the solution of the following SDE:
\begin{align*}
	\d X_t = \d B_t - \lambda X_t\,\d t.
\end{align*}
By applying Itô's formula to $\e^{\lambda t} X_t$, we have
\begin{align*}
	\e^{\lambda t} X_t = X_0 + \int_0^t\e^{\lambda s}\d X_t + \lambda\int_0^t\e^{\lambda s}X_s\,\d t\quad\Rightarrow\quad X_t = X_0\e^{-\lambda t} + \int_0^t\e^{-\lambda(t-s)}\,\d B_t.
\end{align*}
Conditional on $X_0$, the mean and covariance of $(X_t)_{t\geq 0}$ are given by
\begin{align*}
	\E[X_t|X_0] = X_0\e^{-\lambda t},\quad \cov(X_s,X_t|X_0) = \frac{1}{2\lambda}\left(\e^{-\lambda\vert s-t\vert}-\e^{-\lambda(s+t)}\right).
\end{align*}
The generator of this process is given by
\begin{align*}
	Lf(x) = \frac{1}{2}\frac{\partial^2 f}{\partial x^2}(x) - \lambda x\frac{\partial f}{\partial x}(x),\quad f\in C_c^2(\mathbb{R}).
\end{align*}
Moreover, if we set the starting law as $X_0\sim N(0,\frac{1}{2\lambda})$, then the covariance function of $X=(X_t)_{t\geq 0}$ is given by $K(s,t)=\frac{1}{2\lambda}\e^{-\lambda\vert s-t\vert}$.  In this case, $X$ is a stationary Gaussian process. 

\paragraph{Example 6.23\label{example:6.23}} (Geometric Brownian motion). Let $\sigma>0$ and $\mu\in\mathbb{R}$. A \textit{geometric Brownian motion} with parameters $\sigma$ and $\mu$ is the solution of the following SDE:
\begin{align*}
	\d X_t = \sigma X_t\,\d B_t + \mu X_t\,\d t.
\end{align*}
The quadratic variation of $X$ is
\begin{align*}
	X_t = X_0 + \int_0^\infty\sigma X_t\,\d B_t + \int_0^t\mu X_t\,\d t\quad\Rightarrow\quad \langle X,X\rangle_t = \sigma^2\int_0^t X_t^2\,\d t.
\end{align*}
Assume $X_0>0$. By applying Itô's formula to $\log X_t$, we have
\begin{align*}
	\log X_t = \log X_0 + \int_0^t\frac{1}{X_t}\,\d X_t - \frac{1}{2}\int_0^t\frac{1}{X_t^2}\,\d\langle X,X\rangle_t=\log X_0 + \sigma B_t + \biggl(\mu -\frac{\sigma^2}{2}\biggr)t.
\end{align*}
Therefore, starting from $X_0>0$, we have 
\begin{align*}
	X_t = X_0\exp\left(\sigma B_t + \left(\mu -\frac{\sigma^2}{2}\right)t\right).
\end{align*}
Note that $M_t=\exp\left(\sigma B_t - \frac{\sigma^2}{2}t\right)$ is a martingale. Conditional on $X_0$, the mean and covariance of $(X_t)_{t\geq 0}$ are given by
\begin{align*}
	\E[X_t|X_0] = X_0\e^{\mu t},\quad \cov(X_s,X_t|X_0) = X_0^2\e^{\mu(s+t)}\left(\e^{\sigma^2(s\wedge t)}-1\right).
\end{align*}
The generator of this process is given by
\begin{align*}
	Lf(x) = \frac{\sigma^2}{2}x^2\frac{\partial^2 f}{\partial x^2}(x) + \mu x\frac{\partial f}{\partial x}(x),\quad f\in C_c^2(\mathbb{R}).
\end{align*}
Furthermore, for any partition $0=t_0<t_1<t_2<\cdots<t_{n-1}<t_n<\cdots$, we have independence of the successive ratios:
\begin{align*}
	\frac{X_1-X_0}{X_0},\frac{X_2-X_1}{X_1},\cdots,\frac{X_n-X_{n-1}}{X_{n-1}},\cdots,
\end{align*}
which is a consequence of independent increment property of Brownian motions.

\paragraph{Hermitian Adjoint of the Generator.} Now we discuss the adjoint $L^*$ of generator $L$ in sense that
\begin{align*}
	\int_{\mathbb{R}^p} Lf(x)g(x)\,\d x = \int_{\mathbb{R}^p} f(x)L^*g(x)\,\d x.
\end{align*}

We use integration by parts. Since both $f$ and $g$ are compactly supported on $\mathbb{R}^p$, we can choose a common compact support $\Gamma\subset\mathbb{R}^p$. Then
\begin{align*}
	&\int_\Gamma \nabla f\cdot (bg)\,\d m = -\int_\Gamma f\nabla\cdot(bg)\,\d m,\\
	&\int_\Gamma\nabla^2 f\cdot(\Sigma g)\,\d m = -\int_\Gamma\nabla f\cdot\left(\nabla\cdot(\Sigma g)\right)\,\d m = \int_\Gamma f\nabla^2\cdot(\Sigma g)\,\d m.
\end{align*}
Consequently, we have
\begin{align*}
	L^*=\frac{1}{2}\nabla^2\cdot\Sigma - \nabla\cdot b\quad\Leftrightarrow\quad L^* g = \frac{1}{2}\sum_{i,j=1}^p\frac{\partial^2((\sigma\sigma^*)_{ij}g)}{\partial x_i\partial x_j} - \sum_{i=1}^p\frac{\partial(b_i g)}{\partial x_i}.\label{eq:6.13}\tag{6.13}
\end{align*}
We denote by $\mathfrak{D}(L^*)$ the set of functions $g$ such that (\ref{eq:6.13}) makes sense.

\paragraph{Theorem 6.24\label{thm:6.24}} (Fokker-Planck equation). Let $(X_t)_{t\geq 0}$ be a diffusion process with diffusion $\sigma:\mathbb{R}^p\to\mathbb{R}$ and drift $b:\mathbb{R}^p\to\mathbb{R}$, and let $\rho_0$ be a probability density function on $\mathbb{R}^p$. Let $\rho(t,\cdot)$ be the probability density function of $X_t$ for every $t\geq 0$. Then $\rho(\cdot,x)$ is differentiable on $\mathbb{R}_+$ for every $x\in\mathbb{R}^d$, $\rho(t,\cdot)\in\mathfrak{D}(L^*)$ for every $t\geq 0$, and $\rho$ solves the following \textit{Fokker-Planck equation}:
\begin{align*}
	\begin{cases}
		\frac{\partial\rho}{\partial t}(t,x) = \frac{1}{2}\nabla^2_x\cdot\Sigma(x)\rho(t,x) - \nabla_x\cdot b(x)\rho(t,x),\ t>0\\
		\rho(0,x)=\rho_0(x)
	\end{cases}
\end{align*}
\begin{proof}
Let $q(\cdot|t,x)$ be the probability density function of $Q_t(\cdot,x)$, where $t>0$. Let $f\in C_c^2(\mathbb{R}^p)$. Then we have
	\begin{align*}
		\lim_{t\downarrow 0}\frac{1}{t}\left(\int_{\mathbb{R}^p} f(y)q(y|t,x)\,\d s - f(x)\right) = Lf(x)= \frac{1}{2}\Sigma(x)\cdot\nabla^2 f(x) + b(x)\cdot f(x),\quad\forall x\in\mathbb{R}.
	\end{align*}
For any $t>0$, we use interchangeability of derivative and integration:
	\begin{align*}
		\int_{\mathbb{R}^p}f(y)\frac{\partial q}{\partial t}(y|t,x)\,\d y &= \frac{\partial}{\partial t}\int_{\mathbb{R}^p}f(y)q(y|t,x)\,\d y\\
		&= \lim_{s\downarrow 0}\frac{1}{s}\int_{\mathbb{R}^p}\left(q(y|t+s,x)-q(y|t,x)\right)f(y)\,\d y\\
		&=\lim_{s\downarrow 0}\frac{1}{s}\left(\int_{\mathbb{R}^p}\int_{\mathbb{R}^p}q(z|t,x)q(y|s,z)f(y)\,\d z\,\d y-\int_{\mathbb{R}^p}q(z|t,x)f(z)\,\d y\right)\\
		&=\lim_{s\downarrow 0}\frac{1}{s}\int_{\mathbb{R}^p}q(z|t,x)\left(\int_{\mathbb{R}^p}q(y|s,z)f(y)\,\d y-f(z)\right)\,\d z\\
		&=\int_{\mathbb{R}^p}q(z|t,x)Lf(z)\,\d z =\int_{\mathbb{R}^p}L^*q(z|t,x)f(z)\,\d z.
	\end{align*}
Here the third equality uses Chapman-Kolmogorov identity, and the fifth uses dominated convergence. Since the above equation holds for all $f\in C_c^2(\mathbb{R})$, one have the following result:
\begin{align*}
	\frac{\partial q}{\partial t}(y|t,x)=L^*q(y|t,x)=\frac{1}{2}\nabla^2\cdot\Sigma(z)q(y|t,x)-\nabla\cdot b(y)q(y|t,x).\label{eq:6.14}\tag{6.14}
\end{align*}
Now assume that $X_0\sim\rho_0$. Then we have
\begin{align*}
	\rho(t,y)=\int_{\mathbb{R}^p}q(y|t,x)\rho_0(x)\,\d x
\end{align*}
By applying this integration to both sides of (\ref{eq:6.14}), one obtain the desired result.
\end{proof}

\newpage
\section{Connection to PDEs}
\newpage
\begin{thebibliography}{100}
\bibitem{1} Richard M. Dudley. (2002). \textit{Real Analysis and Probability}. Cambridge University Press, Cambridge.
\bibitem{2} Achim Klenke. (2013). \textit{Probability Theory: A Comprehensive Course, 2nd Edition}. Springer, Berlin.
\bibitem{3} Jean-François Le Gall. (2013). \textit{Brownian Motion, Martingales, and Stochastic Calculus}. Springer, Berlin.
\bibitem{4} Samuel Karlin and Howard M. Taylor. (1975). \textit{A First Course in Stochastic Processes, 2nd Edition}. Academic Press, Inc., New York.
\bibitem{5} Jacques Neveu. (1964). \textit{Bases Mathématiques du 
Calcul des Probabilités}. Masson et Cie, Paris.
\bibitem{6} Daniel Revuz and Marc Yor. (1999). \textit{Continuous Martingales 
and Brownian Motion, 3rd Edition}. Springer, Berlin.
\bibitem{7} Giuseppe Da Prato. (2014). \textit{Introduction to Stochastic Analysis and Malliavin Calculus}. Edizioni della Normale, Pisa.
\bibitem{8} Grigorios A. Pavliotis. (2014). \textit{Stochastic Processes and Applications, Diffusion Processes, the Fokker-Planck and Langevin Equations.} Springer, Berlin.
\end{thebibliography}
\end{document}